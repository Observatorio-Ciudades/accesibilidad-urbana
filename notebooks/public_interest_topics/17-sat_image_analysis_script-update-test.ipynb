{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8439623-6181-4ada-b78f-cad1ccdfcfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/accesibilidad-urbana/aup/data.py:25: UserWarning: The `utils.config` function is deprecated and will be removed in a future release. Instead, use the `settings` module directly to configure a global setting's value. For example, `ox.settings.log_console=True`.\n",
      "  ox.config(\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    import aup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5287a10f-d9a7-4afb-b2d0-3cdf06f38385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(index_analysis, city, band_name_dict, start_date, end_date, freq, satellite, save=False, del_data=False):\n",
    "\n",
    "    ###############################\n",
    "    # Create city area of interest with biggest hexs\n",
    "    big_res = min(res)\n",
    "    schema_hex = 'hexgrid'\n",
    "    table_hex = f'hexgrid_{big_res}_city_2020'\n",
    "\n",
    "    # Download hexagons with type=urban\n",
    "    type = 'urban'\n",
    "    query = f\"SELECT hex_id_{big_res},geometry FROM {schema_hex}.{table_hex} WHERE \\\"city\\\" = '{city}\\' AND \\\"type\\\" = '{type}\\'\"\n",
    "    hex_urban = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "    print(\"Loaded urban\")\n",
    "    \n",
    "    # Download hexagons with type=rural within 500m buffer\n",
    "    poly = hex_urban.to_crs(\"EPSG:6372\").buffer(500).reset_index()\n",
    "    poly = poly.to_crs(\"EPSG:4326\")\n",
    "    poly_wkt = poly.dissolve().geometry.to_wkt()[0]\n",
    "    type = 'rural'\n",
    "    query = f\"SELECT hex_id_{big_res},geometry FROM {schema_hex}.{table_hex} WHERE \\\"city\\\" = '{city}\\' AND \\\"type\\\" = '{type}\\' AND (ST_Intersects(geometry, \\'SRID=4326;{poly_wkt}\\'))\"\n",
    "    hex_rural = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "    print(\"Loaded rural\")\n",
    "    \n",
    "    # Concatenate urban and rural hex\n",
    "    hex_city = pd.concat([hex_urban, hex_rural])\n",
    "\n",
    "    print(f'Downloaded {len(hex_city)} hexagon features')\n",
    "    \n",
    "    # Download and process rasters\n",
    "    polygon = hex_city.dissolve().geometry\n",
    "    #Review result\n",
    "    print('Starting download_raster_from_pc')\n",
    "    df_len = aup.download_raster_from_pc(polygon, index_analysis, city, freq,\n",
    "                                        start_date, end_date, tmp_dir, band_name_dict, satellite)\n",
    "\n",
    "    aup.log(f'Finished downloading and processing rasters for {city}')\n",
    "\n",
    "    ### raster to hex\n",
    "    ### hex preprocessing\n",
    "    aup.log('Started loading hexagons at different resolutions')\n",
    "    \n",
    "    # Create res_list\n",
    "    res_list=[]\n",
    "    for r in range(res[0],res[-1]+1):\n",
    "        res_list.append(r)\n",
    "\n",
    "    # Load hexgrids\n",
    "    hex_gdf = hex_city.copy()\n",
    "    hex_gdf.rename(columns={f'hex_id_{big_res}':'hex_id'}, inplace=True)\n",
    "    hex_gdf['res'] = big_res\n",
    "    \n",
    "    for r in res_list:\n",
    "        # biggest resolution already loaded\n",
    "        if r == big_res:\n",
    "            continue\n",
    "        \n",
    "        # Load hexgrid\n",
    "        table_hex = f'hexgrid_{r}_city_2020'\n",
    "        query = f\"SELECT hex_id_{r},geometry FROM {schema_hex}.{table_hex} WHERE (ST_Intersects(geometry, \\'SRID=4326;{poly_wkt}\\'))\"\n",
    "        hex_tmp = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "        # Format hexgrid\n",
    "        hex_tmp.rename(columns={f'hex_id_{r}':'hex_id'}, inplace=True)\n",
    "        hex_tmp['res'] = r\n",
    "        # Concatenate to hex_gdf\n",
    "        hex_gdf = pd.concat([hex_gdf, hex_tmp])\n",
    "\n",
    "        del hex_tmp\n",
    "\n",
    "    aup.log('Finished creating hexagons at different resolutions')\n",
    "\n",
    "    for r in list(hex_gdf.res.unique()):\n",
    "\n",
    "        processing_chunk = 100000\n",
    "\n",
    "        # filters hexagons at specified resolution\n",
    "        hex_gdf_res = hex_gdf.loc[hex_gdf.res==r].copy()\n",
    "        hex_gdf_res = hex_gdf_res.reset_index(drop=True)\n",
    "\n",
    "        if len(hex_gdf_res)>processing_chunk:\n",
    "            aup.log(f'hex_gdf_res len: {len(hex_gdf_res)} is bigger than processing chunk: {processing_chunk}')\n",
    "            c_processing = len(hex_gdf_res)/processing_chunk\n",
    "            aup.log(f'There are {round(c_processing)} processes')\n",
    "            for i in range(int(c_processing)+1):\n",
    "                aup.log(f'Processing from {i*processing_chunk} to {(i+1)*processing_chunk}')\n",
    "                hex_gdf_i = hex_gdf_res.iloc[int(processing_chunk*i):int(processing_chunk*(1+i))].copy()\n",
    "                raster_to_hex_save(hex_gdf_i, df_len, index_analysis, tmp_dir, city, r, save, i)\n",
    "\n",
    "        else:\n",
    "            aup.log('hex_gdf len smaller than processing chunk')\n",
    "            hex_gdf_i = hex_gdf_res.copy()\n",
    "            raster_to_hex_save(hex_gdf_i, df_len, index_analysis, tmp_dir, city, r, save)\n",
    "\n",
    "    aup.log(f'Finished processing city -- {city}')\n",
    "    del hex_gdf\n",
    "\n",
    "    if del_data:\n",
    "        # delete raster files\n",
    "        aup.delete_files_from_folder(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "641f9c3c-deae-47f3-8b9d-6e2cae642d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_to_hex_save(hex_gdf_i, df_len, index_analysis, tmp_dir, city, r, save, i=0):\n",
    "    aup.log(f'Translating raster to hexagon for res: {r}')\n",
    "\n",
    "    hex_raster_analysis, df_raster_analysis = aup.raster_to_hex_analysis(hex_gdf_i, df_len, index_analysis,\n",
    "                                                                tmp_dir, city, r)\n",
    "    aup.log('Finished assigning raster data to hexagons')\n",
    "    aup.log(f'df nan values: {df_raster_analysis[index_analysis].isna().sum()}')\n",
    "    if df_raster_analysis[index_analysis].isna().sum() > 0:\n",
    "        raise NanValues('NaN values are still present after processing')\n",
    "    \n",
    "    # local save (test)\n",
    "    if local_save:\n",
    "        hex_raster_analysis.to_file(tmp_dir+'local_save/'+f'{city}_{index_analysis}_HexRes{r}_v{i}.geojson')\n",
    "        df_raster_analysis.to_csv(tmp_dir+'local_save/'+f'{city}_{index_analysis}_HexRes{r}_v{i}.csv')\n",
    "\n",
    "    if save:\n",
    "        # upload to database\n",
    "        upload_chunk = 150000\n",
    "        aup.log('Starting upload')\n",
    "\n",
    "        if r == 8:\n",
    "\n",
    "            aup.df_to_db_slow(df_raster_analysis, f'{index_analysis}_complete_dataset_hex',\n",
    "                            'raster_analysis', if_exists='append', chunksize=upload_chunk)\n",
    "\n",
    "            aup.gdf_to_db_slow(hex_raster_analysis, f'{index_analysis}_analysis_hex',\n",
    "                            'raster_analysis', if_exists='append')\n",
    "\n",
    "        else:\n",
    "            limit_len = 5000000\n",
    "            if len(df_raster_analysis)>limit_len:\n",
    "                c_upload = len(df_raster_analysis)/limit_len\n",
    "                for k in range(int(c_upload)+1):\n",
    "                    aup.log(f\"Starting range k = {k} of {int(c_upload)}\")\n",
    "                    df_inter_upload = df_raster_analysis.iloc[int(limit_len*k):int(limit_len*(1+k))].copy()\n",
    "                    aup.df_to_db(df_inter_upload,f'{index_analysis}_complete_dataset_hex',\n",
    "                                    'raster_analysis', if_exists='append')\n",
    "            else:\n",
    "                aup.df_to_db(df_raster_analysis,f'{index_analysis}_complete_dataset_hex',\n",
    "                                    'raster_analysis', if_exists='append')\n",
    "            aup.gdf_to_db_slow(hex_raster_analysis, f'{index_analysis}_analysis_hex',\n",
    "                            'raster_analysis', if_exists='append')\n",
    "        aup.log(f'Finished uploading data for res{r}')\n",
    "        \n",
    "    # delete variables\n",
    "    del df_raster_analysis\n",
    "    del hex_raster_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67009e08-8f5d-4006-9bab-fa8ae05c987d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded urban\n",
      "Loaded rural\n",
      "Downloaded 497 hexagon features\n",
      "Starting download_raster_from_pc\n"
     ]
    }
   ],
   "source": [
    "aup.log('--'*20)\n",
    "aup.log('Starting script')\n",
    "\n",
    "band_name_dict = {'nir':[False], #If GSD(resolution) of band is different, set True.\n",
    "                  'red':[False], #If GSD(resolution) of band is different, set True.\n",
    "                  'eq':[\"(nir-red)/(nir+red)\"]} \n",
    "index_analysis = 'ndvi'\n",
    "tmp_dir = f'../../data/processed/tmp_{index_analysis}/'\n",
    "res = [8,11] # 8, 11\n",
    "freq = 'MS'\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2022-12-31'\n",
    "satellite = \"sentinel-2-l2a\"\n",
    "save = False # True\n",
    "local_save = True # True (test)\n",
    "del_data = True # True\n",
    "\n",
    "# Create folder to store skip list\n",
    "folder_dir = f'../../data/processed/{index_analysis}_skip_city/'\n",
    "if os.path.exists(folder_dir) == False:\n",
    "    os.mkdir(folder_dir)\n",
    "\n",
    "df_skip_dir = f'../../data/processed/{index_analysis}_skip_city/skip_list.csv'\n",
    "if os.path.exists(df_skip_dir) == False: # Or folder, will return true or false\n",
    "    df_skip = pd.DataFrame(columns=['city','missing_months','unable_to_download'])\n",
    "    df_skip.to_csv(df_skip_dir)\n",
    "else:\n",
    "    df_skip = pd.read_csv(df_skip_dir)\n",
    "\n",
    "skip_list = list(df_skip.city.unique())\n",
    "\n",
    "# Create folder to store raster analysis\n",
    "if os.path.exists(tmp_dir) == False:\n",
    "    os.mkdir(tmp_dir)\n",
    "\n",
    "#gdf_mun = aup.gdf_from_db('metro_gdf_2020', 'metropolis')\n",
    "#gdf_mun = gdf_mun.sort_values(by='city')\n",
    "\n",
    "# prevent cities being analyzed several times in case of a crash\n",
    "aup.log('Downloading preprocessed data')\n",
    "processed_city_list = []\n",
    "try:\n",
    "    query = f\"SELECT city FROM raster_analysis.{index_analysis}_analysis_hex\"\n",
    "    processed_city_list = aup.df_from_query(query)\n",
    "    processed_city_list = list(processed_city_list.city.unique())\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# analysis\n",
    "\n",
    "#------test\n",
    "city_list = ['Aguascalientes']\n",
    "for city in city_list:\n",
    "#------test\n",
    "\n",
    "#for city in gdf_mun.city.unique():\n",
    "\n",
    "    # if city not in processed_city_list and city not in skip_list:\n",
    "    if city not in processed_city_list and city not in skip_list:\n",
    "\n",
    "        aup.log(f'\\n Starting city {city}')\n",
    "\n",
    "        try:\n",
    "            main(index_analysis, city, band_name_dict, start_date,\n",
    "                end_date, freq, satellite, save, del_data)\n",
    "        except Exception as e:\n",
    "            aup.log(e)\n",
    "            aup.log(f'Error with city {city}')\n",
    "            df_skip.loc[len(df_skip)+1,'city'] = city\n",
    "            df_file_dir = tmp_dir+index_analysis+f'_{city}_dataframe.csv'\n",
    "            if os.path.exists(df_file_dir) == False: # Or folder, will return true or false\n",
    "                df_skip.loc[len(df_skip),'missing_months'] = -1\n",
    "                df_skip.loc[len(df_skip),'unable_to_download'] = -1\n",
    "            else:\n",
    "                df_raster = pd.read_csv(df_file_dir)\n",
    "                missing_months = len(df_raster.loc[df_raster.data_id==0])\n",
    "                not_donwloadable = len(df_raster.loc[df_raster.able_to_download==0])\n",
    "                df_skip.loc[len(df_skip),'missing_months'] = missing_months\n",
    "                df_skip.loc[len(df_skip),'unable_to_download'] = not_donwloadable\n",
    "            df_skip.to_csv(df_skip_dir, index=False)\n",
    "            if del_data:\n",
    "                # delete raster files\n",
    "                aup.delete_files_from_folder(tmp_dir)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7ef8c-e014-4a6d-affa-7275b2763321",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd79ca94-1277-4252-97e8-9374c0f29d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = f'../../data/processed/tmp_{index_analysis}/'\n",
    "res = [8,11] # 8, 11\n",
    "local_save = True # True (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b91fb19c-efee-460e-b92c-140e99b4d9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded urban\n",
      "Loaded rural\n",
      "Downloaded 497 hexagon features\n",
      "Starting download_raster_from_pc\n"
     ]
    }
   ],
   "source": [
    "#main(index_analysis, city, band_name_dict, start_date,end_date, freq, satellite, save, del_data)\n",
    "index_analysis = 'ndvi'\n",
    "city = 'Aguascalientes'\n",
    "band_name_dict = {'nir':[False], #If GSD(resolution) of band is different, set True.\n",
    "                  'red':[False], #If GSD(resolution) of band is different, set True.\n",
    "                  'eq':[\"(nir-red)/(nir+red)\"]} \n",
    "start_date = '2018-01-01'\n",
    "end_date = '2022-12-31'\n",
    "freq = 'MS'\n",
    "satellite = \"sentinel-2-l2a\"\n",
    "save = False # True\n",
    "del_data = True # True\n",
    "\n",
    "# Create city area of interest with biggest hexs\n",
    "big_res = min(res)\n",
    "schema_hex = 'hexgrid'\n",
    "table_hex = f'hexgrid_{big_res}_city_2020'\n",
    "\n",
    "# Download hexagons with type=urban\n",
    "type = 'urban'\n",
    "query = f\"SELECT hex_id_{big_res},geometry FROM {schema_hex}.{table_hex} WHERE \\\"city\\\" = '{city}\\' AND \\\"type\\\" = '{type}\\'\"\n",
    "hex_urban = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "print(\"Loaded urban\")\n",
    "\n",
    "# Download hexagons with type=rural within 500m buffer\n",
    "poly = hex_urban.to_crs(\"EPSG:6372\").buffer(500).reset_index()\n",
    "poly = poly.to_crs(\"EPSG:4326\")\n",
    "poly_wkt = poly.dissolve().geometry.to_wkt()[0]\n",
    "type = 'rural'\n",
    "query = f\"SELECT hex_id_{big_res},geometry FROM {schema_hex}.{table_hex} WHERE \\\"city\\\" = '{city}\\' AND \\\"type\\\" = '{type}\\' AND (ST_Intersects(geometry, \\'SRID=4326;{poly_wkt}\\'))\"\n",
    "hex_rural = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "print(\"Loaded rural\")\n",
    "\n",
    "# Concatenate urban and rural hex\n",
    "hex_city = pd.concat([hex_urban, hex_rural])\n",
    "\n",
    "print(f'Downloaded {len(hex_city)} hexagon features')\n",
    "\n",
    "# Download and process rasters\n",
    "polygon = hex_city.dissolve().geometry\n",
    "#Review result\n",
    "print('Starting download_raster_from_pc')\n",
    "#df_len = aup.download_raster_from_pc(hex_city, index_analysis, city, freq,start_date, end_date, tmp_dir, band_name_dict, satellite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15bdcaee-b14b-4bd0-9502-b0a4e5d2de2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting bounding coordinates from hexagons\n",
      "Defining time of interest\n",
      "Gathering items for time and area of interest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gds/lib/python3.9/site-packages/pystac_client/item_search.py:835: FutureWarning: get_items() is deprecated, use items() instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1410 items\n",
      "Created dictionary from items\n",
      "Created bounding box for raster cropping\n"
     ]
    }
   ],
   "source": [
    "#download_raster_from_pc(gdf, index_analysis, city, freq, start_date, end_date, tmp_dir, band_name_dict, query={}, satellite=\"sentinel-2-l2a\")\n",
    "query={}\n",
    "\n",
    "# ------------------------------\n",
    "\n",
    "gdf = polygon.copy()\n",
    "\n",
    "# create area of interest coordinates from hexagons to download raster data    \n",
    "print('Extracting bounding coordinates from hexagons')\n",
    "# Create buffer around hexagons\n",
    "poly = gdf.to_crs(\"EPSG:6372\").buffer(500)\n",
    "poly = poly.to_crs(\"EPSG:4326\")\n",
    "poly = gpd.GeoDataFrame(geometry=poly).dissolve().geometry\n",
    "# Extracts coordinates from polygon as DataFrame\n",
    "coord_val = poly.bounds\n",
    "# Gets coordinates for bounding box\n",
    "n = coord_val.maxy.max()\n",
    "s = coord_val.miny.min()\n",
    "e = coord_val.maxx.max()\n",
    "w = coord_val.minx.min()\n",
    "# Sets the coordinates for the area of interest\n",
    "area_of_interest = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "        [\n",
    "            [e, s],\n",
    "            [w, s],\n",
    "            [w, n],\n",
    "            [e, n],\n",
    "            [e, s],\n",
    "        ]\n",
    "    ],\n",
    "}\n",
    "\n",
    "# create time of interest\n",
    "print('Defining time of interest')\n",
    "time_of_interest = aup.create_time_of_interest(start_date, end_date, freq=freq)\n",
    "\n",
    "# gathers items for time and area of interest\n",
    "print('Gathering items for time and area of interest')\n",
    "items = aup.gather_items(time_of_interest, area_of_interest, query=query, satellite=satellite)\n",
    "print(f'Fetched {len(items)} items')\n",
    "\n",
    "date_list = aup.available_datasets(items, satellite)\n",
    "\n",
    "# create dictionary from links\n",
    "band_name_list = list(band_name_dict.keys())[:-1]\n",
    "assets_hrefs = aup.link_dict(band_name_list, items, date_list)\n",
    "print('Created dictionary from items')\n",
    "\n",
    "# analyze available data according to raster properties\n",
    "df_len, missing_months = aup.df_date_links(assets_hrefs, start_date, end_date, band_name_list, freq)\n",
    "aup.available_data_check(df_len, missing_months) # test for missing months\n",
    "\n",
    "# creates raster and analyzes percentage of missing data points\n",
    "df_len, missing_months = aup.df_date_links(assets_hrefs, start_date, end_date, band_name_list, freq)\n",
    "pct_missing = round(missing_months/len(df_len),2)*100\n",
    "\n",
    "# if more than 50% of data is missing, raise error and print message\n",
    "if pct_missing >= 50:\n",
    "\n",
    "    raise AvailableData('Missing more than 50 percent of data points')\n",
    "\n",
    "# raster cropping with bounding box from earlier \n",
    "bounding_box = gpd.GeoDataFrame(geometry=poly).envelope\n",
    "gdf_bb = gpd.GeoDataFrame(gpd.GeoSeries(bounding_box), columns=['geometry'])\n",
    "print('Created bounding box for raster cropping')\n",
    "\n",
    "# create GeoDataFrame to test nan values in raster\n",
    "gdf_raster_test = gdf.to_crs(\"EPSG:6372\").buffer(1)\n",
    "gdf_raster_test = gdf_raster_test.to_crs(\"EPSG:4326\")\n",
    "gdf_raster_test = gpd.GeoDataFrame(geometry=gdf_raster_test).dissolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b25893-7eb1-4403-9e4e-61ec5c1d74dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDS-10.0",
   "language": "python",
   "name": "gds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
