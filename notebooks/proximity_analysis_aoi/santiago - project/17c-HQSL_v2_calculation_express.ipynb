{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e54bd5b-657f-4463-b18a-d16e57e4605d",
   "metadata": {},
   "source": [
    "# 17c - HQSL v2 calculation (express)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d304f0d-8aa3-4ec1-9283-b48f2b9f9eec",
   "metadata": {},
   "source": [
    "This notebook does the same as notebook 17b but allows iterating over polygons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5ac54f-91ba-4b5d-9176-5bd0d77dcb78",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a34b4c34-1ef1-4b11-90f5-b152f97f2405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    import aup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e06326-76e6-411b-8ca4-3dcc679ed8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h3\n",
    "def neighbour_mean(hex_id, hex_id_name, hex_bins, col_name):\n",
    "    return hex_bins.loc[hex_bins[hex_id_name].isin(h3.k_ring(hex_id,1)),col_name].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec77b7b-f6c2-46d2-89d6-0360686181f9",
   "metadata": {},
   "source": [
    "## Notebook config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ebb597-cb79-4c1c-93ff-75157231ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- AREAS OF ANALYSIS TO BE PROCESSED\n",
    "# allows for ['santiago','zonascensales','comunas','unidadesvecinales','alameda','hex']\n",
    "areas_of_analysis_list = ['santiago','comunas','unidadesvecinales','alameda']\n",
    "\n",
    "# ---------- RESOLUTION OF HEX (IF NEEDED)\n",
    "# If area_analysis = 'hex', ** NEED ** to input hexs resolution\n",
    "res = 10\n",
    "\n",
    "# ---------- PERSONA SPEED\n",
    "walk_speed = 4.5\n",
    "speed_name = str(walk_speed).replace('.','_')\n",
    "\n",
    "# ---------- LOCAL DIRECTORIES\n",
    "# 'alex' or 'edgar'\n",
    "all_local_dirs = 'alex'\n",
    "\n",
    "if all_local_dirs == 'alex':\n",
    "    grl_dir = \"../../../data/external/temporal_todocker/santiago/proximidad/\"        \n",
    "elif all_local_dirs == 'edgar':\n",
    "    grl_dir = \"../../../data/processed/santiago/\"\n",
    "else:\n",
    "    print(\"WARNING: Fix all_local_dirs variable\")\n",
    "\n",
    "# ---------- SAVING\n",
    "# save variables (preprocessed) output to database?\n",
    "upload_preprocessed = True\n",
    "# save FINAL (index) output to database?\n",
    "save = True\n",
    "# how to save to database?\n",
    "if_exists='replace'\n",
    "# save to local?\n",
    "save_local = False\n",
    "\n",
    "# NOTE THAT LOCAL DATA USE IS INTEGRATED IN FUNCTION (True if area_analysis == 'alameda' or 'hexagon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee441e8-d33f-411e-937f-6fca6929b5bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Base data required - Parameters and weights dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6710070-7c19-4462-b5ee-31632a724edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- PARAMETERS AND WEIGHT DICTS\n",
    "# Structure: {social_functions:{themes:[source_names]}}\n",
    "parameters_dict = {'supplying':{'wellbeing':['carniceria', #Accessibility to Butcher/Fish Shops\n",
    "                                             'hogar', #Accessibility to Hardware/Paint Shops\n",
    "                                             #Not available: Accessibility to Greengrocers\n",
    "                                             'bakeries', #Accessibility to Bakeries and delis\n",
    "                                             'supermercado',#Accessibility to supermarkets\n",
    "                                             'banco'#Accessibility to bank\n",
    "                                            ],\n",
    "                                'sociability':['ferias',#Accessibility to city fairs/markets\n",
    "                                               'local_mini_market',#Accessibility to local and mini markets\n",
    "                                               'correos'#ADDED: MAIL SERVICE\n",
    "                                              ],\n",
    "                                'environmental_impact':['centro_recyc'#Accessibility to recycling center\n",
    "                                                        #Not available: Accessibility to compost\n",
    "                                                       ]\n",
    "                               },\n",
    "                   'caring':{'wellbeing':['hospital', #Accessibility to hospital\n",
    "                                          'clinica',#Accessibility to public clinics\n",
    "                                          'farmacia',#Accessibility to pharmacies\n",
    "                                          'vacunatorio',#Accessibility to vaccination center\n",
    "                                          'consult_ado',#Accessibility to optician/audiologist(###ADDED DENTIST)\n",
    "                                          'salud_mental',###ADDED: MENTAL HEALTH\n",
    "                                          'labs_priv',###ADDED: LABORATORIES\n",
    "                                          'residencia_adumayor'###ADDED: ELDERLY PERMANENT RESIDENCIES\n",
    "                                         ],\n",
    "                             'sociability':['eq_deportivo',#Accessibility to sports equipments\n",
    "                                            'club_deportivo'#Accessibility to sport clubs\n",
    "                                           ],\n",
    "                             'environmental_impact':['noise',\n",
    "                                                     'temp'\n",
    "                                 #Not available: Air polution\n",
    "                                                    ]\n",
    "                            },\n",
    "                   'living':{'wellbeing':['civic_office',#Accessibility to civic offices\n",
    "                                          #Not available: Number of street bentches\n",
    "                                          'tax_collection',#ADDED: AFIP(TAX COLLECTOR)\n",
    "                                          'social_security',#ADDED: SOCIAL SECURITY\n",
    "                                          'police',#Accessibility to police(###MOVED FROM LIVING TO CARING)\n",
    "                                          'bomberos'#Accessibility to fire stations\n",
    "                                          #Not available: Accessibility to street lamp\n",
    "                                         ],\n",
    "                             'sociability':['houses',#Accessibility to permanent residencies\n",
    "                                            'social_viv',#Accessibility to social housing\n",
    "                                            #Not available: Accessibility to student housing\n",
    "                                            'hotel'#ADDED: HOTELS\n",
    "                                           ],\n",
    "                             'environmental_impact':['inter',\n",
    "                                                     #Not available: Corrected compactness\n",
    "                                                     #Not available: Width of sidewalks\n",
    "                                                    ],\n",
    "                            },\n",
    "                   'enjoying':{'wellbeing':['museos',#Accessibility to museums\n",
    "                                            #Not available: Accessibility to theater,operas\n",
    "                                            'cines',#Accessibility to cinemas\n",
    "                                            'sitios_historicos',#Accessibility to historical places\n",
    "                                            'ndvi'#Number of trees\n",
    "                                           ],\n",
    "                               'sociability':['restaurantes_bar_cafe',#Accessibility to bars/cafes + Accessibility to restaurants\n",
    "                                              'librerias',#Accessibility to record and book stores, galleries, fairs\n",
    "                                              #Not available: Accessibility to cultural and/or formative spaces\n",
    "                                              #Not available: Accessibility to places of workship\n",
    "                                              'ep_plaza_small'#Accessibility to boulevards, linear parks, small squares + Accessibility to squares\n",
    "                                             ],\n",
    "                               'environmental_impact':['ep_plaza_big'#Accessibility to big parks\n",
    "                                                       #Not available: Accessibility to shared gardens\n",
    "                                                       #Not available: Accessibility to urban playgrounds\n",
    "                                                      ]\n",
    "                              },\n",
    "                   'learning':{'wellbeing':['edu_basica_pub',#'edu_basica_priv',#Accessibility to public elementary school\n",
    "                                            'edu_media_pub',#'edu_media_priv',#Accessibility to public high school\n",
    "                                            'jardin_inf_pub',#'jardin_inf_priv',#Similar to Accessibility to childcare\n",
    "                                            'universidad',#Accessibility to university\n",
    "                                            'edu_tecnica',#ADDED: TECHNICAL EDUCATION\n",
    "                                           ],\n",
    "                               'sociability':['edu_adultos_pub',#'edu_adultos_priv',#Accessibility to adult formation centers\n",
    "                                              'edu_especial_pub',#'edu_especial_priv',#Accessibility to specialized educational centers\n",
    "                                              #Not available: Accesibility to establishments and services for disabled adults\n",
    "                                              'bibliotecas'#Accessibility to libraries(###MOVED FROM ENJOYING TO LEARNING)\n",
    "                                             ],\n",
    "                               'environmental_impact':['centro_edu_amb'#Accessibility to centers for learning environmental activities\n",
    "                                                       #Not available: Accessibility to gardening schools\n",
    "                                                      ],\n",
    "                              },\n",
    "                   'working':{'wellbeing':['paradas_tp_ruta',#Accessibility to bus stop\n",
    "                                           'paradas_tp_metro',#Accessibility to metro\n",
    "                                           'paradas_tp_tren'#Accessibility to train stop\n",
    "                                          ],\n",
    "                              'sociability':['oficinas'#Accessibility to office\n",
    "                                             #Not available: Accessibility to incubators\n",
    "                                             #Not available: AccSeveral other articles cite 60dB as a safe noise zone. essibility to coworking places\n",
    "                                          ],\n",
    "                              'environmental_impact':['ciclovias',\n",
    "                                                      'estaciones_bicicletas'#Accessibility to bike lanes\n",
    "                                                      #Not available: Accessibility to shared bike stations\n",
    "                                                     ]\n",
    "                             }\n",
    "                  }\n",
    "\n",
    "weight_dict = {'carniceria':'rare', #SUPPLYING\n",
    "               'hogar':'rare',\n",
    "               'bakeries':'rare',\n",
    "               'supermercado':'rare',\n",
    "               'banco':'rare',\n",
    "               'ferias':'rare',\n",
    "               'local_mini_market':'rare',\n",
    "               'correos':'very_rare',\n",
    "               'centro_recyc':'rare',\n",
    "               #CARING\n",
    "               'hospital':'very_rare',\n",
    "               'clinica':'rare',\n",
    "               'farmacia':'rare',\n",
    "               'vacunatorio':'very_rare',\n",
    "               'consult_ado':'very_rare',\n",
    "               'salud_mental':'very_rare',\n",
    "               'labs_priv':'very_rare',\n",
    "               'residencia_adumayor':'rare',\n",
    "               'eq_deportivo':'rare',\n",
    "               'club_deportivo':'rare',\n",
    "               'noise':'specific',\n",
    "               'temp':'specific',\n",
    "               #LIVING\n",
    "               'civic_office':'rare', \n",
    "               'tax_collection':'very_rare',\n",
    "               'social_security':'very_rare',\n",
    "               'police':'very_rare',\n",
    "               'bomberos':'very_rare',\n",
    "               'houses':'specific',\n",
    "               'social_viv':'specific',\n",
    "               'hotel':'rare',\n",
    "               'inter':'specific',\n",
    "               #ENJOYING\n",
    "               'museos':'very_rare',\n",
    "               'cines':'very_rare',\n",
    "               'sitios_historicos':'rare',\n",
    "               'ndvi':'specific',\n",
    "               'restaurantes_bar_cafe':'frequent',\n",
    "               'librerias':'rare',\n",
    "               'ep_plaza_small':'frequent',\n",
    "               'ep_plaza_big':'rare',\n",
    "               #LEARNING\n",
    "               'edu_basica_pub':'rare', \n",
    "               'edu_media_pub':'rare',\n",
    "               'jardin_inf_pub':'rare',\n",
    "               'universidad':'very_rare',\n",
    "               'edu_tecnica':'very_rare',\n",
    "               'edu_adultos_pub':'rare',\n",
    "               'edu_especial_pub':'rare',\n",
    "               'bibliotecas':'very_rare',\n",
    "               'centro_edu_amb':'very_rare',\n",
    "               #WORKING\n",
    "               'paradas_tp_ruta':'frequent',\n",
    "               'paradas_tp_metro':'very_rare',\n",
    "               'paradas_tp_tren':'very_rare',\n",
    "               'oficinas':'specific',\n",
    "               'ciclovias':'rare',\n",
    "               'estaciones_bicicletas':'rare',\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee632e2-a0e4-4e22-9105-81e7b5a2f244",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Base data required - Scale function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3a4f88f-b672-4a37-a2c7-0f76fa17a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_fn(cont):\n",
    "    if cont == 0:\n",
    "        res_val = 0\n",
    "    elif cont > 0 and cont < 2:\n",
    "        res_val = res_val_regression(0, 2, 0, 2.5, cont)\n",
    "    elif cont >= 2 and cont < 4:\n",
    "        res_val = res_val_regression(2, 4, 2.5, 5, cont)\n",
    "    elif cont >= 4 and cont < 7:\n",
    "        res_val = res_val_regression(4, 7, 5, 7.5, cont)\n",
    "    elif cont >= 7 and cont < 10:\n",
    "        res_val = res_val_regression(7, 10, 7.5, 10, cont)\n",
    "    elif cont >= 10:\n",
    "        res_val = 10\n",
    "    \n",
    "    return res_val\n",
    "\n",
    "\n",
    "def very_rare_fn(cont):\n",
    "    min_x = 0\n",
    "    max_x = 1\n",
    "    min_y = 0\n",
    "    max_y = 10\n",
    "    \n",
    "    return res_val_regression(min_x, max_x, min_y, max_y, cont)\n",
    "\n",
    "\n",
    "def frequent_fn(cont):\n",
    "    if cont == 0:\n",
    "        res_val = 0\n",
    "    elif cont > 0 and cont < 6:\n",
    "        res_val = res_val_regression(0, 6, 0, 2.5, cont)\n",
    "    elif cont >= 6 and cont < 12:\n",
    "        res_val = res_val_regression(6, 12, 2.5, 5, cont)\n",
    "    elif cont >= 12 and cont < 18:\n",
    "        res_val = res_val_regression(12, 18, 5, 7.5, cont)\n",
    "    elif cont >= 18 and cont < 25:\n",
    "        res_val = res_val_regression(18, 25, 7.5, 10, cont)\n",
    "    elif cont >= 25:\n",
    "        res_val = 10\n",
    "    \n",
    "    return res_val\n",
    "\n",
    "\n",
    "def res_val_regression(min_x, max_x, min_y, max_y, cont):\n",
    "    slope = (max_y-min_y)/(max_x-min_x)\n",
    "    intersect = min_y - slope * min_x\n",
    "    res_val = slope * cont + intersect\n",
    "    if cont > max_x:\n",
    "        res_val = max_y\n",
    "        \n",
    "    return res_val\n",
    "\n",
    "\n",
    "def office_fn(cont):\n",
    "    if cont == 0:\n",
    "        res_val = 0\n",
    "    elif cont > 0 and cont < 2.823938308:\n",
    "        res_val = res_val_regression(0, 2.823938308, 0, 2.5, cont)\n",
    "    elif cont >= 2.823938308 and cont <  5.539263604:\n",
    "        res_val = res_val_regression(2.823938308, 5.539263604, 2.5, 5, cont)\n",
    "    elif cont >= 5.539263604 and cont < 10.96991420:\n",
    "        res_val = res_val_regression(5.539263604, 10.96991420, 5, 7.5, cont)\n",
    "    elif cont >= 10.96991420 and cont < 16.40056479:\n",
    "        res_val = res_val_regression(10.96991420, 16.40056479, 7.5, 10, cont)\n",
    "    elif cont >= 16.40056479:\n",
    "        res_val = 10\n",
    "    \n",
    "    return res_val\n",
    "\n",
    "\n",
    "def ndvi_fn(cont):\n",
    "    min_x = 0\n",
    "    max_x = 0.4\n",
    "    min_y = 0\n",
    "    max_y = 10\n",
    "    if cont > max_x:\n",
    "        return 10\n",
    "    elif cont <= min_x:\n",
    "        return 0\n",
    "    else:\n",
    "        return res_val_regression(min_x, max_x, min_y, max_y, cont)\n",
    "\n",
    "\n",
    "def inter_fn(cont):\n",
    "    min_x = 20\n",
    "    max_x = 100\n",
    "    min_y = 0\n",
    "    max_y = 10\n",
    "    if cont > max_x:\n",
    "        return 10\n",
    "    elif cont < min_x:\n",
    "        return 0\n",
    "    else:\n",
    "        return res_val_regression(min_x, max_x, min_y, max_y, cont)\n",
    "\n",
    "\n",
    "def noise_fn(cont):\n",
    "    min_x = 55\n",
    "    max_x = 70\n",
    "    min_y = 10\n",
    "    max_y = 0\n",
    "    if cont > max_x:\n",
    "        return 0\n",
    "    elif cont < min_x:\n",
    "        return 10\n",
    "    else:\n",
    "        return res_val_regression(min_x, max_x, min_y, max_y, cont)\n",
    "\n",
    "\n",
    "def temp_fn(cont, mean, std):\n",
    "    if cont >= (mean + 2*std):\n",
    "        res_val = 0\n",
    "    elif cont < (mean + 2*std) and cont >= (mean + std):\n",
    "        res_val = res_val_regression((mean + std), (mean + 2*std), 2.5, 0, cont)\n",
    "    elif cont < (mean + std) and cont >= (mean):\n",
    "        res_val = res_val_regression((mean), (mean + std), 5, 2.5, cont)\n",
    "    elif cont < (mean) and cont >= (mean - std):\n",
    "        res_val = res_val_regression((mean - std), (mean), 7.5, 5, cont)\n",
    "    elif cont < (mean - std) and cont >= (mean - 2*std):\n",
    "        res_val = res_val_regression((mean - 2*std), (mean - std), 10, 7.5, cont)\n",
    "    elif cont < (mean - 2*std):\n",
    "        res_val = 10\n",
    "    if area_analysis == 'santiago':\n",
    "        res_val = 5\n",
    "    \n",
    "    return res_val\n",
    "\n",
    "\n",
    "def household_fn(cont):\n",
    "    res_val = res_val_regression(0, 50, 0, 10, cont)\n",
    "    \n",
    "    return res_val\n",
    "\n",
    "    \n",
    "def social_viv_fn(cont):\n",
    "    min_x = 0\n",
    "    max_x = 20\n",
    "    min_y = 0\n",
    "    max_y = 10\n",
    "    if cont > max_x:\n",
    "        return 10\n",
    "    elif cont < min_x:\n",
    "        return 0\n",
    "    else:\n",
    "        return res_val_regression(min_x, max_x, min_y, max_y, cont)\n",
    "\n",
    "\n",
    "def specific_fn(cont, source, mean, std):\n",
    "    if 'ndvi' in source:\n",
    "        return ndvi_fn(cont)\n",
    "    elif 'inter' in source:\n",
    "        return inter_fn(cont)\n",
    "    elif 'noise' in source:\n",
    "        return noise_fn(cont)\n",
    "    elif 'temp' in source:\n",
    "        return temp_fn(cont, mean, std)\n",
    "    elif 'houses' in source:\n",
    "        return household_fn(cont)\n",
    "    elif 'social_viv' in source:\n",
    "        return social_viv_fn(cont)\n",
    "    elif 'oficinas' in source:\n",
    "        return office_fn(cont)\n",
    "\n",
    "\n",
    "def scale_source_fn(cont, source, weight_dict, mean, std):\n",
    "    if weight_dict[source] == 'rare':\n",
    "        return rare_fn(cont)\n",
    "    elif weight_dict[source] == 'very_rare':\n",
    "        return very_rare_fn(cont)\n",
    "    elif weight_dict[source] == 'frequent':\n",
    "        return frequent_fn(cont)\n",
    "    elif weight_dict[source] == 'specific':\n",
    "        return specific_fn(cont, source, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198bdf00-6b2d-4dac-9d87-f9e0509a7251",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Base data required - Index function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a86640-04ba-47a4-8571-9f6faed5a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hqsl_fn(hex_gdf, parameters_dict):\n",
    "\n",
    "    hex_gdf = hex_gdf.copy()\n",
    "    \n",
    "    social_function_list = []\n",
    "    \n",
    "    for social_function in parameters_dict.keys():\n",
    "        social_function_list.append(social_function)\n",
    "    \n",
    "    hex_gdf['hqsl'] = hex_gdf[social_function_list].sum(axis=1)\n",
    "\n",
    "    base_columns = [code_column,'geometry']\n",
    "    filter_list = ['hqsl']\n",
    "    filter_list.extend(base_columns)\n",
    "    hex_gdf = hex_gdf[filter_list].copy()\n",
    "    \n",
    "    return hex_gdf\n",
    "\n",
    "\n",
    "def social_fn(hex_gdf, parameters_dict):\n",
    "    \n",
    "    hex_gdf = hex_gdf.copy()\n",
    "    \n",
    "    for social_function in parameters_dict.keys():\n",
    "        source_list = []\n",
    "        \n",
    "        for indicator in parameters_dict[social_function].keys():\n",
    "            source_list.extend(parameters_dict[social_function][indicator])\n",
    "        \n",
    "        source_list = [s+'_scaled' for s in source_list]\n",
    "        hex_gdf[social_function] = hex_gdf[source_list].mean(axis=1)\n",
    "\n",
    "    base_columns = [code_column,'geometry']\n",
    "    filter_list = list(parameters_dict.keys())\n",
    "    filter_list.extend(base_columns)\n",
    "    hex_gdf = hex_gdf[filter_list].copy()\n",
    "    \n",
    "    return hex_gdf\n",
    "\n",
    "\n",
    "def indicator_fn(hex_gdf, parameters_dict):\n",
    "    hex_ind = hex_analysis.copy()\n",
    "\n",
    "    filter_list = []\n",
    "    \n",
    "    indicator_list = list(set().union(*parameters_dict.values()))\n",
    "    for indicator in indicator_list:\n",
    "        social_indicator = []\n",
    "        \n",
    "        for social_function in parameters_dict.keys():\n",
    "            social_indicator.append(indicator+'_'+social_function)\n",
    "            \n",
    "            source_indicator = parameters_dict[social_function][indicator]\n",
    "            source_indicator = [s+'_scaled' for s in source_indicator]\n",
    "            \n",
    "            hex_ind[indicator+'_'+social_function] = hex_ind[source_indicator].mean(axis=1)\n",
    "    \n",
    "        hex_ind[indicator] = hex_ind[social_indicator].sum(axis=1)\n",
    "        filter_list.extend(social_indicator)\n",
    "        filter_list.append(indicator)\n",
    "    \n",
    "    base_columns = [code_column,'geometry']\n",
    "    filter_list.extend(base_columns)\n",
    "    hex_ind = hex_ind[filter_list].copy()\n",
    "            \n",
    "    return hex_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b9dabd-56ad-47e4-9592-d43c3417225a",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e040fa4d-e3d8-4350-b40a-b322f0e01fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "--- RUNNING CALCULATION FOR AREA OF ANALYSIS: santiago.\n",
      "santiago - LOAD DATA 1.1 Loading proximity data.\n",
      "santiago - LOAD DATA 1.2 Loading areal data.\n",
      "santiago - DATA TREATMENT 2.1 Joining _priv and _pub pois.\n",
      "santiago - DATA TREATMENT 2.2 Merging proximity and areal data.\n",
      "santiago - HQSL FUNCTION - 3.1 Processing variables analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [00:00<00:00, 1478.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "santiago - HQSL FUNCTION - 3.2 Saving variables analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "santiago - HQSL FUNCTION - 4.1 Processing HQSL index.\n",
      "santiago - HQSL FUNCTION - 4.2 Saving HQSL index to database.\n",
      "--------------------------------------------------------------------------------\n",
      "--- RUNNING CALCULATION FOR AREA OF ANALYSIS: comunas.\n",
      "comunas - LOAD DATA 1.1 Loading proximity data.\n",
      "comunas - LOAD DATA 1.2 Loading areal data.\n",
      "comunas - DATA TREATMENT 2.1 Joining _priv and _pub pois.\n",
      "comunas - DATA TREATMENT 2.2 Merging proximity and areal data.\n",
      "comunas - HQSL FUNCTION - 3.1 Processing variables analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [00:00<00:00, 388.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comunas - HQSL FUNCTION - 3.2 Saving variables analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comunas - HQSL FUNCTION - 4.1 Processing HQSL index.\n",
      "comunas - HQSL FUNCTION - 4.2 Saving HQSL index to database.\n",
      "--------------------------------------------------------------------------------\n",
      "--- RUNNING CALCULATION FOR AREA OF ANALYSIS: unidadesvecinales.\n",
      "unidadesvecinales - LOAD DATA 1.1 Loading proximity data.\n",
      "unidadesvecinales - LOAD DATA 1.2 Loading areal data.\n",
      "unidadesvecinales - DATA TREATMENT 2.1 Joining _priv and _pub pois.\n",
      "unidadesvecinales - DATA TREATMENT 2.2 Merging proximity and areal data.\n",
      "unidadesvecinales - HQSL FUNCTION - 3.1 Processing variables analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [00:03<00:00, 17.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidadesvecinales - HQSL FUNCTION - 3.2 Saving variables analysis.\n",
      "unidadesvecinales - HQSL FUNCTION - 4.1 Processing HQSL index.\n",
      "unidadesvecinales - HQSL FUNCTION - 4.2 Saving HQSL index to database.\n",
      "--------------------------------------------------------------------------------\n",
      "--- RUNNING CALCULATION FOR AREA OF ANALYSIS: alameda.\n",
      "alameda - LOAD DATA 1.1 Loading proximity data.\n",
      "alameda - LOAD DATA 1.2 Loading areal data.\n",
      "alameda - DATA TREATMENT 2.1 Joining _priv and _pub pois.\n",
      "alameda - DATA TREATMENT 2.2 Merging proximity and areal data.\n",
      "alameda - HQSL FUNCTION - 3.1 Processing variables analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [00:00<00:00, 1066.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alameda - HQSL FUNCTION - 3.2 Saving variables analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alameda - HQSL FUNCTION - 4.1 Processing HQSL index.\n",
      "alameda - HQSL FUNCTION - 4.2 Saving HQSL index to database.\n"
     ]
    }
   ],
   "source": [
    "for area_analysis in areas_of_analysis_list:\n",
    "    print(\"--\"*40)\n",
    "    print(f\"--- RUNNING CALCULATION FOR AREA OF ANALYSIS: {area_analysis}.\")\n",
    "\n",
    "    ##########################################################################################\n",
    "    # Define use of local data\n",
    "    if (area_analysis == 'alameda') or (area_analysis == 'hexagon'):\n",
    "        local_data = True\n",
    "    else:\n",
    "        local_data = False\n",
    "\n",
    "    ##########################################################################################\n",
    "    # STEP 1: LOAD DATA\n",
    "    # ------------------------------ 1.1 LOAD PROXIMITY DATA ------------------------------\n",
    "    # ------------------------------\n",
    "    print(f\"{area_analysis} - LOAD DATA 1.1 Loading proximity data.\")\n",
    "    # ------------------------------\n",
    "    # Select unique ID according to selected data\n",
    "    area_dict = {'unidadesvecinales':'COD_UNICO_',\n",
    "                'zonascensales':'GEOCODI',  \n",
    "                'alameda':'name',\n",
    "                'comunas':'Comuna',\n",
    "                'hex':'hex_id',\n",
    "                'santiago':'nom_region'}\n",
    "    code_column = area_dict[area_analysis]\n",
    "    \n",
    "    # Read selected data\n",
    "    if local_data:\n",
    "        if area_analysis == 'hexagon':\n",
    "            prox_gdf = gpd.read_file(grl_dir + f'santiago_hexproximity_{speed_name}_kmh_res{res}.gpkg')\n",
    "        elif area_analysis == 'alameda':\n",
    "            prox_gdf = gpd.read_file(grl_dir + f'santiago_alamedaproximity_{speed_name}_kmh.gpkg')\n",
    "    else:\n",
    "        if area_analysis == 'hex':\n",
    "            query = f'SELECT * FROM projects_research.santiago_hexproximity_{speed_name}_kmh WHERE res = {res}'\n",
    "            prox_gdf = aup.gdf_from_query(query)\n",
    "        elif area_analysis == 'unidadesvecinales':\n",
    "            prox_gdf = aup.gdf_from_db(f'santiago_unidadesvecinalesproximity_{speed_name}_kmh','projects_research')\n",
    "    \n",
    "        elif area_analysis == 'zonascensales':\n",
    "            prox_gdf = aup.gdf_from_db(f'santiago_zonascensalesproximity_{speed_name}_kmh','projects_research')\n",
    "        \n",
    "        elif area_analysis == 'comunas':\n",
    "            prox_gdf = aup.gdf_from_db(f'santiago_comunasproximity_{speed_name}_kmh','projects_research')\n",
    "        \n",
    "        elif area_analysis == 'santiago':\n",
    "            prox_gdf = aup.gdf_from_db(f'santiago_{area_analysis}proximity_{speed_name}_kmh','projects_research')\n",
    "        \n",
    "    # Change some col names\n",
    "    prox_gdf = prox_gdf.rename(columns={'paradas_tp_count_15min':'paradas_tp_ruta_count_15min',\n",
    "                                       'paradas_tp_time':'paradas_tp_ruta_time'})\n",
    "\n",
    "    # ------------------------------ 1.2 LOAD AREAL DATA ------------------------------\n",
    "    # ------------------------------\n",
    "    print(f\"{area_analysis} - LOAD DATA 1.2 Loading areal data.\")\n",
    "    # ------------------------------\n",
    "    if all_local_dirs == 'alex':\n",
    "        if area_analysis == 'hex':\n",
    "            hex_dir = f\"../../../data/processed/santiago/areal_data/{area_analysis}_areal_res{res}.gpkg\"\n",
    "        else:\n",
    "            hex_dir = f\"../../../data/processed/santiago/areal_data/{area_analysis}_areal.gpkg\"\n",
    "            \n",
    "    elif all_local_dirs == 'edgar':\n",
    "        if area_analysis == 'hex':\n",
    "            hex_dir = f'../../../data/processed/00_pois_formated/aereal_data/{area_analysis}_areal_res{res}.gpkg'\n",
    "        else:\n",
    "            hex_dir = f'../../../data/processed/00_pois_formated/aereal_data/{area_analysis}_areal.gpkg'\n",
    "    \n",
    "    hex_areal = gpd.read_file(hex_dir)\n",
    "    hex_areal = hex_areal.rename(columns={'oficinas_sum':'oficinas_count',\n",
    "                                         'pct_social_viv':'social_viv_count',\n",
    "                                         'viv_sum':'houses_count',\n",
    "                                         'pct_hotel':'hotel_count',\n",
    "                                         'ndvi_mean':'ndvi_count'})\n",
    "    \n",
    "    ##########################################################################################\n",
    "    # STEP 2 : DATA TREATMENT\n",
    "    # ------------------------------ 2.1 JOIN _priv AND _pub POIS (SOME) ------------------------------\n",
    "    # ------------------------------\n",
    "    print(f\"{area_analysis} - DATA TREATMENT 2.1 Joining _priv and _pub pois.\")\n",
    "    # ------------------------------\n",
    "    join_pois_list = ['hospital','clinica','consult_ado', 'museos',\n",
    "                     'vacunatorio','eq_deportivo',]\n",
    "    \n",
    "    for source in join_pois_list:\n",
    "        # join count columns for private and public in one encompassing column\n",
    "        prox_gdf[f\"{source}_count_15min\"] = prox_gdf[f\"{source}_priv_count_15min\"] + prox_gdf[f\"{source}_pub_count_15min\"]\n",
    "        # remove 0 values from time\n",
    "        prox_gdf.loc[prox_gdf[f\"{source}_pub_time\"]==0] = np.nan\n",
    "        prox_gdf.loc[prox_gdf[f\"{source}_priv_time\"]==0] = np.nan\n",
    "        # assign general minimum time\n",
    "        prox_gdf[f\"{source}_time\"] = prox_gdf[[f\"{source}_pub_time\", f\"{source}_priv_time\"]].min(axis=1)\n",
    "        # remove duplicate info columns\n",
    "        prox_gdf = prox_gdf.drop(columns=[f\"{source}_pub_count_15min\", f\"{source}_priv_count_15min\",\n",
    "                                         f\"{source}_pub_time\", f\"{source}_priv_time\"])\n",
    "        # fill na with 0 for future processing\n",
    "        prox_gdf['hospital_time'].fillna(0, inplace=True)\n",
    "\n",
    "    # ------------------------------ 2.2 MERGE PROXIMITY AND AREAL DATA ------------------------------\n",
    "    # ------------------------------\n",
    "    print(f\"{area_analysis} - DATA TREATMENT 2.2 Merging proximity and areal data.\")\n",
    "    # ------------------------------\n",
    "    hex_analysis = hex_areal.merge(prox_gdf.drop(columns='geometry'), on=code_column, how='left')\n",
    "    hex_analysis = hex_analysis.explode(ignore_index=True)\n",
    "    hex_analysis = hex_analysis.dissolve(by=code_column)\n",
    "    hex_analysis = hex_analysis.reset_index()\n",
    "\n",
    "    ##########################################################################################\n",
    "    # STEP 3 : HQSL FUNCTION - Step 1/2: Variables analysis\n",
    "    # ------------------------------ 3.1 USE PREVIOUSLY DEFINED SCALE FUNCTIONS ------------------------------\n",
    "    # ------------------------------\n",
    "    print(f\"{area_analysis} - HQSL FUNCTION - 3.1 Processing variables analysis.\")\n",
    "    # ------------------------------\n",
    "    # use scale functions for each column\n",
    "    for i in tqdm(range(len(weight_dict.keys())),position=0,leave=True):\n",
    "        # gather specific source\n",
    "        source = list(weight_dict.keys())[i]\n",
    "        # iterate over columns\n",
    "        for col_name in hex_analysis.columns:\n",
    "            # select column with count information -- refers to the amount of opportunities available at 15 min\n",
    "            if source in col_name and 'count' in col_name:\n",
    "                if f'{source}_time' in hex_analysis.columns:\n",
    "                    hex_analysis[f'{source}_time'].fillna(0, inplace=True)\n",
    "                hex_analysis[col_name].fillna(0, inplace=True)\n",
    "                # source scaling\n",
    "                hex_analysis[f'{source}_scaled'] = hex_analysis[col_name].apply(lambda x:\n",
    "                                                                                scale_source_fn(\n",
    "                                                                                    x,\n",
    "                                                                                source,\n",
    "                                                                                    weight_dict, \n",
    "                                                                    hex_analysis[col_name].mean(),\n",
    "                                                                    hex_analysis[col_name].std()))\n",
    "                # treat 0 time values -- hexagons without nodes \n",
    "                if area_analysis == 'hex':\n",
    "                    if weight_dict[source] != 'specific':\n",
    "                        # assign nan values to hexagons without nodes to avoid affecting the mean calculation process\n",
    "                        #if source in join_pois_list:\n",
    "                        #    hex_analysis.loc[hex_analysis.supermercado_time==0,f'{source}_scaled'] = np.nan\n",
    "                        if source == 'hotel' or source == 'oficinas':\n",
    "                            continue\n",
    "                        else:\n",
    "                            hex_analysis.loc[hex_analysis[f'{source}_time']==0,f'{source}_scaled'] = np.nan\n",
    "                            \n",
    "                        # calculate mean count value\n",
    "                        # print(source)\n",
    "                        hex_analysis.loc[hex_analysis[f'{source}_time']==0, f'{source}_scaled'] = hex_analysis.loc[hex_analysis[f'{source}_time']==0].apply(lambda x: neighbour_mean(x['hex_id'],\n",
    "                                                                                'hex_id',\n",
    "                                                                                hex_analysis,\n",
    "                                                                                f'{source}_scaled'), axis=1)\n",
    "    # ------------------------------ 3.2 SAVE ------------------------------\n",
    "    if upload_preprocessed:\n",
    "        # ------------------------------\n",
    "        print(f\"{area_analysis} - HQSL FUNCTION - 3.2 Saving variables analysis.\")\n",
    "        # ------------------------------\n",
    "        hex_preprocessed = hex_analysis.copy()\n",
    "        if area_analysis == 'hex':\n",
    "            hex_preprocessed['res'] = res\n",
    "        hex_preprocessed = hex_preprocessed.dropna()\n",
    "        table = f'santiago_{area_analysis}variableanalysis_{speed_name}_kmh'\n",
    "        schema = 'projects_research'\n",
    "        aup.gdf_to_db_slow(hex_preprocessed, table, schema, if_exists=if_exists)\n",
    "        del hex_preprocessed\n",
    "\n",
    "    ##########################################################################################\n",
    "    # STEP 4 : HQSL FUNCTION - Step 2/2: HQSL Index calculation\n",
    "    # ------------------------------ 4.1 USE PREVIOUSLY DEFINED INDEX FUNCTIONS ------------------------------\n",
    "    # ------------------------------\n",
    "    print(f\"{area_analysis} - HQSL FUNCTION - 4.1 Processing HQSL index.\")\n",
    "    # ------------------------------\n",
    "    hex_ind = indicator_fn(hex_analysis, parameters_dict)\n",
    "    hex_social_fn = social_fn(hex_analysis, parameters_dict)\n",
    "    hex_hqsl = hqsl_fn(hex_social_fn, parameters_dict)\n",
    "    \n",
    "    hex_idx = hex_ind.merge(hex_social_fn.drop(columns='geometry'), on=code_column)\n",
    "    hex_idx = hex_idx.merge(hex_hqsl.drop(columns='geometry'), on=code_column)\n",
    "\n",
    "    # ------------------------------ 4.2 SAVE ------------------------------\n",
    "    if area_analysis == 'hex':\n",
    "        hex_idx['res'] = res\n",
    "    \n",
    "    hex_idx = hex_idx.dropna()\n",
    "    \n",
    "    if save:\n",
    "        # ------------------------------\n",
    "        print(f\"{area_analysis} - HQSL FUNCTION - 4.2 Saving HQSL index to database.\")\n",
    "        # ------------------------------\n",
    "        table = f'santiago_{area_analysis}analysis_{speed_name}_kmh'\n",
    "        schema = 'projects_research'\n",
    "        aup.gdf_to_db_slow(hex_idx, table, schema, if_exists=if_exists)\n",
    "    if save_local:\n",
    "        # ------------------------------\n",
    "        print(f\"{area_analysis} - HQSL FUNCTION - 4.2 Saving HQSL index locally.\")\n",
    "        # ------------------------------\n",
    "        hex_idx.to_file(grl_dir + f'santiago_{area_analysis}analysis_{speed_name}_kmh.geojson')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDS-10.0",
   "language": "python",
   "name": "gds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
