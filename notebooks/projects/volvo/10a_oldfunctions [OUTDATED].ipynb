{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27190f5d-7230-4eac-b767-8a6bd28b5571",
   "metadata": {},
   "source": [
    "## This notebook is used to __store variations of functions__, so they are available to check in case other solutions are required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0b55c-3a64-45f0-983b-99fc4a1d6cf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2025/03/04 - Function identify_uncovered() from 01_PL_04_Combine_networks [vref repository] before update that considers overlapping_test. Also, shortens edges by 0.5size instead of 0.9size (Latest version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65750403-2057-49a9-a8bf-63786300771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_uncovered(base_nodes, base_edges, complementary_nodes, complementary_edges, contact_analysis_dist, projected_crs=\"EPSG:6372\"):\n",
    "    \"\"\" This function identifies zones within a complementary network (nodes and edges) where currently there's no coverture in a base network.\n",
    "\tArgs:\n",
    "\t\tbase_nodes (geopandas.GeoDataFrame): GeoDataFrame containing nodes of the base network. \n",
    "        base_edges (geopandas.GeoDataFrame): GeoDataFrame containing edges of the base network. \n",
    "\t\tcomplementary_nodes  (geopandas.GeoDataFrame): GeoDataFrame containing nodes of the complementary network.\n",
    "\t\tcomplementary_edges  (geopandas.GeoDataFrame): GeoDataFrame containing edges of the complementary network.\n",
    "\t\tcontact_analysis_dist (float): Distance (meters) used when deciding which nodes from the complementary network should be added to the base network.\n",
    "                                A buffer of {contact_analysis_dist} is created around all center points of each complementary_edge.\n",
    "                                If the buffer touches any base_edges, the complementary_edge is considered as already covered by the base network. \n",
    "                                If the buffer does not touches any base_edge, the complementary_edge is considered uncovered.\n",
    "\t\tprojected_crs (str, optional): string containing projected crs to be used depending on area of interest. Defaults to \"EPSG:6372\".\n",
    "\n",
    "\tReturns:\n",
    "        complementary_uncovered_nodes (geopandas.GeoDataFrame): GeoDataFrame with nodes from the complementary network that are located \n",
    "                                                                in a zone not covered by the base network.\n",
    "        complementary_uncovered_nodes (geopandas.GeoDataFrame): GeoDataFrame with edges from the complementary network that are located\n",
    "                                                                in a zone not covered by the base network.\n",
    "\t\tcontact_nodes (geopandas.GeoDataFrame): GeoDataFrame with nodes from the complementary network that could be used to \n",
    "                                                connect an uncovered zone to a covered zone.\n",
    "\t\"\"\"\n",
    "\n",
    "    # Turn on or off function logs (General Logs)\n",
    "    function_logs = True\n",
    "    # Turn on or off debbugging logs (Lots of logs used for debugging)\n",
    "    debugging_logs = False\n",
    "    \n",
    "    # 1.0 --------------- Extract mid_point of each complementary edge\n",
    "    if function_logs:\n",
    "        print(\"1.0 - Extracting mid_point point of each complementary edge.\")\n",
    "    # ------------------- INPUT USED - READ COMPLEMENTARY EDGES\n",
    "    complementary_edges = complementary_edges.copy()\n",
    "    complementary_edges = complementary_edges.to_crs(projected_crs)\n",
    "    # ------------------- INPUT USED - READ COMPLEMENTARY EDGES\n",
    "\n",
    "    # Create unique ID for each edge using u+v+key\n",
    "    complementary_edges = src.create_unique_edge_id(complementary_edges)\n",
    "    # Find mid_point of each edge\n",
    "    complementary_edges['mid_point'] = complementary_edges.interpolate(complementary_edges.length / 2)\n",
    "    # Assign mid_point to its own gdf and drop column 'mid_point' from complementary_edges\n",
    "    mid_points = complementary_edges[['edge_id','mid_point']].copy()\n",
    "    mid_points.rename(columns={'mid_point':'geometry'},inplace=True)\n",
    "    complementary_edges.drop(columns=['mid_point'],inplace=True)\n",
    "    \n",
    "    # 2.0 --------------- Create contact-analysis buffer around mid_points using contact_analysis_dist \n",
    "    # ------------------- (keep edge-of-origin data)\n",
    "    if function_logs:\n",
    "        print(\"2.0 - Creating contact-analysis buffer around each mid_point.\")\n",
    "    \n",
    "    # Reset mid_points's index (Keeps data ordered starting from 0)\n",
    "    mid_points.reset_index(inplace=True,drop=True) #--> Resets index without saving col 'index'\n",
    "    # Save each mid_point's reseted index in a column named 'index'\n",
    "    points_to_buffer = mid_points.copy()\n",
    "    points_to_buffer.reset_index(inplace=True) #--> Also creates a col 'index', same order since it is reseted\n",
    "    # Create a gdf containing the contact-analysis buffer around mid_points\n",
    "    mid_points_buffer = points_to_buffer.buffer(contact_analysis_dist)\n",
    "    mid_points_buffer = gpd.GeoDataFrame(geometry=mid_points_buffer)\n",
    "    mid_points_buffer.reset_index(inplace=True) #--> Also creates a col 'index', same order since it is reseted\n",
    "    # Transfer data from mid_points to it's buffer using the previously reseted index as merge col\n",
    "    points_to_buffer.drop(columns=['geometry'],inplace=True)\n",
    "    mid_points_buffer = pd.merge(mid_points_buffer,points_to_buffer,on='index') #--> Merges using common reseted col 'index'\n",
    "    mid_points_buffer.drop(columns=['index'],inplace=True)\n",
    "\n",
    "    # Save disk space\n",
    "    del points_to_buffer\n",
    "    \n",
    "    # 3.0 --------------- Find mid_points whose buffer does not intersect with any part of the base network (Considering base_edges).\n",
    "    # ------------------- [This step creates function output COMPLEMENTARY_UNCOVERED_EDGES]\n",
    "    if function_logs:\n",
    "        print(\"3.0 - Extracting complementary_uncovered_edges.\")\n",
    "    \n",
    "    # ------------------- INPUT USED - READ BASE EDGES\n",
    "    base_edges = base_edges.copy()\n",
    "    base_edges = base_edges.to_crs(projected_crs)\n",
    "    # ------------------- INPUT USED - READ BASE EDGES\n",
    "    \n",
    "    # Buffers that touch any base edge\n",
    "    buffer_touch = mid_points_buffer.sjoin(base_edges)\n",
    "    # All unique complementary edge_ids whose mid_point's buffer touched any base_edge\n",
    "    edge_id_touch_lst = list(buffer_touch.edge_id.unique())\n",
    "    # Complementary edges that are NOT(~) near any base edge\n",
    "    complementary_uncovered_edges = complementary_edges.loc[~complementary_edges.edge_id.isin(edge_id_touch_lst)].copy()\n",
    "    complementary_uncovered_edges.reset_index(inplace=True,drop=True) #--> Resets index without saving col 'index'\n",
    "\n",
    "    # 4.0 --------------- Select the complementary_nodes that connect to the complementary_uncovered_edges\n",
    "    # ------------------- [This step creates function output COMPLEMENTARY_UNCOVERED_NODES]\n",
    "    if function_logs:\n",
    "        print(\"4.0 - Extracting complementary_uncovered_nodes.\")\n",
    "    \n",
    "    # ------------------- INPUT USED - READ COMPLEMENTARY NODES\n",
    "    complementary_nodes = complementary_nodes.copy()\n",
    "    complementary_nodes = complementary_nodes.to_crs(projected_crs)\n",
    "    # ------------------- INPUT USED - READ COMPLEMENTARY NODES \n",
    "    \n",
    "    # List of unique 'u's and 'v's that are connected to the complementary_uncovered_edges \n",
    "    complementary_uncovered_osmid_lst = set(list(complementary_uncovered_edges.u.unique()) + list(complementary_uncovered_edges.v.unique()))\n",
    "    # Select any node where its 'osmid' IS in complementary_uncovered_osmid_lst\n",
    "    complementary_uncovered_nodes = complementary_nodes.loc[complementary_nodes.osmid.isin(complementary_uncovered_osmid_lst)].copy()\n",
    "    # [Note: This nodes won't necessarily be in the uncovered zone since they could belong to \n",
    "    # an edge whose mid_point is far from the base network, but whose path extends into the base network.]\n",
    "\n",
    "    # 5.0 --------------- Find the nodes that would be used to connect the uncovered part of the complementary network to the base network.\n",
    "    # ------------------- [This step creates function output CONTACT_NODES]\n",
    "    if function_logs:\n",
    "        print(\"5.0 - Extracting contact_nodes.\")\n",
    "\n",
    "    # 5.1 --- Create a buffer around all complementary_uncovered_nodes \n",
    "    # Reset complementary_uncovered_nodes's index\n",
    "    complementary_uncovered_nodes.reset_index(inplace=True,drop=True) #--> Resets index without saving col 'index'\n",
    "    # Save each complementary_uncovered_nodes's reseted index in a column named 'index'\n",
    "    nodes_to_buffer = complementary_uncovered_nodes.copy()\n",
    "    nodes_to_buffer.reset_index(inplace=True) #--> Also creates a col 'index', same order since it is reseted\n",
    "    # Create a gdf containing the buffer around complementary_uncovered_nodes\n",
    "    complementary_uncovered_nodes_buffer = nodes_to_buffer.buffer(contact_analysis_dist)\n",
    "    complementary_uncovered_nodes_buffer = gpd.GeoDataFrame(geometry=complementary_uncovered_nodes_buffer)\n",
    "    complementary_uncovered_nodes_buffer.reset_index(inplace=True) #--> Also creates a col 'index', same order since it is reseted\n",
    "    # Transfer data from complementary_uncovered_nodes to it's buffer using the index as merge col\n",
    "    nodes_to_buffer.drop(columns=['geometry'],inplace=True)\n",
    "    complementary_uncovered_nodes_buffer = pd.merge(complementary_uncovered_nodes_buffer,nodes_to_buffer,on='index') #--> Merges using common reseted col 'index'\n",
    "    complementary_uncovered_nodes_buffer.drop(columns=['index'],inplace=True)\n",
    "\n",
    "    # Save disk space\n",
    "    del nodes_to_buffer\n",
    "    \n",
    "    # 5.2 --- Find complementary_uncovered_nodes whose buffer DOES intersect with any part of the base network (Considering base_edges).\n",
    "    # Buffers that touch any base edge\n",
    "    buffer_touch = complementary_uncovered_nodes_buffer.sjoin(base_edges)\n",
    "    # All unique osmids that touched any base_edge\n",
    "    contact_osmids = list(buffer_touch.osmid.unique())\n",
    "    # Complementary nodes that ARE near any base_edge\n",
    "    contact_nodes = complementary_uncovered_nodes.loc[complementary_uncovered_nodes.osmid.isin(contact_osmids)].copy()\n",
    "    contact_nodes.reset_index(inplace=True,drop=True) #--> Resets index without saving col 'index'\n",
    "\n",
    "    # 6.0 --------------- Identify and shorten edges that extend into the base network and would be usefull to create connections.\n",
    "    # ------------------- (Up to this step, edges whose mid_points_buffer is in contact with the base network are not included in the\n",
    "    # ------------------- function's output since they are considered to be in an already-covered zone. However, some edges (if shortened)\n",
    "    # ------------------- could be usefull to connect the uncovered zone to the covered zone. This step takes the edge and shortens (Clips)\n",
    "    # ------------------- the edge until it's mid_point_buffer is no longer in contact with the base network)\n",
    "    # ------------------- [This step updates the uncovered network (nodes and edges) and the contact nodes]\n",
    "    if function_logs:\n",
    "        print(\"6.0 - Creating missing connections through complementary_edges that travel from the uncovered zone to the base network.\")\n",
    "\n",
    "    # PREPARATION FOR ANALYSIS:\n",
    "\n",
    "    # Keep track of the amount of edges that underwent a shortening process\n",
    "    fabricated_count = 0\n",
    "\n",
    "    # LOG CODE - Progress logs\n",
    "    # Will create progress logs when progress reaches these percentages:\n",
    "    progress_logs = [0,10,20,30,40,50,60,70,80,90,100] # for log statistics\n",
    "    osmid_count = 0\n",
    "    # LOG CODE - Progress logs\n",
    "\n",
    "    # Create empty GeoDataFrame to store all original_diverging_nodes and original_diverging_edges (Used for GIS visualization purposes)\n",
    "    original_diverging_nodes = gpd.GeoDataFrame()\n",
    "    original_diverging_edges = gpd.GeoDataFrame()\n",
    "    \n",
    "    # Find all complementary_uncovered_osmids (from previously created complementary_uncovered_osmid_lst)\n",
    "    # that are NOT a contact osmid (contact osmids are those that already serve as a connection to the base network)\n",
    "    non_contact_osmids = [osmid for osmid in complementary_uncovered_osmid_lst if osmid not in contact_osmids]\n",
    "    \n",
    "    # Previously produced osmid. Since will be creating non-existing nodes, function produce_osmid() will use a starting number for\n",
    "    # trying to produce unique osmids. That function will check if that osmid already exists in either the base or complementary network.\n",
    "    # Start with number 0.\n",
    "    previously_produced = 0\n",
    "\n",
    "    # Read base_nodes once (will be used to assess if the osmid being produced is unique to both input networks)\n",
    "    # ------------------- INPUT USED - READ BASE EDGES\n",
    "    base_nodes = base_nodes.copy()\n",
    "    base_nodes = base_nodes.to_crs(projected_crs)\n",
    "    # ------------------- INPUT USED - READ BASE EDGES\n",
    "\n",
    "    # Keep track of which parts of the network where fabricated with the following code.\n",
    "    # All edges that keep its original geometry will be assigned 'clipping_i' = 0.\n",
    "    # All edges whose geometry was clipped will be assigned the amount of shortening (clipping) iterations used in them.\n",
    "    complementary_uncovered_nodes['clipping_i'] = 0\n",
    "    complementary_uncovered_edges['clipping_i'] = 0\n",
    "    complementary_uncovered_edges['original_edge_id'] = np.nan\n",
    "    contact_nodes['clipping_i'] = 0\n",
    "\n",
    "    # Shortening dict\n",
    "    # Sometimes an edge could get shortened from both sides. \n",
    "    # (Clipped with starting point 'u' and then, on another case, clipped with starting point 'u')\n",
    "    # If an edge will be shortened from both sides exactly once (Shortened from 'u' to midpoint and from 'v' to midpoint),\n",
    "    # there would be two different new nodes in the same place. The dict helps make sure that only one contact_node is created.\n",
    "    # If not considered, this particular situation can create two different edges that coincide in two different contact_nodes exactly in the midpoint.\n",
    "    # (This is the main reason why it is necessary to keep track of which edges where shortened and up to which point)\n",
    "    shortening_dict = {}\n",
    "    \n",
    "    # Review each node in the uncovered zone of the complementary network that is not already a contact_node\n",
    "    # (Previously created non_contact_osmids)\n",
    "    for osmid in non_contact_osmids:\n",
    "\n",
    "        # Development check\n",
    "        #if osmid != 436813694931:\n",
    "        #    continue\n",
    "        #else:\n",
    "        #    print(osmid)\n",
    "        \n",
    "        # LOG CODE - Progress logs\n",
    "        # Measures current progress, prints if passed a checkpoint of progress_logs list.\n",
    "        current_progress = (osmid_count / len(non_contact_osmids))*100\n",
    "        for checkpoint in progress_logs:\n",
    "            if (current_progress >= checkpoint) and function_logs:\n",
    "                print(f\"6.0 - Exploring osmids. {checkpoint}% done.\")\n",
    "                progress_logs.remove(checkpoint)\n",
    "                break\n",
    "        # LOG CODE - Progress logs\n",
    "\n",
    "        # Retrieve it's edges (Will be refered as diverging_edges). \n",
    "        # Must consider all edges (complementary_edges) and not only complementary_uncovered_edges since\n",
    "        # since it's looking to identify if an edge that comes out of that osmid goes towards base_network.\n",
    "        diverging_edges = complementary_edges.loc[(complementary_edges.u==osmid) | (complementary_edges.v==osmid)].copy()\n",
    "        diverging_edges_ids = list(diverging_edges.edge_id.unique())\n",
    "        \n",
    "        # For each edge diverging from current node:\n",
    "        for diverging_edge_id in diverging_edges_ids:\n",
    "            # If the edge DOES touch the base network:\n",
    "            if diverging_edge_id in edge_id_touch_lst: # (Previously created edge_id_touch_lst)\n",
    "                # If an edge reaches this part of the code, it means that it is a complementary_edge that:\n",
    "                # a) Comes out from an node that's located in the uncovered zone (complementary_uncovered_node).\n",
    "                # b) The node it came out from is NOT a contact_node, it is far from the base network (According to contact_analysis_dist)\n",
    "                # c) The edge's current mid_point is located in proximity to the base network (According to contact_analysis_dist)\n",
    "                # --> Between the base_network and this edge itself, a connection point should be identified.\n",
    "                # --> Objective: Identify that connection (new contact_node) between the complementary and base network.\n",
    "\n",
    "                # 6.1 --- Retrieve current diverging_osmid and diverging_node\n",
    "                # Save the osmid from the node which the current edge uses to come out from the uncovered zone into the base network.\n",
    "                diverging_osmid = osmid\n",
    "                if debugging_logs:\n",
    "                    print(f\"Diverging osmid: {diverging_osmid}.\")\n",
    "                # Extract its node\n",
    "                diverging_node = complementary_uncovered_nodes.loc[complementary_uncovered_nodes.osmid == diverging_osmid].copy()\n",
    "                # Reset index (so that accessing its geometry is always .loc[0,'geometry'])\n",
    "                diverging_node.reset_index(inplace=True, drop=True)\n",
    "                if debugging_logs:\n",
    "                    print(f\"Diverging node's geometry: {diverging_node.loc[0,'geometry']}.\")\n",
    "                # Add current diverging_node to original_diverging_nodes gdf (For GIS visualization purposes)\n",
    "                original_diverging_nodes = pd.concat([original_diverging_nodes, diverging_node])\n",
    "\n",
    "                # 6.2 --- Retrieve current diverging_edge as the connection_edge\n",
    "                # Select the edge that diverts from the diverging_node towards the base_network\n",
    "                connection_edge = complementary_edges.loc[complementary_edges.edge_id == diverging_edge_id].copy()\n",
    "                # Reset index (so that accessing its data is always .loc[0,'data'])\n",
    "                connection_edge.reset_index(inplace=True,drop=True)\n",
    "                # The connection_edge geometry will suffer modifications, save original\n",
    "                original_connection_edge = connection_edge.copy() \n",
    "                # Add current connection_edge to original_diverging_edges gdf (For GIS visualization purposes)\n",
    "                original_diverging_edges = pd.concat([original_diverging_edges, connection_edge])\n",
    "\n",
    "                # 6.3 --- Clip connection_edge until it's mid_point is no longer in proximity to the base network.\n",
    "                # ------- When this point is reached, assign its ending_point (previous mid_point) as a new contact_node.\n",
    "            \n",
    "                # Kickstart while loop for current connection_edge\n",
    "                stop = False\n",
    "                shorten_i = 0\n",
    "                \n",
    "                while (stop == False):\n",
    "\n",
    "                    # Limit of attempts\n",
    "                    limit_of_attempts = 20\n",
    "                    if shorten_i >= limit_of_attempts:\n",
    "                        print(f\"Tried shortening {limit_of_attempts} times edge u {original_connection_edge.u.unique()[0]} and v {original_connection_edge.v.unique()[0]}. Stopped.\")\n",
    "                        break\n",
    "                        \n",
    "                    # 6.3.1 --- Calculate the edge's mid_point.\n",
    "                    # --------- [Will become the clipping_point in function edge_clipping()].\n",
    "                \n",
    "                    # Calculate the connection_edge's mid_point\n",
    "                    connection_edge['mid_point'] = connection_edge.interpolate(connection_edge.length / 2)\n",
    "                    # Assign mid_point to its own gdf and drop column 'mid_point' from connection_edge gdf\n",
    "                    edge_mid_point = connection_edge[['edge_id','mid_point']].copy()\n",
    "                    edge_mid_point.rename(columns={'mid_point':'geometry'},inplace=True)\n",
    "                    connection_edge.drop(columns=['mid_point'],inplace=True)\n",
    "                \n",
    "                    # 6.3.2 --- Evaluate if the current mid_point is still in proximity to the base network\n",
    "                    # 6.3.2.a - Create contact-analysis buffer around the edge_mid_point\n",
    "                    # Reset edge_mid_point's index\n",
    "                    edge_mid_point.reset_index(inplace=True,drop=True) #--> Resets index without saving col 'index'\n",
    "                    # Save edge_mid_point's reseted index in a column named 'index'\n",
    "                    point_to_buffer = edge_mid_point.copy()\n",
    "                    point_to_buffer.reset_index(inplace=True) #--> Also creates a col 'index', same order since it is reseted\n",
    "                    # Create a gdf containing the contact-analysis buffer around the edge_mid_point\n",
    "                    mid_point_buffer = point_to_buffer.buffer(contact_analysis_dist)\n",
    "                    mid_point_buffer = gpd.GeoDataFrame(geometry=mid_point_buffer)\n",
    "                    mid_point_buffer.reset_index(inplace=True) #--> Also creates a col 'index', same order since it is reseted\n",
    "                    # Transfer data from edge_mid_point to it's buffer using the previously reseted index as merge col\n",
    "                    point_to_buffer.drop(columns=['geometry'],inplace=True)\n",
    "                    mid_point_buffer = pd.merge(mid_point_buffer,point_to_buffer,on='index') #--> Merges using common reseted col 'index'\n",
    "                    mid_point_buffer.drop(columns=['index'],inplace=True)\n",
    "                    \n",
    "                    # 6.3.2.b - Find if current mid_point it's still in proximity to base network\n",
    "                    # Buffers that touch any base edge\n",
    "                    buffer_touch = mid_point_buffer.sjoin(base_edges)\n",
    "                    \n",
    "                    if len(buffer_touch) > 0:\n",
    "                        \n",
    "                        # 6.3.3 --- If it still touches, reduce in size.\n",
    "                        # If the mid_point_buffer still touches the base edges, the line is most likely still overlaping with the base network.\n",
    "                        # Apply edge_clipping function to shorten the edge up until the current mid_point\n",
    "                        connection_edge = edge_clipping(starting_point_gdf = diverging_node,\n",
    "                                                        edge_gdf = connection_edge,\n",
    "                                                        clipping_point_gdf = edge_mid_point,\n",
    "                                                        projected_crs = projected_crs)\n",
    "                        # Keep connection_edge format\n",
    "                        connection_edge['edge_id'] = diverging_edge_id\n",
    "                        # Count shortening iteration\n",
    "                        shorten_i+=1\n",
    "                    \n",
    "                    else:\n",
    "                        # 6.3.3 --- If the buffer no longer touches any part of the base network, shortening was a success.\n",
    "                        # --------- However, before registering the shortened edge it it necessary to run a test:\n",
    "                        \n",
    "                        # Double-once-shortening test STARTS ### ### ### ### ### ### ### ### ### ### ### ### \n",
    "                        # EXPLANATION:\n",
    "                        # A complementary_edge could be shortened twice, once from each starting_point. This is a \"double shortening\".\n",
    "                        # It occurs when an edge was shortened from node 1 (e.g. from 'u') and now is being shortened from node 2 (e.g. from 'v').\n",
    "                        # This process produces no problems, **unless both shortening processess undergo just 1 iteration**.\n",
    "                        # In that specific case, both lines start in their nodes ('u' or 'v') and end at the original line's midpoint.\n",
    "                        # This produces TWO different contact_nodes in a very similar location, that could result in TWO similar but different treatments and strange geometries.\n",
    "                        # In order to avoid this, this double_shortening check is conduced in order to make sure that \n",
    "                        # at the end of both processes, just ONE contact node is produced and used by both edges ending on it.\n",
    "\n",
    "                        # If this is NOT the first fabrication case and edge AND the current edge was shortened ONLY once:\n",
    "                        if (fabricated_count > 0) and (shorten_i==1):\n",
    "    \n",
    "                            # Load all original edge_ids that have been shortened ONLY once\n",
    "                            already_shortened_once = complementary_uncovered_edges.loc[complementary_uncovered_edges.clipping_i==1].copy()\n",
    "                            already_shortened_once_lst = list(already_shortened_once.original_edge_id.unique())\n",
    "\n",
    "                            # Check if current diverging_edge_id has already been shortened ONLY once\n",
    "                            if diverging_edge_id in already_shortened_once_lst:\n",
    "                                # If an edge reaches this part of the code, the edge was already shortened ONCE from one end, and was being shortened again ONCE from the other end.\n",
    "                                # Next step --> Do NOT produce a new contact osmid and new point for current edge. Use the existing one. \n",
    "                                # ------------> Register the edge under the existing contact_node instead of creating a new one.\n",
    "\n",
    "                                if debugging_logs:\n",
    "                                    print(f\"Edge {diverging_edge_id} was already shortened [once]. Shortening [once] again from osmid {diverging_osmid}.\")\n",
    "\n",
    "                                # 6.3.3.a1 - Identify the previously produced middle osmid.\n",
    "                                # Retrieve the ORIGINAL (current original_connection_edge's) 'u' and 'v'\n",
    "                                original_u = original_connection_edge.u.unique()[0]\n",
    "                                original_v = original_connection_edge.v.unique()[0]\n",
    "                                original_osmids = [original_u, original_v]\n",
    "                                if debugging_logs:\n",
    "                                    print(f\"Edge's original osmids (u,v) were {original_osmids}.\")\n",
    "                                \n",
    "                                # Retrieve the PREVIOUSLY PRODUCED (previously shortened edge) 'u' and 'v'\n",
    "                                prev_produced_edge = complementary_uncovered_edges.loc[complementary_uncovered_edges.original_edge_id == diverging_edge_id].copy()\n",
    "                                new_u = prev_produced_edge.u.unique()[0]\n",
    "                                new_v = prev_produced_edge.v.unique()[0]\n",
    "                                new_osmids = [new_u, new_v]\n",
    "                                if debugging_logs:\n",
    "                                    print(f\"The first shortening is using osmids (u,v) {new_osmids}.\")\n",
    "                                \n",
    "                                # Identify which osmid is in the new_osmids list but not in the original_osmids list.\n",
    "                                # (Which osmid was produced here, in step 6.3.3)\n",
    "                                for osmid_check in new_osmids:\n",
    "                                    if osmid_check not in original_osmids:\n",
    "                                        produced_osmid = osmid_check\n",
    "\n",
    "                                # 6.3.3.a2 - Add the connection_edge as a new edge to complementary_uncovered_edges gdf (But not the node, the node is already there).\n",
    "                                # Retrieve the diverging_osmid's position in the ORIGINAL connection_edge. (Whether 'u' or 'v')\n",
    "                                # In order to keep that original diverging_node in its position and add the produced_osmid in the other position.\n",
    "                                if diverging_osmid == original_u:\n",
    "                                    connection_edge_u = diverging_osmid\n",
    "                                    connection_edge_v = produced_osmid\n",
    "                                    if debugging_logs:\n",
    "                                        print(f\"The new shortening will use osmids (u,v) = {[connection_edge_u,connection_edge_v]}.\")\n",
    "                                elif diverging_osmid == original_v:\n",
    "                                    connection_edge_u = produced_osmid\n",
    "                                    connection_edge_v = diverging_osmid\n",
    "                                    if debugging_logs:\n",
    "                                        print(f\"The new shortening will use osmids (u,v) = {[connection_edge_u,connection_edge_v]}.\")\n",
    "                                else:\n",
    "                                    print(f\"ERROR while trying to set 'u' and 'v' for shortened connection edge {diverging_edge_id}.\")\n",
    "                                    print(f\"The new shortening was trying to use osmids (u,v) = {[connection_edge_u,connection_edge_v]}.\")\n",
    "                                    intended_crash\n",
    "                                # Use the same key that was used in the original_connection_edge\n",
    "                                connection_edge_key = original_connection_edge.key.unique()[0]\n",
    "                                # Retrieve the modified edge's (connection_edge's) geometry\n",
    "                                connection_edge_geom = connection_edge['geometry'].unique()[0]\n",
    "                                # Find current last position\n",
    "                                iloc_edge = len(complementary_uncovered_edges)\n",
    "                                # Register new edge\n",
    "                                complementary_uncovered_edges.loc[iloc_edge+1,'u'] = connection_edge_u\n",
    "                                complementary_uncovered_edges.loc[iloc_edge+1,'v'] = connection_edge_v\n",
    "                                complementary_uncovered_edges.loc[iloc_edge+1,'key'] = connection_edge_key\n",
    "                                complementary_uncovered_edges.loc[iloc_edge+1,'geometry'] = connection_edge_geom\n",
    "                                complementary_uncovered_edges.loc[iloc_edge+1,'edge_id'] = str(connection_edge_u)+str(connection_edge_v)+str(connection_edge_key)\n",
    "                                complementary_uncovered_edges.loc[iloc_edge+1,'original_edge_id'] = diverging_edge_id\n",
    "                                complementary_uncovered_edges.loc[iloc_edge+1,'clipping_i'] = shorten_i\n",
    "\n",
    "                                # 6.3.3.a3 - Finished registering. Stop while loop. Continue with the next diverging_edge_id of current osmid.\n",
    "                                if function_logs:\n",
    "                                    print(f\"6.0 - Reused contact node {produced_osmid} for original edge {diverging_edge_id}.\")\n",
    "                                \n",
    "                                fabricated_count += 1\n",
    "                                stop = True\n",
    "                                continue\n",
    "\n",
    "                            # Else, this is the first time that the edge is shortened once. \n",
    "                            # Not relevant, continue as usual.\n",
    "                            else:\n",
    "                                pass\n",
    "                                \n",
    "                        # Else, either this is the first edge to be shortened or shorten_i is not equall to 1. \n",
    "                        # Not relevant, continue as usual.\n",
    "                        else:\n",
    "                            pass\n",
    "                        # Double-once-shortening test ENDS ### ### ### ### ### ### ### ### ### ### ### ### \n",
    "\n",
    "                        # CONTINUATION OF NORMAL PROCESS:\n",
    "                        # The current mid_point is not in proximity to the base network.\n",
    "                        # But the previous mid_point (The current connection_edge's endpoint) WAS KNOWN TO BE in proximity to the base network.\n",
    "                        # Next step --> Transform the last mid_point (The currend endpoint) into a contact_node and update the edge.\n",
    "\n",
    "                        # 6.3.3.b1 - Produce a unique osmid (That doesn't exist in either network) in order to add the edge and point\n",
    "                        # Produce a unique osmid\n",
    "                        produced_osmid = produce_osmid(base_nodes, complementary_nodes, previously_produced+1)\n",
    "                        # Save produced osmid to avoid trying numbers unnecessarily\n",
    "                        previously_produced = produced_osmid\n",
    "                        if debugging_logs:\n",
    "                            print(f\"Produced new osmid: {produced_osmid} for current diverging edge {diverging_edge_id}.\")\n",
    "                \n",
    "                        # 6.3.3.b2 - Find the ending_point's coordinates.\n",
    "                        # --------- It is unsure whether the diverging_node is located at the coordinate 0 or at the last coordinate of the connection_edge.\n",
    "                        # --------- So it is necessary to identify which one is which.\n",
    "                        # Extract the edge's coordinates list\n",
    "                        connection_edge_coords = list(connection_edge['geometry'][0].coords)\n",
    "                        # Extract the starting_point's coordinates (Known to be the diverging_node)\n",
    "                        starting_point_coords = diverging_node.loc[0,'geometry'].coords[0]\n",
    "                        # Obtain the ending_point's coordinates (It is the previous iteration edge's mid_point)\n",
    "                        # (It is either the first or last coordinate of the connection_edge)\n",
    "                        if starting_point_coords == connection_edge_coords[0]:\n",
    "                            ending_point_coords = connection_edge_coords[-1]\n",
    "                        elif starting_point_coords == connection_edge_coords[-1]:\n",
    "                            ending_point_coords = connection_edge_coords[0]\n",
    "                        else:\n",
    "                            print(f\"ERROR while trying to find the starting and ending point of shortened connection edge {diverging_edge_id}.\")\n",
    "                            print(f\"Diverging node osmid: {diverging_osmid}.\")\n",
    "                            print(f\"Diverging node (starting point) coords: {starting_point_coords}.\")\n",
    "                            print(f\"Connection edge's coords [0]: {connection_edge_coords[0]}.\")\n",
    "                            print(f\"Connection edge's coords [-1]: {connection_edge_coords[-1]}.\")\n",
    "                            intended_crash\n",
    "                \n",
    "                        # 6.3.3.b3 - Add the ending_point as a node\n",
    "                        # Register to complementary_uncovered_nodes gdf\n",
    "                        iloc_node = len(complementary_uncovered_nodes)+1\n",
    "                        complementary_uncovered_nodes.loc[iloc_node,'osmid'] = produced_osmid\n",
    "                        complementary_uncovered_nodes.loc[iloc_node,'geometry'] = Point(ending_point_coords)\n",
    "                        complementary_uncovered_nodes.loc[iloc_node,'clipping_i'] = shorten_i\n",
    "                        # Register to contact_nodes gdf\n",
    "                        iloc_node = len(contact_nodes)+1\n",
    "                        contact_nodes.loc[iloc_node,'osmid'] = produced_osmid\n",
    "                        contact_nodes.loc[iloc_node,'geometry'] = Point(ending_point_coords)\n",
    "                        contact_nodes.loc[iloc_node,'clipping_i'] = shorten_i\n",
    "                \n",
    "                        # 6.3.3.b4 - Add the connection_edge as a new edge to complementary_uncovered_edges gdf\n",
    "                        # Retrieve the diverging_osmid its position in the ORIGINAL connection_edge. (Whether 'u' or 'v')\n",
    "                        # In order to keep that original diverging_node in its position and add the produced_osmid in the other position.\n",
    "                        original_u = original_connection_edge.u.unique()[0]\n",
    "                        original_v = original_connection_edge.v.unique()[0]\n",
    "                        if diverging_osmid == original_u:\n",
    "                            connection_edge_u = diverging_osmid\n",
    "                            connection_edge_v = produced_osmid\n",
    "                        elif diverging_osmid == original_v:\n",
    "                            connection_edge_u = produced_osmid\n",
    "                            connection_edge_v = diverging_osmid\n",
    "                        else:\n",
    "                            print(f\"ERROR while trying to set 'u' and 'v' for shortened connection edge {diverging_edge_id}.\")\n",
    "                            intended_crash\n",
    "                        # Use the same key that was used in the original connection_edge\n",
    "                        connection_edge_key = original_connection_edge.key.unique()[0]\n",
    "                        # Retrieve the modified edge's (connection_edge's) geometry\n",
    "                        connection_edge_geom = connection_edge['geometry'].unique()[0]\n",
    "                        # Find last position\n",
    "                        iloc_edge = len(complementary_uncovered_edges)+1\n",
    "                        # Register new edge\n",
    "                        complementary_uncovered_edges.loc[iloc_edge,'u'] = connection_edge_u\n",
    "                        complementary_uncovered_edges.loc[iloc_edge,'v'] = connection_edge_v\n",
    "                        complementary_uncovered_edges.loc[iloc_edge,'key'] = connection_edge_key\n",
    "                        complementary_uncovered_edges.loc[iloc_edge,'geometry'] = connection_edge_geom\n",
    "                        complementary_uncovered_edges.loc[iloc_edge,'edge_id'] = str(connection_edge_u)+str(connection_edge_v)+str(connection_edge_key)\n",
    "                        complementary_uncovered_edges.loc[iloc_edge,'original_edge_id'] = diverging_edge_id\n",
    "                        complementary_uncovered_edges.loc[iloc_edge,'clipping_i'] = shorten_i\n",
    "                        \n",
    "                        # 6.3.3.b5 - Finished registering. Stop while loop. Continue with the next diverging_edge_id of current osmid.\n",
    "                        fabricated_count += 1\n",
    "                        stop = True\n",
    "        \n",
    "        # LOG CODE - Progress logs\n",
    "        # Finished reviewing current osmid. Continue with next osmid in non_contact_osmids.\n",
    "        osmid_count+=1\n",
    "        # LOG CODE - Progress logs\n",
    "\n",
    "    if function_logs:\n",
    "        print(f\"Finished. Fabricated {fabricated_count}.\")\n",
    "                \n",
    "    return complementary_uncovered_nodes, complementary_uncovered_edges, contact_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c69d2-0589-4cb0-9279-6d9cbc7284f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2025/03/04 - Part 02 - Step 04 - Draw new edges and identify consequential intersections from 01_PL_04_Combine_networks [vref repository] before update that considers all edges (not only ntw_01 edges) for CASE A and all edges (not only ntw_02 edges) for CASE B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53f84a-78d2-42c7-b321-87de7f8ce62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "casetype_logs = False\n",
    "\n",
    "##### Time\n",
    "time_1 = time.time()\n",
    "##### Time\n",
    "\n",
    "# Reset previously_produced osmids (Used in function produce_osmid())\n",
    "previously_produced = 0\n",
    "\n",
    "# Create intersection_nodes_3, a GeoDataFrame that will be used in function network_intersections_update()\n",
    "# to perform a third round of intersections (Used to fix consequential intersections)\n",
    "# ----- OUTPUT DTYPES MANAGEMENT FOR intersection_nodes_3 GEODATAFRAME -----\n",
    "intersection_nodes_3_cols = {\"osmid\": \"int64\", # Node that is the intersection (clipping point)\n",
    "                             \"u\": \"int64\", # Edge that is intersected\n",
    "                             \"v\": \"int64\", # Edge that is intersected\n",
    "                             \"key\": \"int64\", # Edge that is intersected\n",
    "                             \"retain_how\":\"string\", #Stablishes which part of the split edge to keep\n",
    "                             \"edge_origin\":\"string\" #Helps divide network_intersections_update() process\n",
    "                            }\n",
    "intersection_nodes_3 = gpd.GeoDataFrame(columns=list(intersection_nodes_3_cols.keys()) + [\"geometry\"], crs=projected_crs).astype({**intersection_nodes_3_cols, \"geometry\": \"geometry\"})\n",
    "# Reorder columns\n",
    "intersection_nodes_3 = intersection_nodes_3[[\"osmid\", \"geometry\", # Node to be created to serve as a connection point in ntw_01\n",
    "                                             \"u\", \"v\", \"key\", # Edge from ntw_01 to be split by the created node\n",
    "                                             \"retain_how\", \"edge_origin\"]] # Data about the split and division of splitting process\n",
    "# ----- OUTPUT DTYPES MANAGEMENT FOR intersection_nodes_3 GEODATAFRAME -----\n",
    "fix_idx = 0 #Row count of the created gdf\n",
    "\n",
    "# Store the osmids and edge_ids created to join both networks (To identify them after the process)\n",
    "all_join_osmids = []\n",
    "all_join_edgeids = []\n",
    "\n",
    "# Consequential_intersections and its three cases (A, B and C) explanation.\n",
    "# The concatenated_edges are renamed joined_edges_concat.\n",
    "# The following code iterates over the connected_nodes df (The dataframe with defined relations between networks) and\n",
    "# draws the new needed edges (when needed). The new edges sometimes intersect other edges as a consequence of them being straight lines between\n",
    "# nodes. These new intersections are called consequential_intersections.\n",
    "# The first connected_node explores consequential_intersections with the existing concatenated_edges (renamed as joined_edges_concat), but\n",
    "# since the new-drawn-edge gets added to joined_edges_concat, the following connected_node also explores consequential_intersections with any\n",
    "# recently-drawn edge. This results in cases A, B and C explored at the end of the for loop.\n",
    "\n",
    "# Create joined_edges_concat with a column for edge_id\n",
    "joined_edges_concat = src.create_unique_edge_id(concatenated_edges)\n",
    "\n",
    "# LOG CODE - Progress logs\n",
    "# Will create progress logs when progress reaches these percentages:\n",
    "progress_logs = [0,10,20,30,40,50,60,70,80,90,100] # for log statistics\n",
    "progress_count = 0\n",
    "# LOG CODE - Progress logs\n",
    "\n",
    "# Iterate over each relation stablished\n",
    "for idx, connected_node in connected_nodes.iterrows():\n",
    "\n",
    "    # LOG CODE - Progress logs\n",
    "    # Measures current progress, prints if passed a checkpoint of progress_logs list.\n",
    "    current_progress = (progress_count / len(connected_nodes))*100\n",
    "    for checkpoint in progress_logs:\n",
    "        if (current_progress >= checkpoint):\n",
    "            print(f\"Categorizing node relation types. {checkpoint}% done.\")\n",
    "            progress_logs.remove(checkpoint)\n",
    "            break\n",
    "    # LOG CODE - Progress logs\n",
    "    \n",
    "    # Development checks -----------------------------------\n",
    "    #current_osmid = connected_node.connecting_ntw02_osmid\n",
    "    #osmid_checks = [1837]\n",
    "    #if current_osmid not in osmid_checks:\n",
    "    #    continue\n",
    "    #else:\n",
    "    #    print(current_osmid)\n",
    "    # Development checks -----------------------------------\n",
    "    \n",
    "    # Extract relation's data\n",
    "    # The following words are used to refer to the origin of data:\n",
    "    # 'connection' refers to the data from network 01,\n",
    "    # 'connecting' referes to data from network 02.\n",
    "    # The reasoning used is that network 02 (complementary) is ---connecting--> to network 01 (base)\n",
    "    connection_ntw01_osmid = connected_node.connection_ntw01_osmid\n",
    "    connection_type = connected_node.connection_type\n",
    "    connecting_ntw02_osmid = connected_node.connecting_ntw02_osmid\n",
    "\n",
    "    # If the connection for the current connecting_node was identified to already exist nearby (Part 02 - Step 02), --> skip (continue)\n",
    "    if connection_type == 'existing':\n",
    "        # LOG CODE - Progress logs\n",
    "        progress_count+=1\n",
    "        # LOG CODE - Progress logs\n",
    "        continue # Next nodes_relation\n",
    "\n",
    "    # Identify the node's coordinates on ntw_01 that's going to get connected to the node on ntw_02\n",
    "    connection_node_gdf = concatenated_nodes.loc[concatenated_nodes.osmid==connection_ntw01_osmid].copy()\n",
    "    # Safety check (in case a node failed in re-update of the network)\n",
    "    if len(connection_node_gdf)<1:\n",
    "        print(f\"WARNING: Skipping node not found. Connection (ntw01): {connection_ntw01_osmid}. Connecting (ntw02): {connecting_ntw02_osmid}.\")\n",
    "        continue\n",
    "    connection_node_geom = connection_node_gdf.geometry.unique()[0]\n",
    "    connection_node_coords = connection_node_geom.coords[0]\n",
    "    if casetype_logs:\n",
    "        print(f\"ntw_01 - connection_node_geom: {connection_node_geom}.\") \n",
    "    \n",
    "    # Identify the node's coordinates on ntw_02 that's going to get connected to the node on ntw_01\n",
    "    connecting_node_gdf = concatenated_nodes.loc[concatenated_nodes.osmid==connecting_ntw02_osmid].copy()\n",
    "    # Safety check (in case a node failed in re-update of the network)\n",
    "    if len(connecting_node_gdf)<1:\n",
    "        print(f\"WARNING: Skipping node not found. Connection (ntw01): {connection_ntw01_osmid}. Connecting (ntw02): {connecting_ntw02_osmid}.\")\n",
    "        continue\n",
    "    connecting_node_geom = connecting_node_gdf.geometry.unique()[0]\n",
    "    connecting_node_coords = connecting_node_geom.coords[0]\n",
    "    if casetype_logs:\n",
    "        print(f\"ntw_02 - connecting_node_geom: {connecting_node_geom}.\") \n",
    "\n",
    "    # Create LineString between connection_node and connecting_node\n",
    "    line_geom = LineString([[connection_node_coords[0],connection_node_coords[1]],[connecting_node_coords[0],connecting_node_coords[1]]])\n",
    "    \n",
    "    # Store new LineString to concat to joined_edges_concat and to analyse consequential_intersections\n",
    "    # ----- OUTPUT REGISTRATION FOR new_edge IN joined_edges_concat GEODATAFRAME ----- [Concatenates after overlay]\n",
    "    # Create temporary df with the current case's data\n",
    "    df_temporal = pd.DataFrame({'u': [int(connecting_ntw02_osmid)], # Edge that is being created\n",
    "                                'v': [int(connection_ntw01_osmid)], # Edge that is being created\n",
    "                                'key': [int(0)], # Edge that is being created\n",
    "                                'geometry':[line_geom], # Edge that is being created\n",
    "                                'ntw_join':[int(1)], # Helps identify which edges were created to join both networks\n",
    "                                'ntw_origin':['ntw_join'] # Has data of all origins, created edges get assigned \"ntw_join\"\n",
    "                               }\n",
    "                              )\n",
    "    new_edge = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "    # Force all datatypes to match the datatypes of the gdf to where the data will be merged\n",
    "    dtypes_dict = joined_edges_concat.dtypes.to_dict() #Dict with TARGET dtypes\n",
    "    filtered_dtypes = {col: dtypes_dict[col] for col in new_edge.columns if col in dtypes_dict} # Filters for cols in case of mismatch\n",
    "    new_edge = new_edge.astype(filtered_dtypes) # Assigns those types\n",
    "    if casetype_logs:\n",
    "        print(f\"Created edge between connection and connecting nodes.\") \n",
    "    # ----- OUTPUT REGISTRATION FOR new_edge IN joined_edges_concat GEODATAFRAME ----- [Concatenates after overlay]\n",
    "    \n",
    "    # Create unique edge_id for new_edge (requires input ID cols as int)\n",
    "    #new_edge['u'] = new_edge['u'].astype('int')\n",
    "    #new_edge['v'] = new_edge['v'].astype('int')\n",
    "    #new_edge['key'] = new_edge['key'].astype('int')\n",
    "    new_edge = src.create_unique_edge_id(new_edge)\n",
    "\n",
    "    # Store the edge_ids created to join both networks (To identify them after the process)\n",
    "    join_edge_ids = list(new_edge.edge_id.unique())\n",
    "    all_join_edgeids = all_join_edgeids + join_edge_ids\n",
    "    \n",
    "    # Find all intersections that the new edge creates on the current network edges\n",
    "    # (Used to search for consequential_intersections)\n",
    "    produced_intersections = joined_edges_concat.overlay(new_edge,how='intersection',keep_geom_type=False)\n",
    "    \n",
    "    # Concatenate new_edge to joined_edges_concat gdf \n",
    "    # (It is after .overlay(), else the LineString intersects with itself)\n",
    "    joined_edges_concat = pd.concat([joined_edges_concat,new_edge])\n",
    "    joined_edges_concat.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    # CONSEQUENTIAL INTERSECTIONS ANALYSIS\n",
    "    # Explode the produced_intersections (If the new edge intersected the same edge twice or more, it produces MultiPoints)\n",
    "    produced_intersections = produced_intersections.explode(index_parts=False)\n",
    "    produced_intersections.reset_index(inplace=True,drop=True)\n",
    "    # Remove from the produced_intersections the nodes that are currently being joined (Obviously, the line that connects them intersects with them)\n",
    "    if casetype_logs:\n",
    "        print(f\"Looking for produced intersections. Dropping intersections with {joined_nodes_lst}.\")\n",
    "    \n",
    "    joined_nodes_lst = [connection_node_geom,connecting_node_geom]\n",
    "\n",
    "    # Function that compares geometries considering a tolerance\n",
    "    def geometries_are_equal_with_tolerance(geom1, geom2, tolerance=0.001): # 1mm tolerance\n",
    "        return geom1.equals(geom2) or geom1.distance(geom2) < tolerance\n",
    "    # Compare and filter\n",
    "    produced_intersections = produced_intersections.loc[ ~produced_intersections['geometry'].apply(\n",
    "        lambda x: any(geometries_are_equal_with_tolerance(x, node_geom) for node_geom in joined_nodes_lst)\n",
    "    )].copy()\n",
    "\n",
    "    # Previous way of comparing (Sometimes failed to identify when the line intersected it's own node)\n",
    "    #produced_intersections = produced_intersections.loc[~produced_intersections['geometry'].isin(joined_nodes_lst)].copy()\n",
    "    \n",
    "    if casetype_logs:\n",
    "        print(f\"Found intersections at the following points: {produced_intersections['geometry'].unique()}.\") \n",
    "    \n",
    "    # If there are any intersections remaining, they are consequential_intersections.\n",
    "    # (Meaning, the drawn line is intersecting with other lines in an undesired way)\n",
    "    if len(produced_intersections)>0:\n",
    "        # Rename gdf as \"consequential_intersections\"\n",
    "        consequential_intersections = produced_intersections.copy()\n",
    "        consequential_intersections.reset_index(inplace=True,drop=True)\n",
    "        del produced_intersections\n",
    "\n",
    "        # Rename columns to distinguish network origin\n",
    "        # (Overlay produced _1 for joined_edges_concat data and _2 for new_edge data)\n",
    "        consequential_intersections.rename(columns={'edge_id_1':'intersected_edge_id',\n",
    "                                                    'edge_id_2':'drawn_edge_id'},inplace=True)\n",
    "        \n",
    "        # Load network 01 edges related to the connection\n",
    "        # (Network 01 edges that are related to the current connection_node)\n",
    "        connection_idx = ((joined_edges_concat.u==connection_ntw01_osmid)|(joined_edges_concat.v==connection_ntw01_osmid)) & (joined_edges_concat.ntw_origin=='ntw_01')\n",
    "        related_ntw01_edges = joined_edges_concat.loc[connection_idx].copy()\n",
    "        related_ntw01_edges_ids = list(related_ntw01_edges['edge_id'].unique())\n",
    "        \n",
    "        # Load network 02 edges related to the connection\n",
    "        # (Network 02 edges that are related to the current connecting_node)\n",
    "        connecting_idx = ((joined_edges_concat.u==connecting_ntw02_osmid)|(joined_edges_concat.v==connecting_ntw02_osmid)) & (joined_edges_concat.ntw_origin=='ntw_02')\n",
    "        related_ntw02_edges = joined_edges_concat.loc[connecting_idx].copy()\n",
    "        related_ntw02_edges_ids = list(related_ntw02_edges['edge_id'].unique())\n",
    "        \n",
    "        # Iterate over consequential_intersections:\n",
    "        for idx,intersection in consequential_intersections.iterrows():\n",
    "    \n",
    "            # Extract intersection's data\n",
    "            # Edge that was intersected\n",
    "            intersected_u = int(intersection.u_1)\n",
    "            intersected_v = int(intersection.v_1)\n",
    "            intersected_key = intersection.key_1\n",
    "            intersected_edge_id = intersection.intersected_edge_id\n",
    "            # Drawn edge that is intersecting\n",
    "            drawn_edge_u = int(intersection.u_2)\n",
    "            drawn_edge_v = int(intersection.v_2)\n",
    "            drawn_edge_key = intersection.key_2\n",
    "            drawn_edge_id = intersection.drawn_edge_id\n",
    "\n",
    "            # The intersection (In any studied case) will become a new node.\n",
    "            # Extract its geometry and produce a unique osmid\n",
    "            intersectionpoint_geom = intersection.geometry\n",
    "            produced_osmid = produce_osmid(concatenated_nodes, concatenated_nodes, previously_produced)\n",
    "            # Since concatenated_nodes are not being updated, next time try with next possible osmid\n",
    "            previously_produced = produced_osmid+1 \n",
    "            # Store the osmids created to join both networks (To identify them after the process)\n",
    "            all_join_osmids.append(produced_osmid)\n",
    "\n",
    "            # Cases analysis\n",
    "            if intersected_edge_id in related_ntw01_edges_ids:\n",
    "                # CASE A: The new edge intersects network 01 at an edge comming out of the node \n",
    "                #         TO where the connection was being performed.\n",
    "                # --> CASE A will draw the connection from the connecting_node on ntw02 to the intersection point ONLY.\n",
    "                # -->        because thats were there's already a known connection with ntw01.\n",
    "                # -->        ntw01 edge gets split to add a new node, keeping both sides of the edge.\n",
    "                # -->        drawn edge gets split at the intersection, keeping one side of the edge.\n",
    "                case = 'CASE A'\n",
    "            elif intersected_edge_id in related_ntw02_edges_ids:\n",
    "                # CASE B: The new edge intersects network 02 at an edge comming out of the node \n",
    "                #         FROM where the connection was being performed.\n",
    "                # --> CASE B will draw the connection from the intersection point to the connection_node on ntw_01 ONLY.\n",
    "                # -->        because the rest (from intersection point to ntw02) is redundant.\n",
    "                # -->        ntw02 edge gets split to add a new node, keeping both sides of the edge.\n",
    "                # -->        drawn edge gets split at the intersection, keeping one side of the edge.\n",
    "                case = 'CASE B'\n",
    "            else:\n",
    "                # CASE C: The new edge intersects with either network 01 or network 02 at an UNRELATED edge.\n",
    "                #         (Or at a related ntw02 edge, but one that shouldn't be cut)\n",
    "                # --> CASE C will draw only create a new node on the intersection and split the edges, \n",
    "                # -->        keeping both sides on both split edges.\n",
    "                # -->        ntw01 or ntw02 edge gets split to add a new node, keeping both sides of the edge.\n",
    "                # -->        drawn edge gets split at the intersection, keeping both sides of the edge.\n",
    "                case = 'CASE C'\n",
    "    \n",
    "            # n.n --------------- CASE A\n",
    "            if case == 'CASE A':\n",
    "                if casetype_logs:\n",
    "                    print(f\"CASE A: New edge from connecting node {connecting_ntw02_osmid} intersects network 01 at an edge comming out of the node TO where the connection was being performed.\")\n",
    "    \n",
    "                # Register how the __INTERSECTED EDGE__ (from network 01) will be clipped\n",
    "                # (In the intersection, keeping both ends)\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME -----\n",
    "                # Create temporary df with the current case's data\n",
    "                df_temporal = pd.DataFrame({'osmid': [int(produced_osmid)], # Node that is the intersection (clipping point)\n",
    "                                            'geometry': [intersectionpoint_geom], # Node that is the intersection (clipping point)\n",
    "                                            'u': [int(intersected_u)], # Edge that is intersected\n",
    "                                            'v':[int(intersected_v)], # Edge that is intersected\n",
    "                                            'key':[int(intersected_key)], # Edge that is intersected\n",
    "                                            'retain_how':['both'], #Sets retain_all=True (keeps both parts of the split edge)\n",
    "                                            'edge_origin':['ntw_01'] #Helps divide network_intersections_update() process\n",
    "                                           }\n",
    "                                          )\n",
    "                gdf_temporal = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "                # Force all datatypes to match the datatypes of the gdf to where the data will be merged\n",
    "                dtypes_dict = intersection_nodes_3.dtypes.to_dict() #Dict with TARGET dtypes\n",
    "                filtered_dtypes = {col: dtypes_dict[col] for col in gdf_temporal.columns if col in dtypes_dict} # Filters for cols in case of mismatch\n",
    "                gdf_temporal = gdf_temporal.astype(filtered_dtypes) # Assigns those types\n",
    "                # Concatenate to TARGET without altering original dtypes\n",
    "                intersection_nodes_3 = pd.concat([intersection_nodes_3, gdf_temporal], ignore_index=True)\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME -----                \n",
    "                fix_idx+=1\n",
    "            \n",
    "                # Register how the __DRAWN EDGE__ will be clipped\n",
    "                # (From the connecting_ntw02_node to the intersection, keeping just that segment)\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME -----\n",
    "                # Create temporary df with the current case's data\n",
    "                df_temporal = pd.DataFrame({'osmid': [int(produced_osmid)], # Node that is the intersection (clipping point)\n",
    "                                            'geometry': [intersectionpoint_geom], # Node that is the intersection (clipping point)\n",
    "                                            'u': [int(drawn_edge_u)], # Edge that is intersected (u is from ntw02)\n",
    "                                            'v':[int(drawn_edge_v)], # Edge that is intersected (v is from ntw01)\n",
    "                                            'key':[int(drawn_edge_key)], # Edge that is intersected\n",
    "                                            'retain_how':['u'], #Sets retain_all=False, states which part to keep (u, comming from ntw02)\n",
    "                                            'edge_origin':['ntw_join'] #Helps divide network_intersections_update() process\n",
    "                                           }\n",
    "                                          )\n",
    "                gdf_temporal = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "                # Force all datatypes to match the datatypes of the gdf to where the data will be merged\n",
    "                dtypes_dict = intersection_nodes_3.dtypes.to_dict() #Dict with TARGET dtypes\n",
    "                filtered_dtypes = {col: dtypes_dict[col] for col in gdf_temporal.columns if col in dtypes_dict} # Filters for cols in case of mismatch\n",
    "                gdf_temporal = gdf_temporal.astype(filtered_dtypes) # Assigns those types\n",
    "                # Concatenate to TARGET without altering original dtypes\n",
    "                intersection_nodes_3 = pd.concat([intersection_nodes_3, gdf_temporal], ignore_index=True)\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME ----- \n",
    "                fix_idx+=1\n",
    "    \n",
    "            # CASE B\n",
    "            elif case == 'CASE B':\n",
    "                if casetype_logs:\n",
    "                    print(f\"CASE B: New edge from connecting node {connecting_ntw02_osmid} intersects network 02 at an edge comming out of the node FROM where the connection was being performed.\")\n",
    "                \n",
    "                # Register how the __INTERSECTED EDGE__ (from network 02) will be clipped\n",
    "                # (In the intersection, keeping both ends)\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME -----\n",
    "                # Create temporary df with the current case's data\n",
    "                df_temporal = pd.DataFrame({'osmid': [int(produced_osmid)], # Node that is the intersection (clipping point)\n",
    "                                            'geometry': [intersectionpoint_geom], # Node that is the intersection (clipping point)\n",
    "                                            'u': [int(intersected_u)], # Edge that is intersected\n",
    "                                            'v':[int(intersected_v)], # Edge that is intersected\n",
    "                                            'key':[int(intersected_key)], # Edge that is intersected\n",
    "                                            'retain_how':['both'], #Sets retain_all=True (keeps both parts of the split edge)\n",
    "                                            'edge_origin':['ntw_02'] #Helps divide network_intersections_update() process\n",
    "                                           }\n",
    "                                          )\n",
    "                gdf_temporal = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "                # Force all datatypes to match the datatypes of the gdf to where the data will be merged\n",
    "                dtypes_dict = intersection_nodes_3.dtypes.to_dict() #Dict with TARGET dtypes\n",
    "                filtered_dtypes = {col: dtypes_dict[col] for col in gdf_temporal.columns if col in dtypes_dict} # Filters for cols in case of mismatch\n",
    "                gdf_temporal = gdf_temporal.astype(filtered_dtypes) # Assigns those types\n",
    "                # Concatenate to TARGET without altering original dtypes\n",
    "                intersection_nodes_3 = pd.concat([intersection_nodes_3, gdf_temporal], ignore_index=True)\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME ----- \n",
    "                fix_idx+=1\n",
    "                \n",
    "                # Register how the __DRAWN EDGE__ will be clipped\n",
    "                # (From the connection_ntw01_node to the intersection, keeping just that segment)\n",
    "                \n",
    "                # Keep starting point from the connection_ntw01_osmid\n",
    "                if drawn_edge_u == connection_ntw01_osmid:\n",
    "                    drawn_edge_starting_point = 'u' #Sets retain_all=False, keeping the part connected to ntw_01\n",
    "                elif drawn_edge_v ==connection_ntw01_osmid:\n",
    "                    drawn_edge_starting_point = 'v' #Sets retain_all=False, states which part connected to ntw_01\n",
    "                else:\n",
    "                    intented_crash\n",
    "\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME -----\n",
    "                # Create temporary df with the current case's data\n",
    "                df_temporal = pd.DataFrame({'osmid': [int(produced_osmid)], # Node that is the intersection (clipping point)\n",
    "                                            'geometry': [intersectionpoint_geom], # Node that is the intersection (clipping point)\n",
    "                                            'u': [int(drawn_edge_u)], # Edge that is intersected (u is from ntw02)\n",
    "                                            'v':[int(drawn_edge_v)], # Edge that is intersected (v is from ntw01)\n",
    "                                            'key':[int(drawn_edge_key)], # Edge that is intersected\n",
    "                                            'retain_how':[drawn_edge_starting_point], #Sets retain_all=False, states which part to keep ('u' or 'v')\n",
    "                                            'edge_origin':['ntw_join'] #Helps divide network_intersections_update() process\n",
    "                                           }\n",
    "                                          )\n",
    "                gdf_temporal = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "                # Force all datatypes to match the datatypes of the gdf to where the data will be merged\n",
    "                dtypes_dict = intersection_nodes_3.dtypes.to_dict() #Dict with TARGET dtypes\n",
    "                filtered_dtypes = {col: dtypes_dict[col] for col in gdf_temporal.columns if col in dtypes_dict} # Filters for cols in case of mismatch\n",
    "                gdf_temporal = gdf_temporal.astype(filtered_dtypes) # Assigns those types\n",
    "                # Concatenate to TARGET without altering original dtypes\n",
    "                intersection_nodes_3 = pd.concat([intersection_nodes_3, gdf_temporal], ignore_index=True)\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME ----- \n",
    "                fix_idx+=1\n",
    "                \n",
    "            # CASE C\n",
    "            elif case == 'CASE C':\n",
    "                if casetype_logs:\n",
    "                    print(f\"CASE C: New edge from connecting node {connecting_ntw02_osmid} intersects with either network 01 or network 02 at an UNRELATED edge.\")\n",
    "    \n",
    "                # Register how the __INTERSECTED EDGE__ (from either network 01 or network 02) will be clipped\n",
    "                # (In the intersection, keeping both ends)\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME -----\n",
    "                # Create temporary df with the current case's data\n",
    "                df_temporal = pd.DataFrame({'osmid': [int(produced_osmid)], # Node that is the intersection (clipping point)\n",
    "                                            'geometry': [intersectionpoint_geom], # Node that is the intersection (clipping point)\n",
    "                                            'u': [int(intersected_u)], # Edge that is intersected\n",
    "                                            'v':[int(intersected_v)], # Edge that is intersected\n",
    "                                            'key':[int(intersected_key)], # Edge that is intersected\n",
    "                                            'retain_how':['both'], #Sets retain_all=True (keeps both parts of the split edge)\n",
    "                                            'edge_origin':['ntw_01_or_02'] #Helps divide network_intersections_update() process\n",
    "                                           }\n",
    "                                          )\n",
    "                gdf_temporal = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "                # Force all datatypes to match the datatypes of the gdf to where the data will be merged\n",
    "                dtypes_dict = intersection_nodes_3.dtypes.to_dict() #Dict with TARGET dtypes\n",
    "                filtered_dtypes = {col: dtypes_dict[col] for col in gdf_temporal.columns if col in dtypes_dict} # Filters for cols in case of mismatch\n",
    "                gdf_temporal = gdf_temporal.astype(filtered_dtypes) # Assigns those types\n",
    "                # Concatenate to TARGET without altering original dtypes\n",
    "                intersection_nodes_3 = pd.concat([intersection_nodes_3, gdf_temporal], ignore_index=True)\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME ----- \n",
    "                fix_idx+=1\n",
    "            \n",
    "                # Register how the __DRAWN EDGE__ will be clipped\n",
    "                # (From the connecting_ntw02_node to the intersection, keeping just that segment)\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME -----\n",
    "                # Create temporary df with the current case's data\n",
    "                df_temporal = pd.DataFrame({'osmid': [int(produced_osmid)], # Node that is the intersection (clipping point)\n",
    "                                            'geometry': [intersectionpoint_geom], # Node that is the intersection (clipping point)\n",
    "                                            'u': [int(drawn_edge_u)], # Edge that is intersected (u is from ntw02)\n",
    "                                            'v':[int(drawn_edge_v)], # Edge that is intersected (v is from ntw01)\n",
    "                                            'key':[int(drawn_edge_key)], # Edge that is intersected\n",
    "                                            'retain_how':['both'], #Sets retain_all=True (keeps both parts of the split edge)\n",
    "                                            'edge_origin':['ntw_join'] #Helps divide network_intersections_update() process\n",
    "                                           }\n",
    "                                          )\n",
    "                gdf_temporal = gpd.GeoDataFrame(df_temporal, geometry='geometry', crs=projected_crs)\n",
    "                # Force all datatypes to match the datatypes of the gdf to where the data will be merged\n",
    "                dtypes_dict = intersection_nodes_3.dtypes.to_dict() #Dict with TARGET dtypes\n",
    "                filtered_dtypes = {col: dtypes_dict[col] for col in gdf_temporal.columns if col in dtypes_dict} # Filters for cols in case of mismatch\n",
    "                gdf_temporal = gdf_temporal.astype(filtered_dtypes) # Assigns those types\n",
    "                # Concatenate to TARGET without altering original dtypes\n",
    "                intersection_nodes_3 = pd.concat([intersection_nodes_3, gdf_temporal], ignore_index=True)\n",
    "                # ----- OUTPUT REGISTRATION FOR intersection_nodes_3 GEODATAFRAME ----- \n",
    "                fix_idx+=1\n",
    "\n",
    "    # LOG CODE - Progress logs\n",
    "    # Finished reviewing current osmid. Continue with next osmid in non_contact_osmids.\n",
    "    progress_count+=1\n",
    "    # LOG CODE - Progress logs\n",
    "\n",
    "\n",
    "# Set unique identifiers to int for both outputs\n",
    "#joined_edges_concat['u'] = joined_edges_concat['u'].astype('int')\n",
    "#joined_edges_concat['v'] = joined_edges_concat['v'].astype('int')\n",
    "#joined_edges_concat['key'] = joined_edges_concat['key'].astype('int')\n",
    "\n",
    "#intersection_nodes_3['osmid'] = intersection_nodes_3['osmid'].astype('int')\n",
    "#intersection_nodes_3['u'] = intersection_nodes_3['u'].astype('int')\n",
    "#intersection_nodes_3['v'] = intersection_nodes_3['v'].astype('int')\n",
    "#intersection_nodes_3['key'] = intersection_nodes_3['key'].astype('int')\n",
    "\n",
    "# Fix edge_origin 'ntw_01_or_02' assignment in intersection_nodes_3\n",
    "print(\"Finding origin for edges assigned 'ntw_01_or_02'.\")\n",
    "# Extract list of edge_ids that are known to be either ntw_01 or ntw_02\n",
    "ntw01_edges_ids = list(joined_edges_concat.loc[joined_edges_concat.ntw_origin=='ntw_01'].edge_id.unique())\n",
    "ntw02_edges_ids = list(joined_edges_concat.loc[joined_edges_concat.ntw_origin=='ntw_02'].edge_id.unique())\n",
    "# Create a edge_id col for the intersected edges registered for each intersection on intersection_nodes_3 gdf\n",
    "intersection_nodes_3 = src.create_unique_edge_id(intersection_nodes_3)\n",
    "# Fix edge_origin 'ntw_01_or_02'\n",
    "idx_01 = (intersection_nodes_3.edge_origin=='ntw_01_or_02') & (intersection_nodes_3.edge_id.isin(ntw01_edges_ids))\n",
    "intersection_nodes_3.loc[idx_01,'edge_origin'] = 'ntw_01'\n",
    "idx_02 = (intersection_nodes_3.edge_origin=='ntw_01_or_02') & (intersection_nodes_3.edge_id.isin(ntw02_edges_ids))\n",
    "intersection_nodes_3.loc[idx_02,'edge_origin'] = 'ntw_02'\n",
    "\n",
    "##### Time\n",
    "time_2 = time.time()\n",
    "print(f\"TIME: {time_2-time_1} seconds on drawing edges and identifying consequential intersections.\")\n",
    "##### Time\n",
    "\n",
    "# Show\n",
    "print(joined_edges_concat.shape)\n",
    "print(f\"Duplicates: {len(joined_edges_concat.loc[joined_edges_concat.duplicated('edge_id')])}.\")\n",
    "joined_edges_concat.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70563a-86f6-44b9-baf2-617439e6cb68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Previous network's update and removal of duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd02cba1-945f-4d57-9fd8-11c29c329d93",
   "metadata": {},
   "source": [
    "Previously the network was updated and then duplicates were removed. Function network_intersections_update was updated to avoid the creation of duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf6ec0-9c18-4cef-9b37-a7696acc092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_intersections_update(current_ntw_nodes, current_ntw_edges, intersection_nodes, projected_crs, function_logs=False):\n",
    "\n",
    "    \"\"\" This function takes points with osmid located over existing edges (intersection_nodes) and updates\n",
    "        a network. The intersection_nodes become new nodes and each intersected edge get split \n",
    "        into two separate edges with new 'u', 'v' and 'key' data.\n",
    "    \n",
    "\tArgs:\n",
    "\t\tcurrent_ntw_nodes (geopandas.GeoDataFrame): GeoDataFrame containing the nodes from the network to update.\n",
    "                                                    Requires a unique identifier 'osmid'.\n",
    "        current_ntw_edges (geopandas.GeoDataFrame): GeoDataFrame containing the edges from the network to update.\n",
    "                                                    Requires the unique identifiers 'u ,'v' and 'key'.\n",
    "        intersection_nodes (geopandas.GeoDataFrame): GeoDataFrame containing the points in where each split is performed.\n",
    "                                                    Requires points with 'osmid', and the edge to split ('u','v' and 'key').\n",
    "        projected_crs (str, optional): String containing projected crs to be used depending on area of interest. \n",
    "                                        Defaults to \"EPSG:6372\".\n",
    "        function_logs (bool,optional): Boolean that (if True) prints logs during the functions execution. Defaults to False.\n",
    "\n",
    "                                                 \n",
    "\tReturns:\n",
    "        updated_ntw_nodes (geopandas.GeoDataFrame): GeoDataFrame containing the updated nodes for the network.\n",
    "        updated_ntw_edges (geopandas.GeoDataFrame): GeoDataFrame containing the updated edges for the network. \n",
    "        \n",
    "\t\"\"\" \n",
    "    print(f\"Updating network...\")\n",
    "    \n",
    "    # ------------------- INPUT USED - READ AND FILTER EDGES\n",
    "    current_ntw_nodes = current_ntw_nodes.copy()\n",
    "    current_ntw_nodes = current_ntw_nodes.to_crs(projected_crs)\n",
    "    # Set an identifier to make it easier to locate nodes that resulted from an intersection between networks\n",
    "    current_ntw_nodes['intersecting'] = 0\n",
    "\n",
    "    current_ntw_edges = current_ntw_edges.copy()\n",
    "    current_ntw_edges = current_ntw_edges.to_crs(projected_crs)\n",
    "    # Set an identifier to make it easier to locate edges that were split\n",
    "    current_ntw_edges['intersecting'] = 0\n",
    "    # ------------------- INPUT USED - READ AND FILTER EDGES\n",
    "\n",
    "    # Iterate over each intersection between both networks (intersection_nodes)\n",
    "    for idx, node in intersection_nodes.iterrows():\n",
    "        \n",
    "        # 3.1 --------------- Split the current_ntw intersected edge using the intersection_node as clipping_point. \n",
    "        # ------------------- This split (Using function edge_clipping()) creates two separate edges:\n",
    "        # ------------------- The first edge will be related to the starting_point_gdf (We'll set intersected edge 'u')\n",
    "        # ------------------- The second edge will be related to the opposite side (Will be intersected edge 'v')\n",
    "    \n",
    "        # Current intersection_node's data\n",
    "        intersection_node_osmid = node['osmid']\n",
    "        intersected_u = node['u']\n",
    "        intersected_v = node['v']\n",
    "        intersected_key = node['key']\n",
    "        intersected_retain_how = node['retain_how']        \n",
    "\n",
    "        if function_logs:\n",
    "            print(f\"network_intersections_update(): Iterating over intersection node osmid {intersection_node_osmid}.\") #Debugging check\n",
    "            print(f\"network_intersections_update(): Intersected edge with u {intersected_u} type {type(intersected_u)}.\") #Debugging check\n",
    "            print(f\"network_intersections_update(): Intersected edge with v {intersected_v} type {type(intersected_v)}.\") #Debugging check\n",
    "            print(f\"network_intersections_update(): Intersected edge with key {intersected_key} type {type(intersected_key)}.\") #Debugging check\n",
    "            \n",
    "        # Extract current intersection node as a gdf (Becomes clipping_point_gdf in function edge_clipping)\n",
    "        # (In most cases, osmid is the only value needed to identify the intersection node. In a very specific case where two edges from current_ntw_edges cross at the very same point\n",
    "        #  where an edge from the other network created an intersection, 'u', 'v' and 'key' would also be needed)\n",
    "        intersection_node_idx = (intersection_nodes.u==intersected_u)&(intersection_nodes.v==intersected_v)&(intersection_nodes.key==intersected_key)&(intersection_nodes.osmid==intersection_node_osmid)\n",
    "        intersection_node = intersection_nodes.loc[intersection_node_idx].copy()\n",
    "        intersection_node.reset_index(inplace=True,drop=True)\n",
    "        \n",
    "        # Extract current_ntw intersected edge (Becomes edge_gdf in function edge_clipping)\n",
    "        try:\n",
    "            # Try loading the edge registered as intersected in the intersection_nodes gdf.\n",
    "            # ('intersected_u', 'intersected_v' and 'intersected_key')\n",
    "            intersected_edge = current_ntw_edges.loc[(current_ntw_edges['u'] == intersected_u) & \n",
    "                                                     (current_ntw_edges['v'] == intersected_v) &\n",
    "                                                     (current_ntw_edges['key'] == intersected_key)].copy()\n",
    "            intersected_edge.reset_index(inplace=True,drop=True)\n",
    "            # If it has len=0, it failed.\n",
    "            if len(intersected_edge) == 0:\n",
    "                fail_try\n",
    "                \n",
    "        except:\n",
    "            # If it doesn't work, it means that the edge no longer exists (deleted in following steps in this function)\n",
    "            # This happens because that edge had another intersection along its lenght and \n",
    "            # that original unsplit edge was split and deleted.\n",
    "            # Now a new already split edge lies underneath the current intersection_node.\n",
    "            # --> Find that split edge's data\n",
    "            if function_logs:\n",
    "                print(f\"\"\"network_intersections_update(): Searching for already split edge originating from edge with u {intersected_u}, v {intersected_v} and key {intersected_key}.\"\"\")\n",
    "            # Create a VERY SMALL buffer around the intersection_node\n",
    "            intersection_node_buffer = intersection_node.buffer(1e-9)\n",
    "            intersection_node_buffer = gpd.GeoDataFrame(geometry=intersection_node_buffer)\n",
    "            # Find and rewrite the data of the split edge underneath the intersection_node\n",
    "            edge_data = intersection_node_buffer.sjoin(current_ntw_edges)\n",
    "            if len(edge_data) == 1:\n",
    "                intersected_u = edge_data.u.unique()[0]\n",
    "                intersected_v = edge_data.v.unique()[0]\n",
    "                intersected_key = edge_data.key.unique()[0]\n",
    "            else:\n",
    "                # If this happens, it means that there are two or more intersection_nodes located exactly\n",
    "                # in this point over the intersected_edge. The first has already split the edge, the next is attempting.\n",
    "                # Since the edge doesn't need to be split again, skip this intersection_node.\n",
    "                continue\n",
    "            \n",
    "            # Retrieve intersected edge\n",
    "            intersected_edge = current_ntw_edges.loc[(current_ntw_edges['u'] == intersected_u) & \n",
    "                                                     (current_ntw_edges['v'] == intersected_v) &\n",
    "                                                     (current_ntw_edges['key'] == intersected_key)].copy()\n",
    "            intersected_edge.reset_index(inplace=True,drop=True)\n",
    "        \n",
    "        # Extract current_ntw intersected edge's u node \n",
    "        # (Always becomes starting_point_gdf in function edge_clipping when using 'return_all')\n",
    "        u_node = current_ntw_nodes.loc[(current_ntw_nodes['osmid'] == intersected_u)].copy()\n",
    "        u_node.reset_index(inplace=True,drop=True)\n",
    "\n",
    "        # Extract current_ntw intersected edge's v node \n",
    "        v_node = current_ntw_nodes.loc[(current_ntw_nodes['osmid'] == intersected_v)].copy()\n",
    "        v_node.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "        # Apply edge_clipping function and assign the corresponding 'u', 'v' or 'key' data.\n",
    "        if intersected_retain_how == 'both':\n",
    "            # Clip edge\n",
    "            split_edge_gdf = edge_clipping(starting_point_gdf = u_node,\n",
    "                                           edge_gdf = intersected_edge,\n",
    "                                           clipping_point_gdf = intersection_node,\n",
    "                                           projected_crs = projected_crs,\n",
    "                                           return_all = True,\n",
    "                                           function_logs = function_logs)\n",
    "            # Assign data\n",
    "            # When return_all=True in function edge_clipping, \n",
    "            # assigns 'starting' to the edge related to the starting_point_gdf\n",
    "            # and 'ending' to edge on the opposite side.\n",
    "            u_idx = split_edge_gdf.relation=='starting'\n",
    "            split_edge_gdf.loc[u_idx,'u'] = intersected_u # We assigned 'u' as starting_point_gdf\n",
    "            split_edge_gdf.loc[u_idx,'v'] = intersection_node_osmid # Intersection\n",
    "            split_edge_gdf.loc[u_idx,'key'] = 0 #Since this 'u' and 'v' relation is new, key=0\n",
    "        \n",
    "            v_idx = split_edge_gdf.relation=='ending'\n",
    "            split_edge_gdf.loc[v_idx,'u'] = intersection_node_osmid # Intersection\n",
    "            split_edge_gdf.loc[v_idx,'v'] = intersected_v # Opposite side\n",
    "            split_edge_gdf.loc[v_idx,'key'] = 0 #Since this 'u' and 'v' relation is new, key=0\n",
    "        \n",
    "        elif intersected_retain_how == 'u':\n",
    "            # Clip edge\n",
    "            split_edge_gdf = edge_clipping(starting_point_gdf = u_node,\n",
    "                                           edge_gdf = intersected_edge,\n",
    "                                           clipping_point_gdf = intersection_node,\n",
    "                                           projected_crs = projected_crs,\n",
    "                                           return_all = False)\n",
    "            # Assign data\n",
    "            split_edge_gdf.loc[0,'u'] = intersected_u\n",
    "            split_edge_gdf.loc[0,'v'] = intersection_node_osmid # Intersection\n",
    "            split_edge_gdf.loc[0,'key'] = 0 #Since this 'u' and 'v' relation is new, key=0\n",
    "            \n",
    "        elif intersected_retain_how == 'v':\n",
    "            # Clip edge\n",
    "            split_edge_gdf = edge_clipping(starting_point_gdf = v_node,\n",
    "                                           edge_gdf = intersected_edge,\n",
    "                                           clipping_point_gdf = intersection_node,\n",
    "                                           projected_crs = projected_crs,\n",
    "                                           return_all = False)\n",
    "            # Assign data\n",
    "            split_edge_gdf.loc[0,'u'] = intersection_node_osmid # Intersection\n",
    "            split_edge_gdf.loc[0,'v'] = intersected_v\n",
    "            split_edge_gdf.loc[0,'key'] = 0 #Since this 'u' and 'v' relation is new, key=0\n",
    "            \n",
    "        else:\n",
    "            print(f\"Error splitting edge with u {intersected_u}, v {intersected_v} and key {intersected_key}.\")\n",
    "            print(\"Make sure to include in gdf intersection_nodes column 'retain_how' with either 'u','v' or 'both'.\")\n",
    "            intended_crash    \n",
    "    \n",
    "        # 3.2 --------------- Register changes on current_ntw\n",
    "        # Set an identifier to make it easier to locate nodes that resulted from an intersection between networks\n",
    "        intersection_node['intersecting'] = 1\n",
    "        # Prepare node for concatenation\n",
    "        intersection_node = intersection_node[['osmid','intersecting','geometry']]\n",
    "        # Add new node\n",
    "        current_ntw_nodes = pd.concat([current_ntw_nodes,intersection_node])\n",
    "        # Reset index\n",
    "        current_ntw_nodes.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "        # Keep all edges except the edge that was split\n",
    "        # (Must remove to avoid duplicating edge's geometries)\n",
    "        current_ntw_edges = current_ntw_edges.loc[~((current_ntw_edges['u'] == intersected_u) &\n",
    "                                                    (current_ntw_edges['v'] == intersected_v) &\n",
    "                                                    (current_ntw_edges['key'] == intersected_key))].copy()\n",
    "        # Prepare edges for concatenation\n",
    "        split_edge_gdf = split_edge_gdf[['u','v','key','geometry']]\n",
    "        # Set an identifier to make it easier to locate edges that were split\n",
    "        split_edge_gdf['intersecting'] = 1\n",
    "        # Add new edge\n",
    "        current_ntw_edges = pd.concat([current_ntw_edges,split_edge_gdf])\n",
    "        # Reset index\n",
    "        current_ntw_edges.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    print(f\"Updated network. Formating output.\")\n",
    "    updated_ntw_nodes = current_ntw_nodes[['osmid','intersecting','geometry']].copy()\n",
    "    # Set unique identifiers to int\n",
    "    updated_ntw_nodes['osmid'] = updated_ntw_nodes['osmid'].astype('int')\n",
    "    del current_ntw_nodes\n",
    "    updated_ntw_edges = current_ntw_edges[['u','v','key','intersecting','geometry']].copy()\n",
    "    # Set unique identifiers to int\n",
    "    updated_ntw_edges['u'] = updated_ntw_edges['u'].astype('int')\n",
    "    updated_ntw_edges['v'] = updated_ntw_edges['v'].astype('int')\n",
    "    updated_ntw_edges['key'] = updated_ntw_edges['key'].astype('int')\n",
    "    del current_ntw_edges\n",
    "    \n",
    "    # After iterating over both networks, return result\n",
    "    return updated_ntw_nodes, updated_ntw_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbe9da-5b2c-4455-91a1-2eb7d3155343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_intersection_network_duplicates(nodes_gdf,edges_gdf):\n",
    "\n",
    "    \"\"\" This function was created as a complement to function network_intersections_update().\n",
    "        Whenever three or more lines intersect exactly at the same point, function network_intersections_update() creates duplicated nodes and edges.\n",
    "        Those duplicates cannot be easly dropped since lines can be mirrored. Example:\n",
    "        Line 1: u=1, v=2, key=0, geom=((1,1),(2,1))\n",
    "        Line 2: u=2, v=1, key=0, geom=((2,1),(1,1))\n",
    "        \n",
    "        This function solves for those duplicated network geometries.\n",
    "    \n",
    "\tArgs:\n",
    "\t\tnodes_gdf (geopandas.GeoDataFrame): GeoDataFrame containing the nodes from the recently updated network.\n",
    "                                                    Requires a unique identifier 'osmid'.\n",
    "        edges_gdf (geopandas.GeoDataFrame): GeoDataFrame containing the edges from the recently updated network.\n",
    "                                                    Requires the unique identifiers 'u ,'v' and 'key'.\n",
    "\n",
    "\tReturns:\n",
    "        nodes_gdf (geopandas.GeoDataFrame): GeoDataFrame containing the updated nodes without duplicates.\n",
    "        edges_gdf (geopandas.GeoDataFrame): GeoDataFrame containing the updated edges without duplicates.\n",
    "        \n",
    "\t\"\"\" \n",
    "\n",
    "    # Copy input to avoid rewritting\n",
    "    nodes_gdf = nodes_gdf.copy()\n",
    "    edges_gdf = edges_gdf.copy()\n",
    "\n",
    "    # 1.0 --------------- Dropping duplicates on nodes\n",
    "    print(f\"Dropping duplicates on updated nodes and edges.\")\n",
    "    # Dropping duplicates on nodes by using osmid and geometry.\n",
    "    current_len = len(nodes_gdf)\n",
    "    nodes_gdf.drop_duplicates(subset=['osmid','geometry'],inplace=True)\n",
    "    updated_len = len(nodes_gdf)\n",
    "    print(f\"Dropped {current_len-updated_len} nodes that had the same osmid and geometry.\")\n",
    "\n",
    "    # 2.0 --------------- Dropping duplicates on edges\n",
    "    \n",
    "    # 2.1 --------------- Identify potential duplicates by comparing edge_id in a regular and inverted order\n",
    "    # Unique edge id with regular order ('u','v','key')\n",
    "    edges_gdf = create_unique_edge_id(edges_gdf)\n",
    "    edges_gdf.rename(columns={'edge_id':'edge_id_1'},inplace=True)\n",
    "    current_len = len(edges_gdf)\n",
    "    edges_gdf.drop_duplicates(subset=['edge_id_1'],inplace=True)\n",
    "    updated_len = len(edges_gdf)\n",
    "    print(f\"Dropped {current_len-updated_len} edges that had the same edge_id.\")\n",
    "    \n",
    "    # Unique edge id with inverted order ('v','u','key')\n",
    "    edges_gdf = create_unique_edge_id(edges_gdf,order='vukey')\n",
    "    edges_gdf.rename(columns={'edge_id':'edge_id_2'},inplace=True)\n",
    "    dup_inverted_lst = []\n",
    "    \n",
    "    # Identify edges where edge_id is in both regular and inverted order\n",
    "    for edge_id in list(edges_gdf.edge_id_1.unique()):\n",
    "        if edge_id in list(edges_gdf.edge_id_2.unique()):\n",
    "            dup_inverted_lst.append(edge_id)\n",
    "\n",
    "    # 2.2 --------------- Verify potential duplicates and register one of them to be dropped\n",
    "    # Verify those edges are duplicated\n",
    "    confirmed_dup_edge_lst = []\n",
    "    already_dropped_dict = {}\n",
    "    for edge_id in dup_inverted_lst:\n",
    "        regular_edge = edges_gdf.loc[edges_gdf.edge_id_1==edge_id]\n",
    "        inverted_edge = edges_gdf.loc[edges_gdf.edge_id_2==edge_id]\n",
    "        \n",
    "        # Discard different roads by length\n",
    "        # (Two roads may share start and end point if they each form half a circle)\n",
    "        regular_edge_length = regular_edge.length.unique()[0]\n",
    "        inverted_edge_length = inverted_edge.length.unique()[0]\n",
    "        if regular_edge_length != inverted_edge_length:\n",
    "            continue\n",
    "            \n",
    "        # Discard loop roads \n",
    "        # (One road may have the same start-end and end-start if it is a loop. Also, it would have the same length)\n",
    "        regular_edge_index = regular_edge.index[0]\n",
    "        inverted_edge_index = inverted_edge.index[0]\n",
    "        if regular_edge_index == inverted_edge_index:\n",
    "            continue\n",
    "            \n",
    "        # If reached here, it is duplicated. Check if the mirror relation was already registered\n",
    "        value_1 = regular_edge.u.unique()[0]\n",
    "        value_2 = regular_edge.v.unique()[0]\n",
    "        # If value_1 is already registered in the dictionary and it contains value_2, continue.\n",
    "        if (value_1 in already_dropped_dict.keys()) and (value_2 in already_dropped_dict[value_1]):\n",
    "            print(f\"Relation {value_1}<-->{value_2} already registered.\")\n",
    "            continue\n",
    "        # If value_2 is already registered in the dictionary and it contains value_1, continue.\n",
    "        elif (value_2 in already_dropped_dict.keys()) and (value_1 in already_dropped_dict[value_2]):\n",
    "            print(f\"Relation {value_1}<-->{value_2} already registered.\")\n",
    "            continue\n",
    "        \n",
    "        # Else, it has not been registered\n",
    "        else:\n",
    "            # Confirm that this edge_id will be dropped\n",
    "            confirmed_dup_edge_lst.append(edge_id)\n",
    "            # Save the relation that's being dropped   \n",
    "            if value_1 not in already_dropped_dict.keys():\n",
    "                already_dropped_dict[value_1] = list([value_2])\n",
    "                print(f\"Saved relationship {value_1}<-->{value_2} to be dropped.\")\n",
    "            else:\n",
    "                already_dropped_dict[value_1] = already_dropped_dict[value_1].append(value_2)\n",
    "                print(f\"Saved relationship {value_1}<-->{value_2} to be dropped.\")\n",
    "            \n",
    "    # 2.3 --------------- Drop confirmed relations\n",
    "    current_len = len(edges_gdf)\n",
    "    edges_gdf = edges_gdf.loc[~edges_gdf.edge_id_2.isin(confirmed_dup_edge_lst)]\n",
    "    updated_len = len(edges_gdf)\n",
    "    print(f\"Dropped {current_len-updated_len} edges that had the same edge_id but one was inverted.\")\n",
    "    # Drop columns used for dropping duplicates inside drop_intersection_network_duplicates()\n",
    "    edges_gdf.drop(columns=['edge_id_1','edge_id_2'],inplace=True)\n",
    "\n",
    "    return nodes_gdf,edges_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a1993-97c4-4e54-a9c9-9433a7d68c39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### First iteration of networks update that avoids the creation of duplicates (Dictionary was node{original_edge_id{[new_edges]}} instead of edge_id{node{[new_edges]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d2a700-3967-45a5-a0fb-c05ee69bbe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_intersections_update(current_ntw_nodes, current_ntw_edges, intersection_nodes, projected_crs,\n",
    "                                 intersection_logs=False, clipping_logs=False):\n",
    "\n",
    "    \"\"\" This function takes points with osmid located over existing edges (intersection_nodes) and updates\n",
    "        a network. The intersection_nodes become new nodes and each intersected edge get split \n",
    "        into two separate edges with new 'u', 'v' and 'key' data.\n",
    "    \n",
    "\tArgs:\n",
    "\t\tcurrent_ntw_nodes (geopandas.GeoDataFrame): GeoDataFrame containing the nodes from the network to update.\n",
    "                                                    Requires a unique identifier 'osmid'.\n",
    "        current_ntw_edges (geopandas.GeoDataFrame): GeoDataFrame containing the edges from the network to update.\n",
    "                                                    Requires the unique identifiers 'u ,'v' and 'key'.\n",
    "        intersection_nodes (geopandas.GeoDataFrame): GeoDataFrame containing the points in where each split is performed.\n",
    "                                                    Requires points with 'osmid', and the edge to split ('u','v' and 'key').\n",
    "        projected_crs (str, optional): String containing projected crs to be used depending on area of interest. \n",
    "                                        Defaults to \"EPSG:6372\".\n",
    "        intersection_logs (bool,optional): Boolean that (if True) prints logs during the current function's execution. Defaults to False.\n",
    "        clipping_logs (bool,optional): Boolean that (if True) prints logs during the edge_clipping() function's execution. Defaults to False.\n",
    "\n",
    "                                                 \n",
    "\tReturns:\n",
    "        updated_ntw_nodes (geopandas.GeoDataFrame): GeoDataFrame containing the updated nodes for the network.\n",
    "        updated_ntw_edges (geopandas.GeoDataFrame): GeoDataFrame containing the updated edges for the network. \n",
    "        \n",
    "\t\"\"\" \n",
    "    \n",
    "    print(f\"Updating network...\")\n",
    "    \n",
    "    # ------------------- INPUT USED - READ AND FILTER EDGES\n",
    "    current_ntw_nodes = current_ntw_nodes.copy()\n",
    "    current_ntw_nodes = current_ntw_nodes.to_crs(projected_crs)\n",
    "    # Set an identifier to make it easier to locate nodes that resulted from an intersection between networks\n",
    "    current_ntw_nodes['intersecting'] = 0\n",
    "\n",
    "    current_ntw_edges = current_ntw_edges.copy()\n",
    "    current_ntw_edges = current_ntw_edges.to_crs(projected_crs)\n",
    "    # Set an identifier to make it easier to locate edges that were split\n",
    "    current_ntw_edges['intersecting'] = 0\n",
    "    # ------------------- INPUT USED - READ AND FILTER EDGES\n",
    "\n",
    "    # Iterate over each intersection between both networks (intersection_nodes)\n",
    "\n",
    "    # MULTIPLE INTERSECTION ADAPTATION\n",
    "    # EXPLANATION\n",
    "    # Function find_intersection_nodes() was originaly designed to create a dataframe that stablishes intersection points between\n",
    "    # 1 (one) edge of a network and 1 (one) edge of another network. The dataframe stores the IDs of each intersected edge and the intersecting node.\n",
    "    # However, cases were found where three or more edges intersect exactly at the same point (particularly due to the use of tessellations-generated networks).\n",
    "    # The result is having two (or more) intersection_nodes located exactly at the same point.\n",
    "    # e.g. one intersecting edge_1 from network \"x\" and edge_1 from network \"y\", \n",
    "    # and the other intersecting edge_2 from network \"x\" and edge_1 from network \"y\".\n",
    "\n",
    "    # Following that example, in order to avoid intersecting edge_1 from network \"y\" twice with the same node,\n",
    "    # this MULTIPLE INTERSECTION ADAPTATION saves already performed intersections\n",
    "    performed_intersections = {}\n",
    "    \n",
    "    for idx, node in intersection_nodes.iterrows():\n",
    "        \n",
    "        # Current intersection_node's data\n",
    "        intersection_node_osmid = node['osmid']\n",
    "        intersected_u = node['u']\n",
    "        intersected_v = node['v']\n",
    "        intersected_key = node['key']\n",
    "        intersected_retain_how = node['retain_how']\n",
    "\n",
    "        # Helps debug:\n",
    "        #check_lst = [3991,3992]\n",
    "        #if intersection_node_osmid not in check_lst:\n",
    "        #    continue\n",
    "\n",
    "        # 1.1 --------------- MULTIPLE INTERSECTION ADAPTATION CHECK\n",
    "        # ------------------- This step reviews the performed_intersections dictionary to verify if the current intersection_node has\n",
    "        # ------------------- already split the current intersected edge\n",
    "\n",
    "        # Dictionary format:\n",
    "        # Each intersection_node stores the original_edge_id it intersected and the resulting edge_ids\n",
    "        # {intersection_node_osmid:{original_edge_id:[new_edge_ids]}}\n",
    "\n",
    "        # Find original unique edge_id of current edge being intersected\n",
    "        original_edge_id = str(intersected_u)+str(intersected_v)+str(intersected_key)\n",
    "        # Check dictionary\n",
    "        if intersection_node_osmid in performed_intersections.keys():\n",
    "            nodes_dictionary = performed_intersections[intersection_node_osmid]\n",
    "            if original_edge_id in nodes_dictionary.keys():\n",
    "                # Case A: The current intersection_node already intersected the current edge_id.\n",
    "                # Approach: Do not intersect again (continue)\n",
    "                if intersection_logs:\n",
    "                    print(f\"network_intersections_update(): Skipping intersection between osmid {intersection_node_osmid} and edge {original_edge_id}. It already happened.\")\n",
    "                continue\n",
    "            else:\n",
    "                # Case B: The current edge_id has not been intersected, but the intersection_node has already been used.\n",
    "                # Approach: Append the current edge_id to the node's intersected list.\n",
    "                nodes_dictionary[original_edge_id] = [] #Inserting new key for the new edge_id with an empty list of resulting edge_ids.\n",
    "                performed_intersections[intersection_node_osmid] = nodes_dictionary # Updating nodes_dictionary inside the general dictionary\n",
    "                if intersection_logs:\n",
    "                    print(f\"network_intersections_update(): Registering intersection between osmid {intersection_node_osmid} and edge {original_edge_id}.\")\n",
    "        else:\n",
    "            # Case C: The current intersection_node has not intersected any edge_id (First time for this node)\n",
    "            # Approach: Add to dictionary and create list\n",
    "            performed_intersections[intersection_node_osmid] = {original_edge_id:[]} # Inserting the node key, nodes_dictionary and its list\n",
    "            if intersection_logs:\n",
    "                print(f\"network_intersections_update(): First time registering intersection between osmid {intersection_node_osmid} and edge {original_edge_id}.\")\n",
    "        \n",
    "        # 1.2 --------------- Split the current_ntw's intersected edge using the intersection_node as clipping_point. \n",
    "        # ------------------- This split (Using function edge_clipping()) creates two separate edges:\n",
    "        # ------------------- The first edge will be related to the starting_point_gdf (We'll set intersected edge 'u')\n",
    "        # ------------------- The second edge will be related to the opposite side (Will be intersected edge 'v')\n",
    "    \n",
    "            \n",
    "        # 1.2.1 - Extract current intersection node as a gdf (Becomes clipping_point_gdf in function edge_clipping)\n",
    "        # ------- MULTIPLE INTERSECTION ADAPTATION:\n",
    "        # ------- In cases where the intersection_nodes always intersect two edges only, osmid is only needed to identify the current node.\n",
    "        # ------- Else, more data and dropping duplicates is required.\n",
    "        intersection_node_idx = (intersection_nodes.u==intersected_u)&(intersection_nodes.v==intersected_v)&(intersection_nodes.key==intersected_key)&(intersection_nodes.osmid==intersection_node_osmid)\n",
    "        intersection_node = intersection_nodes.loc[intersection_node_idx].copy()\n",
    "        intersection_node.drop_duplicates(inplace=True)\n",
    "        intersection_node.reset_index(inplace=True,drop=True)\n",
    "        if intersection_logs:\n",
    "            print(f\"network_intersections_update(): Printing intersection node for osmid {intersection_node_osmid}.\")\n",
    "            print(intersection_node)\n",
    "        \n",
    "        # 1.2.2 - Extract current_ntw's intersected edge (Becomes edge_gdf in function edge_clipping)\n",
    "        try:\n",
    "            # 1.2.2a TRY: Load the edge registered as intersected in the intersection_nodes gdf.\n",
    "            # (Using the edge's original u('intersected_u'), v('intersected_v') and key('intersected_key'))\n",
    "            intersected_edge = current_ntw_edges.loc[(current_ntw_edges['u'] == intersected_u) & \n",
    "                                                     (current_ntw_edges['v'] == intersected_v) &\n",
    "                                                     (current_ntw_edges['key'] == intersected_key)].copy()\n",
    "            intersected_edge.reset_index(inplace=True,drop=True)          \n",
    "            if len(intersected_edge) == 0:\n",
    "                # If it has len=0, it means that the edge no longer exists (deleted in following steps in this function).\n",
    "                # This happens because that edge had another intersection along its lenght and that original unsplit edge was split and deleted.\n",
    "                fail_try\n",
    "            if intersection_logs:\n",
    "                print(f\"network_intersections_update(): Intersection_node {intersection_node_osmid} found {len(intersected_edge)} edges.\")  \n",
    "                \n",
    "        except:\n",
    "            # 1.2.2b EXCEPT: Find the new (already split) edge by buffering the current intersection_node.\n",
    "            if intersection_logs:\n",
    "                print(f\"network_intersections_update(): Searching for already split edge originating from edge with u {intersected_u}, v {intersected_v} and key {intersected_key}.\")\n",
    "            # Create a VERY SMALL buffer around the intersection_node\n",
    "            intersection_node_buffer = intersection_node.buffer(1e-9)\n",
    "            intersection_node_buffer = gpd.GeoDataFrame(geometry=intersection_node_buffer)\n",
    "            # Find and rewrite the data of the split edge underneath the intersection_node\n",
    "            edge_data = intersection_node_buffer.sjoin(current_ntw_edges)\n",
    "            if len(edge_data) == 1:\n",
    "                # CASE A: Found one edge touching the intersection_node_buffer.\n",
    "                intersected_u = edge_data.u.unique()[0]\n",
    "                intersected_v = edge_data.v.unique()[0]\n",
    "                intersected_key = edge_data.key.unique()[0]\n",
    "            elif len(edge_data) == 0:\n",
    "                # CASE B: Found no edges touching the intersection_node_buffer. (Problem)\n",
    "                print(f\"ERROR: Problem on intersection_node {intersection_node_osmid}. Found {len(edge_data)} edges.\")\n",
    "                intended_crash\n",
    "            else:\n",
    "                # CASE C: Found multiple edges touching the intersection_node_buffer.\n",
    "                # ------- MULTIPLE INTERSECTION ADAPTATION\n",
    "                # ------- It is possible that the node is intersecting two (or more) edges and that some have already been clipped.\n",
    "                # ------- Remove from the found edge_data the already intersected edges in order to keep the ones that could be intersected.\n",
    "                # Create a unique edge_id for all edges found\n",
    "                edge_data['u'] = edge_data['u'].apply(lambda x: int(round(float(x),0)))\n",
    "                edge_data['v'] = edge_data['v'].apply(lambda x: int(round(float(x),0)))\n",
    "                edge_data['key'] = edge_data['key'].apply(lambda x: int(round(float(x),0)))\n",
    "                edge_data = create_unique_edge_id(edge_data)\n",
    "                # Discard already intersected edges\n",
    "                previously_created_edge_ids_lst = [] # Set an empty list of previously created edge_ids by current intersection_node\n",
    "                nodes_dictionary = performed_intersections[intersection_node_osmid] # Read the node's history\n",
    "                for original_edge_id in nodes_dictionary.keys(): # For each original_edge_id that the node has split\n",
    "                    for new_edge_id in nodes_dictionary[original_edge_id]: # Read the resulting new_edge_ids\n",
    "                        previously_created_edge_ids_lst.append(new_edge_id) # Register them to the list\n",
    "                edge_data = edge_data.loc[~edge_data.edge_id.isin(previously_created_edge_ids_lst)].copy() # Discard those edges\n",
    "                # Find the edge to be split\n",
    "                if len(edge_data) == 1:\n",
    "                    intersected_u = edge_data.u.unique()[0]\n",
    "                    intersected_v = edge_data.v.unique()[0]\n",
    "                    intersected_key = edge_data.key.unique()[0]\n",
    "                else:    \n",
    "                    # With the new MULTIPLE INTERSECTION ADAPTATION, if any intersection_node reaches this part of the code, it may be a problem.\n",
    "                    # It means that either zero or +2 edges with the same u, v and key are located where the intersection_node is.\n",
    "                    print(f\"ERROR: Problem on intersection_node {intersection_node_osmid}.\")\n",
    "                    print(f\"Found {len(edge_data)} edges while searching for substitute for edge {original_edge_id}.\")\n",
    "                    print(\"Printing edge_data\")\n",
    "                    print(edge_data)\n",
    "                    print(f\"Previously created edge_ids_lst: {previously_created_edge_ids_lst}.\")\n",
    "                    intended_crash\n",
    "            # Retrieve the found edge\n",
    "            intersected_edge = current_ntw_edges.loc[(current_ntw_edges['u'] == intersected_u) & \n",
    "                                                     (current_ntw_edges['v'] == intersected_v) &\n",
    "                                                     (current_ntw_edges['key'] == intersected_key)].copy()\n",
    "            intersected_edge.reset_index(inplace=True,drop=True)\n",
    "            if intersection_logs:\n",
    "                print(f\"Intersection_node {intersection_node_osmid} found already intersected edge u {intersected_u}, v {intersected_v}, key {intersected_key}.\")\n",
    "\n",
    "        # 1.2.3 - Clip the intersected_edge with the intersection_node\n",
    "        # Extract current_ntw's intersected edge's u node as a point\n",
    "        # (Always becomes starting_point_gdf in function edge_clipping when using 'return_all')\n",
    "        u_node = current_ntw_nodes.loc[(current_ntw_nodes['osmid'] == intersected_u)].copy()\n",
    "        u_node.reset_index(inplace=True,drop=True)\n",
    "        # Extract current_ntw's intersected edge's v node as a point\n",
    "        v_node = current_ntw_nodes.loc[(current_ntw_nodes['osmid'] == intersected_v)].copy()\n",
    "        v_node.reset_index(inplace=True,drop=True)\n",
    "        # Apply edge_clipping function and assign the corresponding 'u', 'v' or 'key' data.\n",
    "        if intersected_retain_how == 'both':\n",
    "            # Clip edge\n",
    "            split_edge_gdf = edge_clipping(starting_point_gdf = u_node,\n",
    "                                           edge_gdf = intersected_edge,\n",
    "                                           clipping_point_gdf = intersection_node,\n",
    "                                           projected_crs = projected_crs,\n",
    "                                           return_all = True,\n",
    "                                           function_logs = clipping_logs)\n",
    "            if intersection_logs:\n",
    "                print(f\"network_intersections_update(): Intersection_node {intersection_node_osmid} split the edge and created {len(split_edge_gdf)} new edges.\")\n",
    "            # Assign data\n",
    "            # When return_all=True in function edge_clipping, \n",
    "            # assigns 'starting' to the edge related to the starting_point_gdf\n",
    "            # and 'ending' to edge on the opposite side.\n",
    "            # Identify split edge 1\n",
    "            u_idx = split_edge_gdf.relation=='starting'\n",
    "            split_edge_gdf.loc[u_idx,'u'] = intersected_u # We assigned 'u' as starting_point_gdf\n",
    "            split_edge_gdf.loc[u_idx,'v'] = intersection_node_osmid # Intersection\n",
    "            split_edge_gdf.loc[u_idx,'key'] = 0 #Since this 'u' and 'v' relation is new, key=0\n",
    "            # Identify split edge 2\n",
    "            v_idx = split_edge_gdf.relation=='ending'\n",
    "            split_edge_gdf.loc[v_idx,'u'] = intersection_node_osmid # Intersection\n",
    "            split_edge_gdf.loc[v_idx,'v'] = intersected_v # Opposite side\n",
    "            split_edge_gdf.loc[v_idx,'key'] = 0 #Since this 'u' and 'v' relation is new, key=0\n",
    "\n",
    "            # ------- MULTIPLE INTERSECTION ADAPTATION\n",
    "            # Prepare new_edge_ids to register in dictionary\n",
    "            new_edge_id_1 = str(intersected_u)+str(intersection_node_osmid)+str(0)\n",
    "            new_edge_id_2 = str(intersection_node_osmid)+str(intersected_v)+str(0)\n",
    "            # Read from general dictionary the edge_ids previously created by this intersection_node in this original_edge\n",
    "            already_created_edge_ids = performed_intersections[intersection_node_osmid][original_edge_id] \n",
    "            # Add to the list the new generated edge_ids\n",
    "            already_created_edge_ids.append(new_edge_id_1)\n",
    "            already_created_edge_ids.append(new_edge_id_2)\n",
    "            # Insert data to general dictionary\n",
    "            performed_intersections[intersection_node_osmid][original_edge_id] = already_created_edge_ids\n",
    "        \n",
    "        elif intersected_retain_how == 'u':\n",
    "            # Clip edge\n",
    "            split_edge_gdf = edge_clipping(starting_point_gdf = u_node,\n",
    "                                           edge_gdf = intersected_edge,\n",
    "                                           clipping_point_gdf = intersection_node,\n",
    "                                           projected_crs = projected_crs,\n",
    "                                           return_all = False,\n",
    "                                           function_logs = clipping_logs)\n",
    "            if intersection_logs:\n",
    "                print(f\"network_intersections_update(): Intersection_node {intersection_node_osmid} split the edge and created {len(split_edge_gdf)} new edges.\")\n",
    "            # Assign data\n",
    "            split_edge_gdf.loc[0,'u'] = intersected_u\n",
    "            split_edge_gdf.loc[0,'v'] = intersection_node_osmid # Intersection\n",
    "            split_edge_gdf.loc[0,'key'] = 0 #Since this 'u' and 'v' relation is new, key=0\n",
    "        elif intersected_retain_how == 'v':\n",
    "            # Clip edge\n",
    "            split_edge_gdf = edge_clipping(starting_point_gdf = v_node,\n",
    "                                           edge_gdf = intersected_edge,\n",
    "                                           clipping_point_gdf = intersection_node,\n",
    "                                           projected_crs = projected_crs,\n",
    "                                           return_all = False,\n",
    "                                           function_logs = clipping_logs)\n",
    "            if intersection_logs:\n",
    "                print(f\"network_intersections_update(): Intersection_node {intersection_node_osmid} split the edge and created {len(split_edge_gdf)} new edges.\")\n",
    "            # Assign data\n",
    "            split_edge_gdf.loc[0,'u'] = intersection_node_osmid # Intersection\n",
    "            split_edge_gdf.loc[0,'v'] = intersected_v\n",
    "            split_edge_gdf.loc[0,'key'] = 0 #Since this 'u' and 'v' relation is new, key=0\n",
    "        else:\n",
    "            print(f\"ERROR splitting edge with u {intersected_u}, v {intersected_v} and key {intersected_key}.\")\n",
    "            print(\"Make sure to include in gdf intersection_nodes column 'retain_how' with either 'u','v' or 'both'.\")\n",
    "            intended_crash    \n",
    "    \n",
    "        # 1.3 --------------- Register changes on current_ntw\n",
    "        # ------------------- The intersection_node is concatenated into current_ntw_nodes.\n",
    "        # ------------------- The split edge(s) is(are) concatenated into current_ntw_edges.\n",
    "\n",
    "        # 1.3.1 - Register node\n",
    "        # Set an identifier to make it easier to locate nodes that resulted from an intersection between networks\n",
    "        intersection_node['intersecting'] = 1\n",
    "        # Prepare node for concatenation\n",
    "        intersection_node = intersection_node[['osmid','intersecting','geometry']]\n",
    "        # Add new node\n",
    "        current_ntw_nodes = pd.concat([current_ntw_nodes,intersection_node])\n",
    "        # Reset index\n",
    "        current_ntw_nodes.reset_index(inplace=True,drop=True)\n",
    "        if intersection_logs:\n",
    "                print(f\"network_intersections_update(): Concatenated {len(intersection_node)} nodes.\")\n",
    "\n",
    "        # 1.3.1 - Register edge(s)\n",
    "        # Keep all edges except the edge that was split\n",
    "        # (Must remove to avoid duplicating edge's geometries)\n",
    "        current_ntw_edges = current_ntw_edges.loc[~((current_ntw_edges['u'] == int(intersected_u)) &\n",
    "                                                    (current_ntw_edges['v'] == int(intersected_v)) &\n",
    "                                                    (current_ntw_edges['key'] == int(intersected_key)))].copy()\n",
    "        # Prepare edges for concatenation\n",
    "        split_edge_gdf = split_edge_gdf[['u','v','key','geometry']]\n",
    "        # Set an identifier to make it easier to locate edges that were split\n",
    "        split_edge_gdf['intersecting'] = 1\n",
    "        # Add new edge\n",
    "        current_ntw_edges = pd.concat([current_ntw_edges,split_edge_gdf])\n",
    "        # Reset index\n",
    "        current_ntw_edges.reset_index(inplace=True,drop=True)\n",
    "        if intersection_logs:\n",
    "            print(f\"network_intersections_update(): Concatenated {len(split_edge_gdf)} edges.\")\n",
    "\n",
    "    # 1.4 --------------- Format final output\n",
    "    # ------------------- Filters for columns of interest and sets column types\n",
    "    updated_ntw_nodes = current_ntw_nodes[['osmid','intersecting','geometry']].copy()\n",
    "    # Set unique identifiers to int\n",
    "    updated_ntw_nodes['osmid'] = updated_ntw_nodes['osmid'].astype('int')\n",
    "    del current_ntw_nodes\n",
    "    updated_ntw_edges = current_ntw_edges[['u','v','key','intersecting','geometry']].copy()\n",
    "    # Set unique identifiers to int\n",
    "    updated_ntw_edges['u'] = updated_ntw_edges['u'].astype('int')\n",
    "    updated_ntw_edges['v'] = updated_ntw_edges['v'].astype('int')\n",
    "    updated_ntw_edges['key'] = updated_ntw_edges['key'].astype('int')\n",
    "    del current_ntw_edges\n",
    "\n",
    "    print(f\"Finished updating network.\")\n",
    "    \n",
    "    # After iterating over both networks, return result\n",
    "    return updated_ntw_nodes, updated_ntw_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9048e27e-b4f4-48dd-9d08-01264f3da6a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Part 02 Step 04's update before function that removes duplicates from network was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0653f2-e7e6-4e68-ac72-f30f984b78bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply network_intersections_update() function\n",
    "# Splitting the process using col 'edge_origin' was necessary because function network_intersections_update\n",
    "# iterates over each osmid and intersection_nodes_3 can have 2 rows for each osmid.\n",
    "# (e.g. a consequential_intersection cuts 1 ntw_01 edge and 1 ntw_join edge in a point that's better suited for being the\n",
    "#  connection point, so both edges get cut at that intersection point).\n",
    "\n",
    "# Nodes get updated every time network_intersections_update runs (Creates node duplicates, will remove)\n",
    "joined_nodes_fix = concatenated_nodes.copy()\n",
    "# Gets updated by concatenating each iteration's joined_edges_current.\n",
    "# Each iteration updates the edges from each origin.\n",
    "# (Prevents crash from trying to split multiple close edges with the same node at once)\n",
    "joined_edges_fix = gpd.GeoDataFrame()\n",
    "\n",
    "edge_origins = ['ntw_01','ntw_02','ntw_join']\n",
    "\n",
    "for current_edge_origin in edge_origins:\n",
    "    print(f\"Fixing edges from edge_origin {current_edge_origin}.\")\n",
    "    # Isolate current origin\n",
    "    current_edges = joined_edges_concat.loc[joined_edges_concat.ntw_origin==current_edge_origin]\n",
    "    # Intersecting edges that were originaly located in edge_origin\n",
    "    intersection_nodes_3_current = intersection_nodes_3.loc[intersection_nodes_3.edge_origin==current_edge_origin]\n",
    "    # (Second round of intersections, derived from new nodes created from network_02 to edges on network_01)\n",
    "    joined_nodes_fix, joined_edges_current = network_intersections_update(current_ntw_nodes = joined_nodes_fix,\n",
    "                                                                          current_ntw_edges = current_edges,\n",
    "                                                                          intersection_nodes = intersection_nodes_3_current,\n",
    "                                                                          projected_crs = projected_crs)\n",
    "    \n",
    "    # Store the edge_ids created to join both networks\n",
    "    joined_edges_current = create_unique_edge_id(joined_edges_current)\n",
    "    join_idx = joined_edges_current.intersecting==1\n",
    "    edge_ids = list(joined_edges_current.loc[join_idx].edge_id.unique())\n",
    "    all_join_edgeids = all_join_edgeids + edge_ids\n",
    "    \n",
    "    # Concatenate resulting edges\n",
    "    joined_edges_fix = pd.concat([joined_edges_fix,joined_edges_current])\n",
    "\n",
    "# Drop node duplicates and cols 'intersecting'\n",
    "joined_nodes_fix.drop(columns=['intersecting'],inplace=True)\n",
    "joined_nodes_fix.drop_duplicates(inplace=True)\n",
    "joined_edges_fix.drop(columns=['intersecting'],inplace=True)\n",
    "\n",
    "# Reset indexes\n",
    "joined_nodes_fix.reset_index(inplace=True,drop=True)\n",
    "joined_edges_fix.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Show\n",
    "print(joined_nodes_fix.dtypes)\n",
    "print(joined_nodes_fix.shape)\n",
    "joined_nodes_fix.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
