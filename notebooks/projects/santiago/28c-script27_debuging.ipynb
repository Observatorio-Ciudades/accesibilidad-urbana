{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187e380d-2ca3-4079-a2ea-ee307a7a4b8a",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab229e4-de16-4044-b788-43693b2f748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from shapely.geometry import Point\n",
    "import osmnx as ox\n",
    "\n",
    "from tqdm import tqdm\n",
    "import h3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    import aup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f286a-afe0-4b34-a71d-3a966382b97d",
   "metadata": {},
   "source": [
    "## Parameters and required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e2bd8f-8749-4df1-8e05-804f4b1b592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################### DATA FOR PART 1 and 2 ###########################################################\n",
    "############################################## NAVIGABLE NETWORK AND PROXIMITY DATA ##################################################\n",
    "\n",
    "# ------------------------------ BASE DATA REQUIRED ------------------------------\n",
    "\n",
    "# --------------- LIST OF POIS TO BE EXAMINED\n",
    "# This list should contain the source_name that will be assigned to each processed poi.\n",
    "# That source_name will be stored in a 'source' column at first and be turned into a column name after all pois are processed.\n",
    "# That source_name must also be the name of the file stored in pois_dir (.gpkg)\n",
    "# e.g if source_list = ['vacunatorio_pub'], vacanatorio_pub.gpkg must exist.\n",
    "\n",
    "source_list = ['carniceria','hogar','bakeries','supermercado','banco', #supplying-wellbeing\n",
    "               #supplying-sociability\n",
    "               'ferias','local_mini_market','correos', \n",
    "               #supplying-environmental impact\n",
    "               'centro_recyc',\n",
    "\n",
    "               #caring-wellbeing\n",
    "               'hospital_priv','hospital_pub','clinica_priv','clinica_pub','farmacia','vacunatorio_priv','vacunatorio_pub','consult_ado_priv','consult_ado_pub','salud_mental','labs_priv','residencia_adumayor',\n",
    "               #caring-sociability\n",
    "               'eq_deportivo_priv','eq_deportivo_pub','club_deportivo',\n",
    "               #caring-environmental impact [areal data: 'noise','temp']\n",
    "\n",
    "               #living-wellbeing\n",
    "               'civic_office','tax_collection','social_security','police','bomberos',\n",
    "               #living-sociability [areal data: 'houses','social_viv','hotel']\n",
    "               #living-environmental impact [areal_data: 'inter']\n",
    "               \n",
    "               #enjoying-wellbeing [areal data: 'ndvi']\n",
    "               'museos_priv','museos_pub','cines','sitios_historicos',\n",
    "               #enjoying-sociability\n",
    "               'restaurantes_bar_cafe','librerias','ep_plaza_small',\n",
    "               #enjoying-environmental impact\n",
    "               'ep_plaza_big',\n",
    "\n",
    "               #learning-wellbeing\n",
    "               'edu_basica_pub','edu_media_pub','jardin_inf_pub','universidad', 'edu_tecnica',\n",
    "               #learning-sociability\n",
    "               'edu_adultos_pub','edu_especial_pub','bibliotecas',\n",
    "               #learning-environmental impact\n",
    "               'centro_edu_amb',\n",
    "\n",
    "               #working-wellbeing\n",
    "               'paradas_tp_ruta','paradas_tp_metro','paradas_tp_tren',\n",
    "               #working-sociability [areal data: 'oficinas']\n",
    "               #working-environmental impact\n",
    "               'ciclovias','estaciones_bicicletas']\n",
    "\n",
    "# --------------- UNIQUE ID POIS (Special proximity cases)\n",
    "# From source_list, sources that have an unique ID and require special processing (id_pois_time() function instead of pois_time() function)\n",
    "# Unique ID for each of them is 'ID'.\n",
    "unique_id_sources = ['ferias','ep_plaza_small','ep_plaza_big','ciclovias']\n",
    "\n",
    "# --------------- LOCAL INPUT AND OUTPUT DIRECTORIES\n",
    "# IMPORTANT NOTE: Make sure all directories exist.\n",
    "# general directory (All directories derive from here)\n",
    "gral_dir = '../../../data/external/santiago/'\n",
    "\n",
    "# Dir 1 - If not using OSMnx network (INPUT NETWORK), will use this file to create a navigable network (create_filtered_navigable_network() function)\n",
    "public_space_quality_dir = gral_dir + \"calidad_ep/red_buena_calidad.shp\"\n",
    "\n",
    "# \"calidad_ep/redvial2019_buffer_3750m_c_utilidad_2.shp\"\n",
    "# \"calidad_ep/red_buena_calidad.shp\"\n",
    "# \"calidad_ep/red_buena_calidad_pza_italia.shp\"\n",
    "\n",
    "# Dir 2 - Local directory where pois files are located\n",
    "all_pois_dir = gral_dir + \"pois/\"\n",
    "\n",
    "# Dir 3 - Local directory where areas of analysis are located (Used in PART 2 and PART 3)\n",
    "areas_dir = gral_dir + \"areas_of_analysis/\"\n",
    "\n",
    "# Dir 4 - Local directory where outputs are saved\n",
    "local_save_dir = gral_dir + \"output/\"\n",
    "\n",
    "# Dir 5 - Local directory where areal data is located\n",
    "areal_dir = gral_dir + 'areal_data/'\n",
    "\n",
    "# --------------- AREA OF INTEREST\n",
    "# Area of interest (aoi)\n",
    "aoi_schema = 'projects_research'\n",
    "aoi_table = 'santiago_aoi'\n",
    "# 'AM_Santiago' represents Santiago's metropolitan area, 'alamedabuffer_4500m' also available\n",
    "city = 'alamedabuffer_4500m'\n",
    "\n",
    "# --------------- PROJECTION\n",
    "# Projection to be used whenever necessary\n",
    "projected_crs = 'EPSG:32719'\n",
    "\n",
    "# --------------- METHODOLOGY\n",
    "# Pois proximity methodology - Count pois at a given time proximity? (If true, second tupple value is distance in minutes)\n",
    "count_pois = (True,15)\n",
    "\n",
    "# walking_speed (float): Decimal number containing walking speed (in km/hr) to be used if prox_measure=\"length\",\n",
    "#\t\t\t\t\t\t or if prox_measure=\"time_min\" but needing to fill time_min NaNs.\n",
    "walking_speed_list = [4.5] #[3.5,4.5,5,12,24,20,40]\n",
    "\n",
    "# --------------- INPUT NETWORK\n",
    "# If using previously downloaded OSMnx network available in database, set following to true\n",
    "osmnx_network = False\n",
    "# If true, set schemas and tables\n",
    "network_schema = 'projects_research'\n",
    "edges_table = 'santiago_edges'\n",
    "nodes_table = 'santiago_nodes'\n",
    "# Else (osmnx_network = False), set external network data (Allows for filtering network according to a given column value)\n",
    "# IMPORTANT NOTE: Make sure public_space_quality_dir file exists.\n",
    "filtering_column = 'pje_ep'\n",
    "filtering_value = 0.5 # Will keep equal or more than this value\n",
    "\n",
    "# --------------- SAVING SPACE IN DISK\n",
    "# Save space in disk by deleting data that won't be used again?\n",
    "save_space = True\n",
    "\n",
    "# --------------- SAVING DATA\n",
    "# IMPORTANT NOTE: Make sure local_save_dir exists\n",
    "local_save = True # save output to local?\n",
    "\n",
    "###################################################### DATA FOR PART 3 ###############################################################\n",
    "########################################################### HQSL #####################################################################\n",
    "\n",
    "# --------------- PARAMETERS AND WEIGHT DICTS\n",
    "# Structure: {social_functions:{themes:[source_names]}}\n",
    "parameters_dict = {'supplying':{'wellbeing':['carniceria', #Accessibility to Butcher/Fish Shops\n",
    "                                            'hogar', #Accessibility to Hardware/Paint Shops\n",
    "                                            #Not available: Accessibility to Greengrocers\n",
    "                                            'bakeries', #Accessibility to Bakeries and delis\n",
    "                                            'supermercado',#Accessibility to supermarkets\n",
    "                                            'banco'#Accessibility to bank\n",
    "                                            ],\n",
    "                                'sociability':['ferias',#Accessibility to city fairs/markets\n",
    "                                            'local_mini_market',#Accessibility to local and mini markets\n",
    "                                            'correos'#ADDED: MAIL SERVICE\n",
    "                                            ],\n",
    "                                'environmental_impact':['centro_recyc'#Accessibility to recycling center\n",
    "                                                        #Not available: Accessibility to compost\n",
    "                                                    ]\n",
    "                            },\n",
    "                'caring':{'wellbeing':['hospital', #Accessibility to hospital\n",
    "                                        'clinica',#Accessibility to public clinics\n",
    "                                        'farmacia',#Accessibility to pharmacies\n",
    "                                        'vacunatorio',#Accessibility to vaccination center\n",
    "                                        'consult_ado',#Accessibility to optician/audiologist(###ADDED DENTIST)\n",
    "                                        'salud_mental',###ADDED: MENTAL HEALTH\n",
    "                                        'labs_priv',###ADDED: LABORATORIES\n",
    "                                        'residencia_adumayor'###ADDED: ELDERLY PERMANENT RESIDENCIES\n",
    "                                        ],\n",
    "                            'sociability':['eq_deportivo',#Accessibility to sports equipments\n",
    "                                            'club_deportivo'#Accessibility to sport clubs\n",
    "                                        ],\n",
    "                            'environmental_impact':['noise',\n",
    "                                                    'temp'\n",
    "                                #Not available: Air polution\n",
    "                                                    ]\n",
    "                            },\n",
    "                'living':{'wellbeing':['civic_office',#Accessibility to civic offices\n",
    "                                        #Not available: Number of street bentches\n",
    "                                        'tax_collection',#ADDED: AFIP(TAX COLLECTOR)\n",
    "                                        'social_security',#ADDED: SOCIAL SECURITY\n",
    "                                        'police',#Accessibility to police(###MOVED FROM LIVING TO CARING)\n",
    "                                        'bomberos'#Accessibility to fire stations\n",
    "                                        #Not available: Accessibility to street lamp\n",
    "                                        ],\n",
    "                            'sociability':['houses',#Accessibility to permanent residencies\n",
    "                                            'social_viv',#Accessibility to social housing\n",
    "                                            #Not available: Accessibility to student housing\n",
    "                                            'hotel'#ADDED: HOTELS\n",
    "                                        ],\n",
    "                            'environmental_impact':['inter',\n",
    "                                                    #Not available: Corrected compactness\n",
    "                                                    #Not available: Width of sidewalks\n",
    "                                                    ],\n",
    "                            },\n",
    "                'enjoying':{'wellbeing':['museos',#Accessibility to museums\n",
    "                                            #Not available: Accessibility to theater,operas\n",
    "                                            'cines',#Accessibility to cinemas\n",
    "                                            'sitios_historicos',#Accessibility to historical places\n",
    "                                            'ndvi'#Number of trees\n",
    "                                        ],\n",
    "                            'sociability':['restaurantes_bar_cafe',#Accessibility to bars/cafes + Accessibility to restaurants\n",
    "                                            'librerias',#Accessibility to record and book stores, galleries, fairs\n",
    "                                            #Not available: Accessibility to cultural and/or formative spaces\n",
    "                                            #Not available: Accessibility to places of workship\n",
    "                                            'ep_plaza_small'#Accessibility to boulevards, linear parks, small squares + Accessibility to squares\n",
    "                                            ],\n",
    "                            'environmental_impact':['ep_plaza_big'#Accessibility to big parks\n",
    "                                                    #Not available: Accessibility to shared gardens\n",
    "                                                    #Not available: Accessibility to urban playgrounds\n",
    "                                                    ]\n",
    "                            },\n",
    "                'learning':{'wellbeing':['edu_basica_pub',#'edu_basica_priv',#Accessibility to public elementary school\n",
    "                                            'edu_media_pub',#'edu_media_priv',#Accessibility to public high school\n",
    "                                            'jardin_inf_pub',#'jardin_inf_priv',#Similar to Accessibility to childcare\n",
    "                                            'universidad',#Accessibility to university\n",
    "                                            'edu_tecnica',#ADDED: TECHNICAL EDUCATION\n",
    "                                        ],\n",
    "                            'sociability':['edu_adultos_pub',#'edu_adultos_priv',#Accessibility to adult formation centers\n",
    "                                            'edu_especial_pub',#'edu_especial_priv',#Accessibility to specialized educational centers\n",
    "                                            #Not available: Accesibility to establishments and services for disabled adults\n",
    "                                            'bibliotecas'#Accessibility to libraries(###MOVED FROM ENJOYING TO LEARNING)\n",
    "                                            ],\n",
    "                            'environmental_impact':['centro_edu_amb'#Accessibility to centers for learning environmental activities\n",
    "                                                    #Not available: Accessibility to gardening schools\n",
    "                                                    ],\n",
    "                            },\n",
    "                'working':{'wellbeing':['paradas_tp_ruta',#Accessibility to bus stop\n",
    "                                        'paradas_tp_metro',#Accessibility to metro\n",
    "                                        'paradas_tp_tren'#Accessibility to train stop\n",
    "                                        ],\n",
    "                            'sociability':['oficinas'#Accessibility to office\n",
    "                                            #Not available: Accessibility to incubators\n",
    "                                            #Not available: AccSeveral other articles cite 60dB as a safe noise zone. essibility to coworking places\n",
    "                                        ],\n",
    "                            'environmental_impact':['ciclovias',\n",
    "                                                    'estaciones_bicicletas'#Accessibility to bike lanes\n",
    "                                                    #Not available: Accessibility to shared bike stations\n",
    "                                                    ]\n",
    "                            }\n",
    "                }\n",
    "\n",
    "weight_dict = {'carniceria':'rare', #SUPPLYING\n",
    "            'hogar':'rare',\n",
    "            'bakeries':'rare',\n",
    "            'supermercado':'rare',\n",
    "            'banco':'rare',\n",
    "            'ferias':'rare',\n",
    "            'local_mini_market':'rare',\n",
    "            'correos':'very_rare',\n",
    "            'centro_recyc':'rare',\n",
    "            #CARING\n",
    "            'hospital':'very_rare',\n",
    "            'clinica':'rare',\n",
    "            'farmacia':'rare',\n",
    "            'vacunatorio':'very_rare',\n",
    "            'consult_ado':'very_rare',\n",
    "            'salud_mental':'very_rare',\n",
    "            'labs_priv':'very_rare',\n",
    "            'residencia_adumayor':'rare',\n",
    "            'eq_deportivo':'rare',\n",
    "            'club_deportivo':'rare',\n",
    "            'noise':'specific',\n",
    "            'temp':'specific',\n",
    "            #LIVING\n",
    "            'civic_office':'rare', \n",
    "            'tax_collection':'very_rare',\n",
    "            'social_security':'very_rare',\n",
    "            'police':'very_rare',\n",
    "            'bomberos':'very_rare',\n",
    "            'houses':'specific',\n",
    "            'social_viv':'specific',\n",
    "            'hotel':'rare',\n",
    "            'inter':'specific',\n",
    "            #ENJOYING\n",
    "            'museos':'very_rare',\n",
    "            'cines':'very_rare',\n",
    "            'sitios_historicos':'rare',\n",
    "            'ndvi':'specific',\n",
    "            'restaurantes_bar_cafe':'frequent',\n",
    "            'librerias':'rare',\n",
    "            'ep_plaza_small':'frequent',\n",
    "            'ep_plaza_big':'rare',\n",
    "            #LEARNING\n",
    "            'edu_basica_pub':'rare', \n",
    "            'edu_media_pub':'rare',\n",
    "            'jardin_inf_pub':'rare',\n",
    "            'universidad':'very_rare',\n",
    "            'edu_tecnica':'very_rare',\n",
    "            'edu_adultos_pub':'rare',\n",
    "            'edu_especial_pub':'rare',\n",
    "            'bibliotecas':'very_rare',\n",
    "            'centro_edu_amb':'very_rare',\n",
    "            #WORKING\n",
    "            'paradas_tp_ruta':'frequent',\n",
    "            'paradas_tp_metro':'very_rare',\n",
    "            'paradas_tp_tren':'very_rare',\n",
    "            'oficinas':'specific',\n",
    "            'ciclovias':'rare',\n",
    "            'estaciones_bicicletas':'rare',\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0db590e-cef9-4639-bc2e-535d18be163f",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14010d-8ecf-4c78-95a5-a9c2a2e99f2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create navigable network function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d64ea4-c2ac-458e-b25e-4643d7b59923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filtered_navigable_network(public_space_quality_dir, projected_crs, filtering_column, filtering_value):\n",
    "\n",
    "    # 1.0 --------------- LOAD DATA\n",
    "    # ------------------- This step loads the public space quality index gdf for the current project\n",
    "    # Load data\n",
    "    pub_space_qty = gpd.read_file(public_space_quality_dir)\n",
    "    # Set CRS\n",
    "    pub_space_qty = pub_space_qty.set_crs(projected_crs)\n",
    "    # Filter for data of relevance\n",
    "    gdf = pub_space_qty[[filtering_column,'geometry']].copy()\n",
    "    gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "    # 2.0 --------------- EXTRACT VERTICES\n",
    "    # ------------------- This step extracts points from each linestring and stores them in gdf_points.\n",
    "    # Explode multi-part geometries into single parts\n",
    "    gdf_exploded = gdf.explode(index_parts=False)\n",
    "    # Reset index\n",
    "    gdf_exploded.reset_index(inplace=True)\n",
    "    gdf_exploded.drop(columns=['index'],inplace=True)\n",
    "    #Initialize an empty list to store the points and its values\n",
    "    points = []\n",
    "    attributes = []\n",
    "    #Iterate through each LineString and extract its vertices\n",
    "    for idx, row in gdf_exploded.iterrows():\n",
    "        line = row.geometry\n",
    "        for coord in line.coords:\n",
    "            points.append(Point(coord))\n",
    "            attributes.append(row[filtering_column])\n",
    "    # Create a new GeoDataFrame from the points\n",
    "    gdf_points = gpd.GeoDataFrame(attributes,geometry=points)\n",
    "    # Rename data\n",
    "    gdf_points.rename(columns={0:filtering_column},inplace=True)\n",
    "\n",
    "    # 3.0 --------------- CREATE NODES AND EDGES COMPATIBLE WITH OSMnx AND FILTER THEM.\n",
    "    # ------------------- This step uses the lines and points available to create nodes and edges, then filters by filtering value.\n",
    "    # Create nodes and edges\n",
    "    nodes = gdf_points.copy()\n",
    "    edges = gdf_exploded.copy()\n",
    "\n",
    "    # Set gdf CRS\n",
    "    try:\n",
    "        nodes = nodes.to_crs(\"EPSG:4326\")\n",
    "    except:\n",
    "        nodes = nodes.set_crs(\"EPSG:4326\")\n",
    "    try:\n",
    "        edges = edges.to_crs(\"EPSG:4326\")\n",
    "    except:\n",
    "        edges = edges.set_crs(\"EPSG:4326\")\n",
    "\n",
    "    nodes, edges = aup.create_network(nodes, edges, projected_crs)\n",
    "    # Filter them\n",
    "    edges_filt = edges.loc[edges[filtering_column] >= filtering_value]\n",
    "\n",
    "    # 4.0 --------------- CREATE NAVIGABLE NETWORK\n",
    "    # ------------------- This step creates G from the previous nodes and edges_filt.\n",
    "    # Format nodes and edges\n",
    "    nodes_gdf = nodes.copy()\n",
    "    nodes_gdf.set_index('osmid',inplace=True)\n",
    "    edges_gdf = edges_filt.copy()\n",
    "    edges_gdf.set_index(['u','v','key'],inplace=True)\n",
    "    # Set x and y columns\n",
    "    nodes_gdf['x'] = nodes_gdf['geometry'].x\n",
    "    nodes_gdf['y'] = nodes_gdf['geometry'].y\n",
    "    # Create network G\n",
    "    G = ox.graph_from_gdfs(nodes_gdf, edges_gdf)\n",
    "\n",
    "    return G, nodes_gdf, edges_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4adb18-32bb-4e34-98b1-6c3535a6e491",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Scale functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc97a443-ea2a-4994-83fa-f3754fb61f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################\n",
    "# SCALE FUNCTIONS\n",
    "\n",
    "def rare_fn(cont):\n",
    "    if cont == 0:\n",
    "        res_val = 0\n",
    "    elif cont > 0 and cont < 2:\n",
    "        res_val = res_val_regression(0, 2, 0, 2.5, cont)\n",
    "    elif cont >= 2 and cont < 4:\n",
    "        res_val = res_val_regression(2, 4, 2.5, 5, cont)\n",
    "    elif cont >= 4 and cont < 7:\n",
    "        res_val = res_val_regression(4, 7, 5, 7.5, cont)\n",
    "    elif cont >= 7 and cont < 10:\n",
    "        res_val = res_val_regression(7, 10, 7.5, 10, cont)\n",
    "    elif cont >= 10:\n",
    "        res_val = 10\n",
    "    \n",
    "    return res_val\n",
    "\n",
    "\n",
    "def very_rare_fn(cont):\n",
    "    min_x = 0\n",
    "    max_x = 1\n",
    "    min_y = 0\n",
    "    max_y = 10\n",
    "    \n",
    "    return res_val_regression(min_x, max_x, min_y, max_y, cont)\n",
    "\n",
    "\n",
    "def frequent_fn(cont):\n",
    "    if cont == 0:\n",
    "        res_val = 0\n",
    "    elif cont > 0 and cont < 6:\n",
    "        res_val = res_val_regression(0, 6, 0, 2.5, cont)\n",
    "    elif cont >= 6 and cont < 12:\n",
    "        res_val = res_val_regression(6, 12, 2.5, 5, cont)\n",
    "    elif cont >= 12 and cont < 18:\n",
    "        res_val = res_val_regression(12, 18, 5, 7.5, cont)\n",
    "    elif cont >= 18 and cont < 25:\n",
    "        res_val = res_val_regression(18, 25, 7.5, 10, cont)\n",
    "    elif cont >= 25:\n",
    "        res_val = 10\n",
    "    \n",
    "    return res_val\n",
    "\n",
    "\n",
    "def res_val_regression(min_x, max_x, min_y, max_y, cont):\n",
    "    slope = (max_y-min_y)/(max_x-min_x)\n",
    "    intersect = min_y - slope * min_x\n",
    "    res_val = slope * cont + intersect\n",
    "    if cont > max_x:\n",
    "        res_val = max_y\n",
    "        \n",
    "    return res_val\n",
    "\n",
    "\n",
    "def office_fn(cont):\n",
    "    if cont == 0:\n",
    "        res_val = 0\n",
    "    elif cont > 0 and cont < 2.823938308:\n",
    "        res_val = res_val_regression(0, 2.823938308, 0, 2.5, cont)\n",
    "    elif cont >= 2.823938308 and cont <  5.539263604:\n",
    "        res_val = res_val_regression(2.823938308, 5.539263604, 2.5, 5, cont)\n",
    "    elif cont >= 5.539263604 and cont < 10.96991420:\n",
    "        res_val = res_val_regression(5.539263604, 10.96991420, 5, 7.5, cont)\n",
    "    elif cont >= 10.96991420 and cont < 16.40056479:\n",
    "        res_val = res_val_regression(10.96991420, 16.40056479, 7.5, 10, cont)\n",
    "    elif cont >= 16.40056479:\n",
    "        res_val = 10\n",
    "    \n",
    "    return res_val\n",
    "\n",
    "\n",
    "def ndvi_fn(cont):\n",
    "    min_x = 0\n",
    "    max_x = 0.4\n",
    "    min_y = 0\n",
    "    max_y = 10\n",
    "    if cont > max_x:\n",
    "        return 10\n",
    "    elif cont <= min_x:\n",
    "        return 0\n",
    "    else:\n",
    "        return res_val_regression(min_x, max_x, min_y, max_y, cont)\n",
    "\n",
    "\n",
    "def inter_fn(cont):\n",
    "    min_x = 20\n",
    "    max_x = 100\n",
    "    min_y = 0\n",
    "    max_y = 10\n",
    "    if cont > max_x:\n",
    "        return 10\n",
    "    elif cont < min_x:\n",
    "        return 0\n",
    "    else:\n",
    "        return res_val_regression(min_x, max_x, min_y, max_y, cont)\n",
    "\n",
    "\n",
    "def noise_fn(cont):\n",
    "    min_x = 55\n",
    "    max_x = 70\n",
    "    min_y = 10\n",
    "    max_y = 0\n",
    "    if cont > max_x:\n",
    "        return 0\n",
    "    elif cont < min_x:\n",
    "        return 10\n",
    "    else:\n",
    "        return res_val_regression(min_x, max_x, min_y, max_y, cont)\n",
    "\n",
    "\n",
    "def temp_fn(area_analysis, cont, mean, std):\n",
    "    if cont >= (mean + 2*std):\n",
    "        res_val = 0\n",
    "    elif cont < (mean + 2*std) and cont >= (mean + std):\n",
    "        res_val = res_val_regression((mean + std), (mean + 2*std), 2.5, 0, cont)\n",
    "    elif cont < (mean + std) and cont >= (mean):\n",
    "        res_val = res_val_regression((mean), (mean + std), 5, 2.5, cont)\n",
    "    elif cont < (mean) and cont >= (mean - std):\n",
    "        res_val = res_val_regression((mean - std), (mean), 7.5, 5, cont)\n",
    "    elif cont < (mean - std) and cont >= (mean - 2*std):\n",
    "        res_val = res_val_regression((mean - 2*std), (mean - std), 10, 7.5, cont)\n",
    "    elif cont < (mean - 2*std):\n",
    "        res_val = 10\n",
    "    if area_analysis == 'santiago':\n",
    "        res_val = 5\n",
    "    \n",
    "    return res_val\n",
    "\n",
    "\n",
    "def household_fn(cont):\n",
    "    res_val = res_val_regression(0, 50, 0, 10, cont)\n",
    "    \n",
    "    return res_val\n",
    "\n",
    "    \n",
    "def social_viv_fn(cont):\n",
    "    min_x = 0\n",
    "    max_x = 20\n",
    "    min_y = 0\n",
    "    max_y = 10\n",
    "    if cont > max_x:\n",
    "        return 10\n",
    "    elif cont < min_x:\n",
    "        return 0\n",
    "    else:\n",
    "        return res_val_regression(min_x, max_x, min_y, max_y, cont)\n",
    "    \n",
    "\n",
    "def specific_fn(cont, source, area_analysis, mean, std):\n",
    "    if 'ndvi' in source:\n",
    "        return ndvi_fn(cont)\n",
    "    elif 'inter' in source:\n",
    "        return inter_fn(cont)\n",
    "    elif 'noise' in source:\n",
    "        return noise_fn(cont)\n",
    "    elif 'temp' in source:\n",
    "        return temp_fn(area_analysis, cont, mean, std)\n",
    "    elif 'houses' in source:\n",
    "        return household_fn(cont)\n",
    "    elif 'social_viv' in source:\n",
    "        return social_viv_fn(cont)\n",
    "    elif 'oficinas' in source:\n",
    "        return office_fn(cont)\n",
    "\n",
    "\n",
    "def scale_source_fn(cont, source, weight_dict, area_analysis, mean, std):\n",
    "    if weight_dict[source] == 'rare':\n",
    "        return rare_fn(cont)\n",
    "    elif weight_dict[source] == 'very_rare':\n",
    "        return very_rare_fn(cont)\n",
    "    elif weight_dict[source] == 'frequent':\n",
    "        return frequent_fn(cont)\n",
    "    elif weight_dict[source] == 'specific':\n",
    "        return specific_fn(cont, source, area_analysis, mean, std)\n",
    "    \n",
    "\n",
    "def neighbour_mean(hex_id, hex_id_name, hex_bins, col_name):\n",
    "    return hex_bins.loc[hex_bins[hex_id_name].isin(h3.k_ring(hex_id,1)),col_name].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adfb440-bd63-4103-86c7-d66b68e86919",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### HQSL Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3f6bb1-2973-4f79-b684-a46a057e8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################\n",
    "# HQSL FUNCTIONS\n",
    "\n",
    "def hqsl_fn(hex_gdf, parameters_dict, code_column):\n",
    "\n",
    "    hex_gdf = hex_gdf.copy()\n",
    "    \n",
    "    social_function_list = []\n",
    "    \n",
    "    for social_function in parameters_dict.keys():\n",
    "        social_function_list.append(social_function)\n",
    "    \n",
    "    hex_gdf['hqsl'] = hex_gdf[social_function_list].sum(axis=1)\n",
    "\n",
    "    base_columns = [code_column,'geometry']\n",
    "    filter_list = ['hqsl']\n",
    "    filter_list.extend(base_columns)\n",
    "    hex_gdf = hex_gdf[filter_list].copy()\n",
    "    \n",
    "    return hex_gdf\n",
    "\n",
    "\n",
    "def social_fn(hex_gdf, parameters_dict, code_column):\n",
    "    \n",
    "    hex_gdf = hex_gdf.copy()\n",
    "    \n",
    "    for social_function in parameters_dict.keys():\n",
    "        source_list = []\n",
    "        \n",
    "        for indicator in parameters_dict[social_function].keys():\n",
    "            source_list.extend(parameters_dict[social_function][indicator])\n",
    "        \n",
    "        source_list = [s+'_scaled' for s in source_list]\n",
    "        hex_gdf[social_function] = hex_gdf[source_list].mean(axis=1)\n",
    "\n",
    "    base_columns = [code_column,'geometry']\n",
    "    filter_list = list(parameters_dict.keys())\n",
    "    filter_list.extend(base_columns)\n",
    "    hex_gdf = hex_gdf[filter_list].copy()\n",
    "    \n",
    "    return hex_gdf\n",
    "\n",
    "\n",
    "def indicator_fn(hex_gdf, parameters_dict, code_column):\n",
    "    hex_ind = hex_gdf.copy()\n",
    "\n",
    "    filter_list = []\n",
    "    \n",
    "    indicator_list = list(set().union(*parameters_dict.values()))\n",
    "    for indicator in indicator_list:\n",
    "        social_indicator = []\n",
    "        \n",
    "        for social_function in parameters_dict.keys():\n",
    "            social_indicator.append(indicator+'_'+social_function)\n",
    "            \n",
    "            source_indicator = parameters_dict[social_function][indicator]\n",
    "            source_indicator = [s+'_scaled' for s in source_indicator]\n",
    "            \n",
    "            hex_ind[indicator+'_'+social_function] = hex_ind[source_indicator].mean(axis=1)\n",
    "    \n",
    "        hex_ind[indicator] = hex_ind[social_indicator].sum(axis=1)\n",
    "        filter_list.extend(social_indicator)\n",
    "        filter_list.append(indicator)\n",
    "    \n",
    "    base_columns = [code_column,'geometry']\n",
    "    filter_list.extend(base_columns)\n",
    "    hex_ind = hex_ind[filter_list].copy()\n",
    "            \n",
    "    return hex_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e84665-9b74-4334-8ceb-48e9d61068ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c407aa98-1b6a-4692-a272-4222fd315e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################\n",
    "# MAIN FUNCTION\n",
    "\n",
    "def main(source_list, aoi, G, nodes, edges, walking_speed, local_save):\n",
    "    \n",
    "    ############################################################### PART 1 ###############################################################\n",
    "    #################################################### FIND NODES PROXIMITY TO POIS ####################################################\n",
    "    # ------------------- This step loads each source of interest, calculates nodes proximity and saves it to nodes_analysis\n",
    "\n",
    "    print(f\"STARTING PART 1: NODES PROXIMITY TO POINTS OF INTEREST.\")\n",
    "\n",
    "    k = len(source_list)\n",
    "    i = 1\n",
    "    all_source_cols =[]\n",
    "\n",
    "    for source in source_list:\n",
    "\n",
    "        source_cols =[]\n",
    "\n",
    "        # ----------\n",
    "        # UNIQUE ID CONSIDERATION\n",
    "        # Check if current source has a unique ID that needs to be considered in the process\n",
    "        if source in unique_id_sources:\n",
    "            unique_id = True\n",
    "        else:\n",
    "            unique_id = False\n",
    "        # ----------\n",
    "\n",
    "        print(\"--\"*40)\n",
    "        print(f\"--- Starting nodes proximity to pois for source {i}/{k}: {source}. \")\n",
    "\n",
    "        # 1.1) Read pois from pois dir\n",
    "        print(f\"--- Source {i}/{k} (1.1) - Reading pois dir.\")\n",
    "        # Directory where pois to be examined are located\n",
    "        pois_dir = all_pois_dir + f'{source}.gpkg'\n",
    "        # Load all pois from directory\n",
    "        pois = gpd.read_file(pois_dir)\n",
    "\n",
    "        # ----------\n",
    "        # UNIQUE ID AND SMALL PARKS CONSIDERATION\n",
    "        if unique_id:\n",
    "            if source == 'ep_plaza_small':\n",
    "                # For small parks, area is relevant to sub-divide process (below 2000m2 --> pois_time(), above 2000m2 --> id_pois_time())\n",
    "                pois = pois[['area_ha','ID','geometry']]\n",
    "            else:\n",
    "                # For the rest, keep already existing unique ID and geometry\n",
    "                pois = pois[['ID','geometry']]\n",
    "        else:\n",
    "            # If not unique_ID, ID col is source name (irrelevant), keeps geometry\n",
    "            pois['ID'] = source\n",
    "            pois = pois[['ID','geometry']]\n",
    "        # ----------\n",
    "\n",
    "        # Format\n",
    "        try:\n",
    "            pois = pois.to_crs(\"EPSG:4326\")\n",
    "        except:\n",
    "            pois = pois.set_crs(\"EPSG:4326\")\n",
    "\n",
    "        # 1.2) Clip pois to aoi\n",
    "        source_pois = gpd.sjoin(pois, aoi)\n",
    "\n",
    "        # ----------\n",
    "        # SMALL PARKS CONSIDERATION\n",
    "        if source == 'ep_plaza_small':\n",
    "            # For small parks, area is relevant to sub-divide process\n",
    "            source_pois = source_pois[['area_ha','ID','geometry']]\n",
    "        else:\n",
    "            source_pois = source_pois[['ID','geometry']]\n",
    "        # ----------\n",
    "\n",
    "        print(f\"--- Source {i}/{k} (1.2) - Keeping {len(source_pois)} pois inside aoi from original {len(pois)} pois.\")\n",
    "\n",
    "        if save_space:\n",
    "            del pois\n",
    "\n",
    "        # 1.3) Calculate nodes proximity\n",
    "        # ----------\n",
    "        # UNIQUE ID AND SMALL PARKS CONSIDERATION\n",
    "        if unique_id:\n",
    "            #################################################### SMALL PARKS ONLY [SECTION STARTS]\n",
    "            if source == 'ep_plaza_small':\n",
    "                print(f\"--- Source {i}/{k} (1.3) - Calculating nodes proximity for special case.\")\n",
    "\n",
    "                # pois_time() [for public spaces below 2000m2]\n",
    "                # For VERY small public spaces (below 2000m2), the proximity analysis will consider any poi derived from the geometry of interest (goi, polygon) because anyway it is small.\n",
    "                # Because we just care about one poi only (any), this step filters and drops duplicate IDs, keeping the first occurrence.\n",
    "                very_small_source_pois = source_pois.loc[source_pois['area_ha']<0.2].copy().drop_duplicates(subset='ID')\n",
    "                # Calculate time data from nodes to source for very_small_source_pois (Has 1 pois for each goi)\n",
    "                print(f\"--- Calculating very small {source} nodes proximity with function pois_time().\")\n",
    "                source_nodes_time_1 = aup.pois_time(G, nodes, edges, very_small_source_pois, source,'length',\n",
    "                                                    walking_speed, count_pois, projected_crs)\n",
    "                if save_space:\n",
    "                    del very_small_source_pois\n",
    "                \n",
    "                # id_pois_time() [for public spaces above 2000m2]\n",
    "                # For larger public spaces (above 2000m2), having several accesses becomes relevant, and goi IDs becomes necessary (needs id_pois_time() function)\n",
    "                small_source_pois = source_pois.loc[source_pois['area_ha']>=0.2].copy()\n",
    "                # Calculate time data from nodes to source for small_source_pois (Has n pois for each goi, needs goi_id)\n",
    "                print(f\"--- Calculating not that small {source} nodes proximity with function id_pois_time().\")\n",
    "                source_nodes_time_2 = aup.id_pois_time(G, nodes, edges, small_source_pois, source,'length',\n",
    "                                                       walking_speed, goi_id='ID', count_pois=count_pois, projected_crs=projected_crs)\n",
    "                if save_space:\n",
    "                    del small_source_pois\n",
    "\n",
    "                # Now merge source_nodes_time_1 results with source_nodes_time_2 results\n",
    "                if count_pois[0]:\n",
    "                    source_nodes_time_all = source_nodes_time_1.merge(source_nodes_time_2[['osmid', 'time_'+source, f'{source}_{count_pois[1]}min']],on='osmid')\n",
    "                else:\n",
    "                    source_nodes_time_all = source_nodes_time_1.merge(source_nodes_time_2[['osmid', 'time_'+source]],on='osmid')\n",
    "\n",
    "                if save_space:\n",
    "                    del source_nodes_time_1\n",
    "                    del source_nodes_time_2\n",
    "                \n",
    "                # For time data, find *min* time between both source_nodes_time.\n",
    "                time_cols = [f'time_{source}_x', f'time_{source}_y']\n",
    "                source_nodes_time_all[f'time_{source}'] = source_nodes_time_all[time_cols].min(axis=1)\n",
    "                source_nodes_time_all.drop(columns=time_cols,inplace=True)\n",
    "\n",
    "                # For count data, find *sum* of counted pois for both source_nodes_time\n",
    "                if count_pois[0]:\n",
    "                    count_cols = [f'{source}_{count_pois[1]}min_x',f'{source}_{count_pois[1]}min_y']\n",
    "                    source_nodes_time_all[f'{source}_{count_pois[1]}min'] = source_nodes_time_all[count_cols].sum(axis=1)\n",
    "                    source_nodes_time_all.drop(columns=count_cols,inplace=True)\n",
    "\n",
    "                # Finally, rename result\n",
    "                source_nodes_time = source_nodes_time_all.copy()\n",
    "                \n",
    "                if save_space:\n",
    "                    del source_nodes_time_all\n",
    "            #################################################### SMALL PARKS ONLY [SECTION ENDS]\n",
    "\n",
    "            else:\n",
    "                print(f\"--- Source {i}/{k} (1.3) - Calculating nodes proximity for unique ID case.\")\n",
    "                # Function id_pois_time() consideres the unique ID belonging to each geometry of interest (goi).\n",
    "                source_nodes_time = aup.id_pois_time(G, nodes, edges, source_pois, source, 'length', walking_speed, \n",
    "                                                    goi_id='ID', count_pois=count_pois, projected_crs=projected_crs)\n",
    "        else:\n",
    "            print(f\"--- Source {i}/{k} (1.3) - Calculating nodes proximity for regular case.\")\n",
    "            # Function pois_time() calculates proximity data from nodes to source (all) without considering any unique ID.\n",
    "            source_nodes_time = aup.pois_time(G, nodes, edges, source_pois, source,'length',walking_speed, \n",
    "                                              count_pois, projected_crs)\n",
    "        # ----------\n",
    "\n",
    "        if save_space:\n",
    "            del source_pois\n",
    "\n",
    "        #### Changes when comparing to Script 23, 23b and notebook 04b:\n",
    "        # Previously we formated nodes analysis as tidy format in order to be able to loop-upload nodes proximity data.\n",
    "        # That was relevant as new data was flowing each day. However, that's no longer needed.\n",
    "        # Instead, data is formated directly and added to nodes_analysis\n",
    "        ####\n",
    "\n",
    "        # 1.4) New nodes_analysis format (Not tidy data)\n",
    "        # Rename time column\n",
    "        source_nodes_time.rename(columns={'time_'+source:f'{source}_time'}, inplace=True)\n",
    "        # Register time column\n",
    "        source_cols.append(f'{source}_time') # Current source only\n",
    "        all_source_cols.append(f'{source}_time') # All sources, this list will be used in PART 2.\n",
    "\n",
    "        # Rename and format count column\n",
    "        if count_pois[0]:\n",
    "            source_nodes_time.rename(columns={f'{source}_{count_pois[1]}min':f'{source}_count_{count_pois[1]}min'}, inplace=True)\n",
    "            source_nodes_time[f'{source}_count_{count_pois[1]}min'] = source_nodes_time[f'{source}_count_{count_pois[1]}min'].astype(int)\n",
    "            # Register count column\n",
    "            source_cols.append(f'{source}_count_{count_pois[1]}min') # Current source only\n",
    "            all_source_cols.append(f'{source}_count_{count_pois[1]}min') # All sources, this list will be used in PART 2.\n",
    "\n",
    "        # Create or append to nodes_analysis\n",
    "        if i == 1:\n",
    "            nodes_analysis = source_nodes_time[['osmid','geometry']+source_cols]\n",
    "            print(f\"--- Source {i}/{k} (1.4) - Created nodes analysis with {len(source_nodes_time)} for the first time.\")\n",
    "        else:\n",
    "            nodes_analysis = nodes_analysis.merge(source_nodes_time[['osmid']+source_cols], on='osmid', how='left')\n",
    "            print(f\"--- Source {i}/{k} (1.4) - Appended {len(source_nodes_time)} nodes to nodes analysis.\")\n",
    "        \n",
    "        if save_space:\n",
    "            del source_nodes_time\n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    ############################################################### PART 2 ###############################################################\n",
    "    #################################################### NODES DATA TO AREA OF ANALYSIS ##################################################\n",
    "    # Avoid overestimating universities\n",
    "    nodes_analysis.loc[nodes_analysis.universidad_count_15min > 3, 'universidad_count_15min'] = 3\n",
    "\n",
    "    area_dict = {'unidadesvecinales':'COD_UNICO_',\n",
    "                 'zonascensales':'GEOCODI',\n",
    "                 'hex':'hex_id'\n",
    "                 }\n",
    "    \n",
    "    k = len(area_dict.keys())\n",
    "    i = 1\n",
    "\n",
    "    for area_analysis in area_dict.keys():\n",
    "\n",
    "        print(f\"CALCULATING PROXIMITY AND HQSL FOR AREA OF ANALYSIS {i}/{k}: {area_analysis}.\")\n",
    "\n",
    "        print(f\"--- STARTING PART 2: NODES DATA TO {area_analysis}.\")\n",
    "\n",
    "        # 2.1 --------------- LOAD AND FORMAT AREA OF ANALYSIS GDF\n",
    "        # ------------------- This step loads the current area of analysis and prepares it as an empty container\n",
    "\n",
    "        code_column = area_dict[area_analysis]\n",
    "        \n",
    "        # Load area of analysis gdf\n",
    "        if area_analysis == 'unidadesvecinales':\n",
    "            gdf = gpd.read_file(areas_dir+\"santiago_unidadesvecinales_zonaurbana.geojson\")\n",
    "            gdf = gdf[[code_column,'geometry']].copy()\n",
    "            print(f\"--- Area of analysis {i}/{k} (2.1) - Loaded area of analysis gdf.\")\n",
    "\n",
    "        elif area_analysis == 'zonascensales':\n",
    "            gdf = gpd.read_file(areas_dir+\"zonas_censales_hogares_RM.shp\")\n",
    "            gdf = gdf[[code_column,'geometry']].copy()\n",
    "            print(f\"--- Area of analysis {i}/{k} (2.1) - Loaded area of analysis gdf.\")\n",
    "\n",
    "        elif area_analysis == 'hex':\n",
    "            # For this script, will only use res=10\n",
    "            res = 10\n",
    "\n",
    "            gdf = aup.create_hexgrid(aoi, res)\n",
    "            gdf.rename(columns={f'hex_id_{res}':'hex_id'}, inplace=True)\n",
    "            gdf['res'] = res\n",
    "            gdf = gdf[[code_column,'res','geometry']].copy()  \n",
    "            print(f\"--- Area of analysis {i}/{k} (2.1) - Created {len(gdf)} hexagons at resolution {res}.\")\n",
    "        \n",
    "        # Set gdf CRS\n",
    "        try:\n",
    "            gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "        except:\n",
    "            gdf = gdf.set_crs(\"EPSG:4326\")\n",
    "        \n",
    "        # Explode area of analysis gdf\n",
    "        gdf = gdf.explode(ignore_index=True)\n",
    "\n",
    "        # Clip area of analysis gdf to area of interest \n",
    "        # (Data available within area of interest only, not clipping causes problems when computing neighbors data.)\n",
    "        gdf_cut = gpd.sjoin(gdf, aoi[['geometry']])\n",
    "        gdf_cut.drop(columns=['index_right'],inplace=True)\n",
    "        gdf = gdf_cut.copy()\n",
    "\n",
    "        # 2.2 --------------- GROUP DATA BY AREA OF ANALYSIS\n",
    "        # ------------------- This groups proximity data by area of analysis\n",
    "\n",
    "        if area_analysis == 'hex':\n",
    "\n",
    "            hex_gdf = gdf.copy()\n",
    "            poly_proximity = gpd.GeoDataFrame()\n",
    "\n",
    "            for r in hex_gdf.res.unique():\n",
    "\n",
    "                # Calculate mean proximity within area of analysis\n",
    "                hex_tmp = hex_gdf[hex_gdf.res == r].copy()\n",
    "                hex_tmp = aup.group_by_hex_mean(nodes_analysis, hex_tmp, r, all_source_cols, 'hex_id')\n",
    "                hex_tmp = hex_tmp.drop(columns=['res_x','res_y'])\n",
    "                hex_tmp['res'] = r\n",
    "                print(f\"--- Area of analysis {i}/{k} (2.2) - Calculated mean proximity for {len(hex_tmp)} hexagons at resolution {r}.\")\n",
    "\n",
    "                # Merge to poly_proximity gdf\n",
    "                poly_proximity = pd.concat([poly_proximity, hex_tmp], \n",
    "                                           ignore_index = True, \n",
    "                                           axis = 0)\n",
    "                print(f\"--- Area of analysis {i}/{k} (2.2) - Merged {len(hex_tmp)} hexagons to poly_proximity gdf.\")\n",
    "\n",
    "                del hex_tmp\n",
    "        \n",
    "        # If not hex\n",
    "        else:\n",
    "            r = 0 # no resolution needed for polygons different from h3 hexagons\n",
    "            poly_proximity = aup.group_by_hex_mean(nodes_analysis, gdf, r, all_source_cols, code_column)\n",
    "        print(f\"--- Area of analysis {i}/{k} (2.2) - Calculated mean proximity for {len(poly_proximity)} polygons.\")\n",
    "\n",
    "\n",
    "        # 2.3 --------------- FINAL FORMAT AND SAVE\n",
    "        # ------------------- This step gives final formating to proximity data and saves it localy\n",
    "        print(f\"--- Area of analysis {i}/{k} (2.3) - Giving final format and saving {area_analysis} proximity data.\")\n",
    "\n",
    "        poly_proximity = poly_proximity.set_geometry('geometry')\n",
    "        try:\n",
    "            poly_proximity = poly_proximity.to_crs(\"EPSG:4326\")\n",
    "        except:\n",
    "            poly_proximity = poly_proximity.set_crs(\"EPSG:4326\")\n",
    "        \n",
    "        poly_proximity['city'] = 'Santiago'\n",
    "\n",
    "        if local_save:\n",
    "            area_proximity_table = f\"santiago_{area_analysis}proximity_{str_walk_speed}_kmh.gpkg\"\n",
    "            poly_proximity.to_file(local_save_dir + area_proximity_table, driver='GPKG')\n",
    "            print(f\"--- Area of analysis {i}/{k} (2.3) - Saved {area_analysis} proximity data locally.\")\n",
    "\n",
    "    ########################################################## PART 3 ####################################################################\n",
    "    ########################################################### HQSL #####################################################################\n",
    "\n",
    "        print(f\"--- STARTING PART 3 (HQSL) FOR {area_analysis}.\")\n",
    "\n",
    "        prox_gdf = poly_proximity.copy()\n",
    "\n",
    "        # 3.1 --------------- AREAL DATA\n",
    "        # ------------------- This step loads areal data (Not processed through proximity analysis).\n",
    "        print(f\"--- Area of analysis {i}/{k} (3.1) - Loading areal data.\")\n",
    "\n",
    "        if area_analysis == 'hex':\n",
    "            poly_areal = gpd.read_file(areal_dir+f'{area_analysis}_areal_res{res}.gpkg')\n",
    "        else:\n",
    "            poly_areal = gpd.read_file(areal_dir+f'{area_analysis}_areal.gpkg')\n",
    "            \n",
    "        poly_areal = poly_areal.rename(columns={'oficinas_sum':'oficinas_count',\n",
    "                                                'pct_social_viv':'social_viv_count',\n",
    "                                                'viv_sum':'houses_count',\n",
    "                                                'pct_hotel':'hotel_count',\n",
    "                                                'ndvi_mean':'ndvi_count'})\n",
    "        \n",
    "        # Clip poly_aereal gdf to area of interest \n",
    "        # (Data available within area of interest only, not clipping causes problems when computing neighbors data.)\n",
    "        poly_areal_cut = gpd.sjoin(poly_areal, aoi[['geometry']])\n",
    "        poly_areal_cut.drop(columns=['index_right'],inplace=True)\n",
    "        poly_areal = poly_areal_cut.copy()\n",
    "        \n",
    "        # 3.2 --------------- DATA TREATMENT\n",
    "        # ------------------- This step prepares proximity data and merges it with areal data\n",
    "        print(f\"--- Area of analysis {i}/{k} (3.2) - Joining _priv and _pub pois in {area_analysis}.\")\n",
    "        \n",
    "        join_pois_list = ['hospital','clinica','consult_ado', 'museos','vacunatorio','eq_deportivo',]\n",
    "        \n",
    "        for source in join_pois_list:\n",
    "            # join count columns for private and public in one encompassing column\n",
    "            prox_gdf[f\"{source}_count_15min\"] = prox_gdf[f\"{source}_priv_count_15min\"] + prox_gdf[f\"{source}_pub_count_15min\"]\n",
    "            # remove 0 values from time\n",
    "            prox_gdf.loc[prox_gdf[f\"{source}_pub_time\"]==0] = np.nan\n",
    "            prox_gdf.loc[prox_gdf[f\"{source}_priv_time\"]==0] = np.nan\n",
    "            # assign general minimum time\n",
    "            prox_gdf[f\"{source}_time\"] = prox_gdf[[f\"{source}_pub_time\", f\"{source}_priv_time\"]].min(axis=1)\n",
    "            # remove duplicate info columns\n",
    "            prox_gdf = prox_gdf.drop(columns=[f\"{source}_pub_count_15min\", f\"{source}_priv_count_15min\",\n",
    "                                              f\"{source}_pub_time\", f\"{source}_priv_time\"])\n",
    "            # fill na with 0 for future processing\n",
    "            prox_gdf['hospital_time'].fillna(0, inplace=True)\n",
    "\n",
    "        # Merge areal and proximity data\n",
    "        poly_analysis = poly_areal.merge(prox_gdf.drop(columns='geometry'), on=code_column, how='left')\n",
    "        poly_analysis = poly_analysis.explode(ignore_index=True)\n",
    "        poly_analysis = poly_analysis.dissolve(by=code_column)\n",
    "        poly_analysis = poly_analysis.reset_index()\n",
    "\n",
    "        # 3.3 --------------- HQSL Function - Variables analysis\n",
    "        # ------------------- This step scales data\n",
    "        print(f\"--- Area of analysis {i}/{k} (3.3) - Processing variables analysis.\")\n",
    "        # ------------------------------\n",
    "        # use scale functions for each column\n",
    "        for j in tqdm(range(len(weight_dict.keys())),position=0,leave=True):\n",
    "            # gather specific source\n",
    "            source = list(weight_dict.keys())[j]\n",
    "            # iterate over columns\n",
    "            for col_name in poly_analysis.columns:\n",
    "                # select column with count information -- refers to the amount of opportunities available at 15 min\n",
    "                if source in col_name and 'count' in col_name:\n",
    "                    if f'{source}_time' in poly_analysis.columns:\n",
    "                        poly_analysis[f'{source}_time'].fillna(0, inplace=True)\n",
    "                    poly_analysis[col_name].fillna(0, inplace=True)\n",
    "\n",
    "                    # source scaling\n",
    "                    poly_analysis[f'{source}_scaled'] = poly_analysis[col_name].apply(lambda x:scale_source_fn(x,\n",
    "                                                                                                               source,\n",
    "                                                                                                               weight_dict,\n",
    "                                                                                                               area_analysis,\n",
    "                                                                                                               poly_analysis[col_name].mean(),\n",
    "                                                                                                               poly_analysis[col_name].std()))\n",
    "                    # treat 0 time values -- hexagons without nodes \n",
    "                    if area_analysis == 'hex':\n",
    "                        if weight_dict[source] != 'specific':\n",
    "                            # assign nan values to hexagons without nodes to avoid affecting the mean calculation process\n",
    "                            #if source in join_pois_list:\n",
    "                            #    hex_analysis.loc[hex_analysis.supermercado_time==0,f'{source}_scaled'] = np.nan\n",
    "                            if source == 'hotel' or source == 'oficinas':\n",
    "                                continue\n",
    "                            else:\n",
    "                                poly_analysis.loc[poly_analysis[f'{source}_time']==0,f'{source}_scaled'] = np.nan\n",
    "                                \n",
    "                            # calculate mean count value\n",
    "                            poly_analysis.loc[poly_analysis[f'{source}_time']==0, f'{source}_scaled'] = poly_analysis.loc[poly_analysis[f'{source}_time']==0].apply(lambda x: neighbour_mean(x['hex_id'],\n",
    "                                                                                                                                                                                             'hex_id',\n",
    "                                                                                                                                                                                             poly_analysis,\n",
    "                                                                                                                                                                                             f'{source}_scaled'), axis=1)\n",
    "        # 3.4 --------------- HQSL Function - HQSL Index calculation\n",
    "        # ------------------- This step calculates HQSL\n",
    "        print(f\"--- Area of analysis {i}/{k} (3.4) - Calculating HQSL.\")\n",
    "\n",
    "        # ------------------------------\n",
    "        hex_ind = indicator_fn(poly_analysis, parameters_dict, code_column)\n",
    "        hex_social_fn = social_fn(poly_analysis, parameters_dict, code_column)\n",
    "        hex_hqsl = hqsl_fn(hex_social_fn, parameters_dict, code_column)\n",
    "        \n",
    "        hex_idx = hex_ind.merge(hex_social_fn.drop(columns='geometry'), on=code_column)\n",
    "        hex_idx = hex_idx.merge(hex_hqsl.drop(columns='geometry'), on=code_column)\n",
    "\n",
    "        # 3.5 --------------- SAVING\n",
    "        # ------------------- This step saves HQSL result.\n",
    "        if area_analysis == 'hex':\n",
    "            hex_idx['res'] = res\n",
    "        \n",
    "        hex_idx = hex_idx.dropna()\n",
    "                                \n",
    "        if local_save:\n",
    "            print(f\"--- Area of analysis {i}/{k} (3.5) - Saving HQSL index locally.\")\n",
    "            hex_idx.to_file(gral_dir +'output/'+ f'santiago_{area_analysis}analysis_{str_walk_speed}_kmh.gpkg', driver='GPKG')\n",
    "        \n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9103e4-50cb-4825-832d-3dd3da0d28af",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa3877c5-7bee-4998-a94f-ec2906e7d2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Downloading area of interest.\n",
      "--- Converting local data to OSMnx format network.\n",
      "------------------------------------------------------------------------------------------\n",
      "--- Running Script for speed: 4.5km/hr.\n",
      "STARTING PART 1: NODES PROXIMITY TO POINTS OF INTEREST.\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 1/51: carniceria. \n",
      "--- Source 1/51 (1.1) - Reading pois dir.\n",
      "--- Source 1/51 (1.2) - Keeping 228 pois inside aoi from original 775 pois.\n",
      "--- Source 1/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each carniceria.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for carniceria.\n",
      "Starting range k = 1 of 2 for carniceria.\n",
      "Starting range k = 2 of 2 for carniceria.\n",
      "Finished time analysis for carniceria.\n",
      "--- Source 1/51 (1.4) - Created nodes analysis with 14537 for the first time.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 2/51: hogar. \n",
      "--- Source 2/51 (1.1) - Reading pois dir.\n",
      "--- Source 2/51 (1.2) - Keeping 493 pois inside aoi from original 1185 pois.\n",
      "--- Source 2/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each hogar.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for hogar.\n",
      "Starting range k = 1 of 3 for hogar.\n",
      "Starting range k = 2 of 3 for hogar.\n",
      "Starting range k = 3 of 3 for hogar.\n",
      "Finished time analysis for hogar.\n",
      "--- Source 2/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 3/51: bakeries. \n",
      "--- Source 3/51 (1.1) - Reading pois dir.\n",
      "--- Source 3/51 (1.2) - Keeping 367 pois inside aoi from original 928 pois.\n",
      "--- Source 3/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each bakeries.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for bakeries.\n",
      "Starting range k = 1 of 2 for bakeries.\n",
      "Starting range k = 2 of 2 for bakeries.\n",
      "Finished time analysis for bakeries.\n",
      "--- Source 3/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 4/51: supermercado. \n",
      "--- Source 4/51 (1.1) - Reading pois dir.\n",
      "--- Source 4/51 (1.2) - Keeping 140 pois inside aoi from original 515 pois.\n",
      "--- Source 4/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each supermercado.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for supermercado.\n",
      "Starting range k = 1 of 1 for supermercado.\n",
      "Finished time analysis for supermercado.\n",
      "--- Source 4/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 5/51: banco. \n",
      "--- Source 5/51 (1.1) - Reading pois dir.\n",
      "--- Source 5/51 (1.2) - Keeping 681 pois inside aoi from original 1637 pois.\n",
      "--- Source 5/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each banco.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for banco.\n",
      "Starting range k = 1 of 4 for banco.\n",
      "Starting range k = 2 of 4 for banco.\n",
      "Starting range k = 3 of 4 for banco.\n",
      "Starting range k = 4 of 4 for banco.\n",
      "Finished time analysis for banco.\n",
      "--- Source 5/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 6/51: ferias. \n",
      "--- Source 6/51 (1.1) - Reading pois dir.\n",
      "--- Source 6/51 (1.2) - Keeping 395 pois inside aoi from original 1855 pois.\n",
      "--- Source 6/51 (1.3) - Calculating nodes proximity for unique ID case.\n",
      "Found and assigned nearest node osmid to each ferias.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for ferias.\n",
      "Calculating proximity data for geometry of interest (goi) 1 of 93 for ferias.\n",
      "0% done.\n",
      "First geometry of interest (goi)'s time.\n",
      "First batch count.\n",
      "Calculating proximity data for geometry of interest (goi) 10 of 93 for ferias.\n",
      "10% done.\n",
      "Calculating proximity data for geometry of interest (goi) 19 of 93 for ferias.\n",
      "20% done.\n",
      "Calculating proximity data for geometry of interest (goi) 28 of 93 for ferias.\n",
      "30% done.\n",
      "Calculating proximity data for geometry of interest (goi) 38 of 93 for ferias.\n",
      "40% done.\n",
      "Calculating proximity data for geometry of interest (goi) 47 of 93 for ferias.\n",
      "50% done.\n",
      "Calculating proximity data for geometry of interest (goi) 56 of 93 for ferias.\n",
      "60% done.\n",
      "Calculating proximity data for geometry of interest (goi) 66 of 93 for ferias.\n",
      "70% done.\n",
      "Calculating proximity data for geometry of interest (goi) 75 of 93 for ferias.\n",
      "80% done.\n",
      "Calculating proximity data for geometry of interest (goi) 84 of 93 for ferias.\n",
      "90% done.\n",
      "Calculating proximity data for geometry of interest (goi) 93 of 93 for ferias.\n",
      "100% done.\n",
      "Finished time analysis for ferias.\n",
      "--- Source 6/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 7/51: local_mini_market. \n",
      "--- Source 7/51 (1.1) - Reading pois dir.\n",
      "--- Source 7/51 (1.2) - Keeping 1439 pois inside aoi from original 3049 pois.\n",
      "--- Source 7/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each local_mini_market.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for local_mini_market.\n",
      "Starting range k = 1 of 8 for local_mini_market.\n",
      "Starting range k = 2 of 8 for local_mini_market.\n",
      "Starting range k = 3 of 8 for local_mini_market.\n",
      "Starting range k = 4 of 8 for local_mini_market.\n",
      "Starting range k = 5 of 8 for local_mini_market.\n",
      "Starting range k = 6 of 8 for local_mini_market.\n",
      "Starting range k = 7 of 8 for local_mini_market.\n",
      "Starting range k = 8 of 8 for local_mini_market.\n",
      "Finished time analysis for local_mini_market.\n",
      "--- Source 7/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 8/51: correos. \n",
      "--- Source 8/51 (1.1) - Reading pois dir.\n",
      "--- Source 8/51 (1.2) - Keeping 24 pois inside aoi from original 63 pois.\n",
      "--- Source 8/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each correos.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for correos.\n",
      "Starting range k = 1 of 1 for correos.\n",
      "Finished time analysis for correos.\n",
      "--- Source 8/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 9/51: centro_recyc. \n",
      "--- Source 9/51 (1.1) - Reading pois dir.\n",
      "--- Source 9/51 (1.2) - Keeping 379 pois inside aoi from original 1239 pois.\n",
      "--- Source 9/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each centro_recyc.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for centro_recyc.\n",
      "Starting range k = 1 of 2 for centro_recyc.\n",
      "Starting range k = 2 of 2 for centro_recyc.\n",
      "Finished time analysis for centro_recyc.\n",
      "--- Source 9/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 10/51: hospital_priv. \n",
      "--- Source 10/51 (1.1) - Reading pois dir.\n",
      "--- Source 10/51 (1.2) - Keeping 5 pois inside aoi from original 6 pois.\n",
      "--- Source 10/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each hospital_priv.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for hospital_priv.\n",
      "Starting range k = 1 of 1 for hospital_priv.\n",
      "Finished time analysis for hospital_priv.\n",
      "--- Source 10/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 11/51: hospital_pub. \n",
      "--- Source 11/51 (1.1) - Reading pois dir.\n",
      "--- Source 11/51 (1.2) - Keeping 16 pois inside aoi from original 33 pois.\n",
      "--- Source 11/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each hospital_pub.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for hospital_pub.\n",
      "Starting range k = 1 of 1 for hospital_pub.\n",
      "Finished time analysis for hospital_pub.\n",
      "--- Source 11/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 12/51: clinica_priv. \n",
      "--- Source 12/51 (1.1) - Reading pois dir.\n",
      "--- Source 12/51 (1.2) - Keeping 92 pois inside aoi from original 241 pois.\n",
      "--- Source 12/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each clinica_priv.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for clinica_priv.\n",
      "Starting range k = 1 of 1 for clinica_priv.\n",
      "Finished time analysis for clinica_priv.\n",
      "--- Source 12/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 13/51: clinica_pub. \n",
      "--- Source 13/51 (1.1) - Reading pois dir.\n",
      "--- Source 13/51 (1.2) - Keeping 68 pois inside aoi from original 402 pois.\n",
      "--- Source 13/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each clinica_pub.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for clinica_pub.\n",
      "Starting range k = 1 of 1 for clinica_pub.\n",
      "Finished time analysis for clinica_pub.\n",
      "--- Source 13/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 14/51: farmacia. \n",
      "--- Source 14/51 (1.1) - Reading pois dir.\n",
      "--- Source 14/51 (1.2) - Keeping 922 pois inside aoi from original 2405 pois.\n",
      "--- Source 14/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each farmacia.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for farmacia.\n",
      "Starting range k = 1 of 5 for farmacia.\n",
      "Starting range k = 2 of 5 for farmacia.\n",
      "Starting range k = 3 of 5 for farmacia.\n",
      "Starting range k = 4 of 5 for farmacia.\n",
      "Starting range k = 5 of 5 for farmacia.\n",
      "Finished time analysis for farmacia.\n",
      "--- Source 14/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 15/51: vacunatorio_priv. \n",
      "--- Source 15/51 (1.1) - Reading pois dir.\n",
      "--- Source 15/51 (1.2) - Keeping 13 pois inside aoi from original 20 pois.\n",
      "--- Source 15/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each vacunatorio_priv.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for vacunatorio_priv.\n",
      "Starting range k = 1 of 1 for vacunatorio_priv.\n",
      "Finished time analysis for vacunatorio_priv.\n",
      "--- Source 15/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 16/51: vacunatorio_pub. \n",
      "--- Source 16/51 (1.1) - Reading pois dir.\n",
      "--- Source 16/51 (1.2) - Keeping 1 pois inside aoi from original 1 pois.\n",
      "--- Source 16/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each vacunatorio_pub.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for vacunatorio_pub.\n",
      "Starting range k = 1 of 1 for vacunatorio_pub.\n",
      "Finished time analysis for vacunatorio_pub.\n",
      "--- Source 16/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 17/51: consult_ado_priv. \n",
      "--- Source 17/51 (1.1) - Reading pois dir.\n",
      "--- Source 17/51 (1.2) - Keeping 36 pois inside aoi from original 80 pois.\n",
      "--- Source 17/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each consult_ado_priv.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for consult_ado_priv.\n",
      "Starting range k = 1 of 1 for consult_ado_priv.\n",
      "Finished time analysis for consult_ado_priv.\n",
      "--- Source 17/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 18/51: consult_ado_pub. \n",
      "--- Source 18/51 (1.1) - Reading pois dir.\n",
      "--- Source 18/51 (1.2) - Keeping 1 pois inside aoi from original 3 pois.\n",
      "--- Source 18/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each consult_ado_pub.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for consult_ado_pub.\n",
      "Starting range k = 1 of 1 for consult_ado_pub.\n",
      "Finished time analysis for consult_ado_pub.\n",
      "--- Source 18/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 19/51: salud_mental. \n",
      "--- Source 19/51 (1.1) - Reading pois dir.\n",
      "--- Source 19/51 (1.2) - Keeping 10 pois inside aoi from original 40 pois.\n",
      "--- Source 19/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each salud_mental.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for salud_mental.\n",
      "Starting range k = 1 of 1 for salud_mental.\n",
      "Finished time analysis for salud_mental.\n",
      "--- Source 19/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 20/51: labs_priv. \n",
      "--- Source 20/51 (1.1) - Reading pois dir.\n",
      "--- Source 20/51 (1.2) - Keeping 14 pois inside aoi from original 21 pois.\n",
      "--- Source 20/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each labs_priv.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for labs_priv.\n",
      "Starting range k = 1 of 1 for labs_priv.\n",
      "Finished time analysis for labs_priv.\n",
      "--- Source 20/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 21/51: residencia_adumayor. \n",
      "--- Source 21/51 (1.1) - Reading pois dir.\n",
      "--- Source 21/51 (1.2) - Keeping 121 pois inside aoi from original 368 pois.\n",
      "--- Source 21/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each residencia_adumayor.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for residencia_adumayor.\n",
      "Starting range k = 1 of 1 for residencia_adumayor.\n",
      "Finished time analysis for residencia_adumayor.\n",
      "--- Source 21/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 22/51: eq_deportivo_priv. \n",
      "--- Source 22/51 (1.1) - Reading pois dir.\n",
      "--- Source 22/51 (1.2) - Keeping 200 pois inside aoi from original 1111 pois.\n",
      "--- Source 22/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each eq_deportivo_priv.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for eq_deportivo_priv.\n",
      "Starting range k = 1 of 1 for eq_deportivo_priv.\n",
      "Finished time analysis for eq_deportivo_priv.\n",
      "--- Source 22/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 23/51: eq_deportivo_pub. \n",
      "--- Source 23/51 (1.1) - Reading pois dir.\n",
      "--- Source 23/51 (1.2) - Keeping 282 pois inside aoi from original 1708 pois.\n",
      "--- Source 23/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each eq_deportivo_pub.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for eq_deportivo_pub.\n",
      "Starting range k = 1 of 2 for eq_deportivo_pub.\n",
      "Starting range k = 2 of 2 for eq_deportivo_pub.\n",
      "Finished time analysis for eq_deportivo_pub.\n",
      "--- Source 23/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 24/51: club_deportivo. \n",
      "--- Source 24/51 (1.1) - Reading pois dir.\n",
      "--- Source 24/51 (1.2) - Keeping 21 pois inside aoi from original 124 pois.\n",
      "--- Source 24/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each club_deportivo.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for club_deportivo.\n",
      "Starting range k = 1 of 1 for club_deportivo.\n",
      "Finished time analysis for club_deportivo.\n",
      "--- Source 24/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 25/51: civic_office. \n",
      "--- Source 25/51 (1.1) - Reading pois dir.\n",
      "--- Source 25/51 (1.2) - Keeping 63 pois inside aoi from original 206 pois.\n",
      "--- Source 25/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each civic_office.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for civic_office.\n",
      "Starting range k = 1 of 1 for civic_office.\n",
      "Finished time analysis for civic_office.\n",
      "--- Source 25/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 26/51: tax_collection. \n",
      "--- Source 26/51 (1.1) - Reading pois dir.\n",
      "--- Source 26/51 (1.2) - Keeping 8 pois inside aoi from original 14 pois.\n",
      "--- Source 26/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each tax_collection.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for tax_collection.\n",
      "Starting range k = 1 of 1 for tax_collection.\n",
      "Finished time analysis for tax_collection.\n",
      "--- Source 26/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 27/51: social_security. \n",
      "--- Source 27/51 (1.1) - Reading pois dir.\n",
      "--- Source 27/51 (1.2) - Keeping 21 pois inside aoi from original 55 pois.\n",
      "--- Source 27/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each social_security.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for social_security.\n",
      "Starting range k = 1 of 1 for social_security.\n",
      "Finished time analysis for social_security.\n",
      "--- Source 27/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 28/51: police. \n",
      "--- Source 28/51 (1.1) - Reading pois dir.\n",
      "--- Source 28/51 (1.2) - Keeping 37 pois inside aoi from original 118 pois.\n",
      "--- Source 28/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each police.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for police.\n",
      "Starting range k = 1 of 1 for police.\n",
      "Finished time analysis for police.\n",
      "--- Source 28/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 29/51: bomberos. \n",
      "--- Source 29/51 (1.1) - Reading pois dir.\n",
      "--- Source 29/51 (1.2) - Keeping 32 pois inside aoi from original 113 pois.\n",
      "--- Source 29/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each bomberos.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for bomberos.\n",
      "Starting range k = 1 of 1 for bomberos.\n",
      "Finished time analysis for bomberos.\n",
      "--- Source 29/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 30/51: museos_priv. \n",
      "--- Source 30/51 (1.1) - Reading pois dir.\n",
      "--- Source 30/51 (1.2) - Keeping 30 pois inside aoi from original 47 pois.\n",
      "--- Source 30/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each museos_priv.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for museos_priv.\n",
      "Starting range k = 1 of 1 for museos_priv.\n",
      "Finished time analysis for museos_priv.\n",
      "--- Source 30/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 31/51: museos_pub. \n",
      "--- Source 31/51 (1.1) - Reading pois dir.\n",
      "--- Source 31/51 (1.2) - Keeping 26 pois inside aoi from original 29 pois.\n",
      "--- Source 31/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each museos_pub.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for museos_pub.\n",
      "Starting range k = 1 of 1 for museos_pub.\n",
      "Finished time analysis for museos_pub.\n",
      "--- Source 31/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 32/51: cines. \n",
      "--- Source 32/51 (1.1) - Reading pois dir.\n",
      "--- Source 32/51 (1.2) - Keeping 18 pois inside aoi from original 53 pois.\n",
      "--- Source 32/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each cines.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for cines.\n",
      "Starting range k = 1 of 1 for cines.\n",
      "Finished time analysis for cines.\n",
      "--- Source 32/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 33/51: sitios_historicos. \n",
      "--- Source 33/51 (1.1) - Reading pois dir.\n",
      "--- Source 33/51 (1.2) - Keeping 220 pois inside aoi from original 283 pois.\n",
      "--- Source 33/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each sitios_historicos.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for sitios_historicos.\n",
      "Starting range k = 1 of 2 for sitios_historicos.\n",
      "Starting range k = 2 of 2 for sitios_historicos.\n",
      "Finished time analysis for sitios_historicos.\n",
      "--- Source 33/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 34/51: restaurantes_bar_cafe. \n",
      "--- Source 34/51 (1.1) - Reading pois dir.\n",
      "--- Source 34/51 (1.2) - Keeping 2370 pois inside aoi from original 5095 pois.\n",
      "--- Source 34/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each restaurantes_bar_cafe.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for restaurantes_bar_cafe.\n",
      "Starting range k = 1 of 12 for restaurantes_bar_cafe.\n",
      "Starting range k = 2 of 12 for restaurantes_bar_cafe.\n",
      "Starting range k = 3 of 12 for restaurantes_bar_cafe.\n",
      "Starting range k = 4 of 12 for restaurantes_bar_cafe.\n",
      "Starting range k = 5 of 12 for restaurantes_bar_cafe.\n",
      "Starting range k = 6 of 12 for restaurantes_bar_cafe.\n",
      "Starting range k = 7 of 12 for restaurantes_bar_cafe.\n",
      "Starting range k = 8 of 12 for restaurantes_bar_cafe.\n",
      "Starting range k = 9 of 12 for restaurantes_bar_cafe.\n",
      "Starting range k = 10 of 12 for restaurantes_bar_cafe.\n",
      "Starting range k = 11 of 12 for restaurantes_bar_cafe.\n",
      "Starting range k = 12 of 12 for restaurantes_bar_cafe.\n",
      "Finished time analysis for restaurantes_bar_cafe.\n",
      "--- Source 34/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 35/51: librerias. \n",
      "--- Source 35/51 (1.1) - Reading pois dir.\n",
      "--- Source 35/51 (1.2) - Keeping 133 pois inside aoi from original 229 pois.\n",
      "--- Source 35/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each librerias.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for librerias.\n",
      "Starting range k = 1 of 1 for librerias.\n",
      "Finished time analysis for librerias.\n",
      "--- Source 35/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 36/51: ep_plaza_small. \n",
      "--- Source 36/51 (1.1) - Reading pois dir.\n",
      "--- Source 36/51 (1.2) - Keeping 21140 pois inside aoi from original 166287 pois.\n",
      "--- Source 36/51 (1.3) - Calculating nodes proximity for special case.\n",
      "--- Calculating very small ep_plaza_small nodes proximity with function pois_time().\n",
      "Found and assigned nearest node osmid to each ep_plaza_small.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for ep_plaza_small.\n",
      "Starting range k = 1 of 5 for ep_plaza_small.\n",
      "Starting range k = 2 of 5 for ep_plaza_small.\n",
      "Starting range k = 3 of 5 for ep_plaza_small.\n",
      "Starting range k = 4 of 5 for ep_plaza_small.\n",
      "Starting range k = 5 of 5 for ep_plaza_small.\n",
      "Finished time analysis for ep_plaza_small.\n",
      "--- Calculating not that small ep_plaza_small nodes proximity with function id_pois_time().\n",
      "Found and assigned nearest node osmid to each ep_plaza_small.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for ep_plaza_small.\n",
      "Calculating proximity data for geometry of interest (goi) 1 of 265 for ep_plaza_small.\n",
      "0% done.\n",
      "First geometry of interest (goi)'s time.\n",
      "First batch count.\n",
      "Calculating proximity data for geometry of interest (goi) 27 of 265 for ep_plaza_small.\n",
      "10% done.\n",
      "Calculating proximity data for geometry of interest (goi) 53 of 265 for ep_plaza_small.\n",
      "20% done.\n",
      "Calculating proximity data for geometry of interest (goi) 80 of 265 for ep_plaza_small.\n",
      "30% done.\n",
      "Calculating proximity data for geometry of interest (goi) 106 of 265 for ep_plaza_small.\n",
      "40% done.\n",
      "Calculating proximity data for geometry of interest (goi) 133 of 265 for ep_plaza_small.\n",
      "50% done.\n",
      "Calculating proximity data for geometry of interest (goi) 159 of 265 for ep_plaza_small.\n",
      "60% done.\n",
      "Calculating proximity data for geometry of interest (goi) 186 of 265 for ep_plaza_small.\n",
      "70% done.\n",
      "Calculating proximity data for geometry of interest (goi) 212 of 265 for ep_plaza_small.\n",
      "80% done.\n",
      "Calculating proximity data for geometry of interest (goi) 239 of 265 for ep_plaza_small.\n",
      "90% done.\n",
      "Calculating proximity data for geometry of interest (goi) 265 of 265 for ep_plaza_small.\n",
      "100% done.\n",
      "Finished time analysis for ep_plaza_small.\n",
      "--- Source 36/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 37/51: ep_plaza_big. \n",
      "--- Source 37/51 (1.1) - Reading pois dir.\n",
      "--- Source 37/51 (1.2) - Keeping 3864 pois inside aoi from original 19945 pois.\n",
      "--- Source 37/51 (1.3) - Calculating nodes proximity for unique ID case.\n",
      "Found and assigned nearest node osmid to each ep_plaza_big.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for ep_plaza_big.\n",
      "Calculating proximity data for geometry of interest (goi) 1 of 249 for ep_plaza_big.\n",
      "0% done.\n",
      "First geometry of interest (goi)'s time.\n",
      "First batch count.\n",
      "Calculating proximity data for geometry of interest (goi) 25 of 249 for ep_plaza_big.\n",
      "10% done.\n",
      "Calculating proximity data for geometry of interest (goi) 50 of 249 for ep_plaza_big.\n",
      "20% done.\n",
      "Calculating proximity data for geometry of interest (goi) 75 of 249 for ep_plaza_big.\n",
      "30% done.\n",
      "Calculating proximity data for geometry of interest (goi) 100 of 249 for ep_plaza_big.\n",
      "40% done.\n",
      "Calculating proximity data for geometry of interest (goi) 125 of 249 for ep_plaza_big.\n",
      "50% done.\n",
      "Calculating proximity data for geometry of interest (goi) 150 of 249 for ep_plaza_big.\n",
      "60% done.\n",
      "Calculating proximity data for geometry of interest (goi) 175 of 249 for ep_plaza_big.\n",
      "70% done.\n",
      "Calculating proximity data for geometry of interest (goi) 200 of 249 for ep_plaza_big.\n",
      "80% done.\n",
      "Calculating proximity data for geometry of interest (goi) 225 of 249 for ep_plaza_big.\n",
      "90% done.\n",
      "Calculating proximity data for geometry of interest (goi) 249 of 249 for ep_plaza_big.\n",
      "100% done.\n",
      "Finished time analysis for ep_plaza_big.\n",
      "--- Source 37/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 38/51: edu_basica_pub. \n",
      "--- Source 38/51 (1.1) - Reading pois dir.\n",
      "--- Source 38/51 (1.2) - Keeping 311 pois inside aoi from original 1442 pois.\n",
      "--- Source 38/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each edu_basica_pub.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for edu_basica_pub.\n",
      "Starting range k = 1 of 2 for edu_basica_pub.\n",
      "Starting range k = 2 of 2 for edu_basica_pub.\n",
      "Finished time analysis for edu_basica_pub.\n",
      "--- Source 38/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 39/51: edu_media_pub. \n",
      "--- Source 39/51 (1.1) - Reading pois dir.\n",
      "--- Source 39/51 (1.2) - Keeping 171 pois inside aoi from original 749 pois.\n",
      "--- Source 39/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each edu_media_pub.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for edu_media_pub.\n",
      "Starting range k = 1 of 1 for edu_media_pub.\n",
      "Finished time analysis for edu_media_pub.\n",
      "--- Source 39/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 40/51: jardin_inf_pub. \n",
      "--- Source 40/51 (1.1) - Reading pois dir.\n",
      "--- Source 40/51 (1.2) - Keeping 151 pois inside aoi from original 807 pois.\n",
      "--- Source 40/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each jardin_inf_pub.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for jardin_inf_pub.\n",
      "Starting range k = 1 of 1 for jardin_inf_pub.\n",
      "Finished time analysis for jardin_inf_pub.\n",
      "--- Source 40/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 41/51: universidad. \n",
      "--- Source 41/51 (1.1) - Reading pois dir.\n",
      "--- Source 41/51 (1.2) - Keeping 371 pois inside aoi from original 475 pois.\n",
      "--- Source 41/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each universidad.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for universidad.\n",
      "Starting range k = 1 of 2 for universidad.\n",
      "Starting range k = 2 of 2 for universidad.\n",
      "Finished time analysis for universidad.\n",
      "--- Source 41/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 42/51: edu_tecnica. \n",
      "--- Source 42/51 (1.1) - Reading pois dir.\n",
      "--- Source 42/51 (1.2) - Keeping 30 pois inside aoi from original 48 pois.\n",
      "--- Source 42/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each edu_tecnica.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for edu_tecnica.\n",
      "Starting range k = 1 of 1 for edu_tecnica.\n",
      "Finished time analysis for edu_tecnica.\n",
      "--- Source 42/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 43/51: edu_adultos_pub. \n",
      "--- Source 43/51 (1.1) - Reading pois dir.\n",
      "--- Source 43/51 (1.2) - Keeping 28 pois inside aoi from original 192 pois.\n",
      "--- Source 43/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each edu_adultos_pub.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for edu_adultos_pub.\n",
      "Starting range k = 1 of 1 for edu_adultos_pub.\n",
      "Finished time analysis for edu_adultos_pub.\n",
      "--- Source 43/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 44/51: edu_especial_pub. \n",
      "--- Source 44/51 (1.1) - Reading pois dir.\n",
      "--- Source 44/51 (1.2) - Keeping 111 pois inside aoi from original 703 pois.\n",
      "--- Source 44/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each edu_especial_pub.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for edu_especial_pub.\n",
      "Starting range k = 1 of 1 for edu_especial_pub.\n",
      "Finished time analysis for edu_especial_pub.\n",
      "--- Source 44/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 45/51: bibliotecas. \n",
      "--- Source 45/51 (1.1) - Reading pois dir.\n",
      "--- Source 45/51 (1.2) - Keeping 10 pois inside aoi from original 50 pois.\n",
      "--- Source 45/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each bibliotecas.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for bibliotecas.\n",
      "Starting range k = 1 of 1 for bibliotecas.\n",
      "Finished time analysis for bibliotecas.\n",
      "--- Source 45/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 46/51: centro_edu_amb. \n",
      "--- Source 46/51 (1.1) - Reading pois dir.\n",
      "--- Source 46/51 (1.2) - Keeping 2 pois inside aoi from original 9 pois.\n",
      "--- Source 46/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each centro_edu_amb.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for centro_edu_amb.\n",
      "Starting range k = 1 of 1 for centro_edu_amb.\n",
      "Finished time analysis for centro_edu_amb.\n",
      "--- Source 46/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 47/51: paradas_tp_ruta. \n",
      "--- Source 47/51 (1.1) - Reading pois dir.\n",
      "--- Source 47/51 (1.2) - Keeping 2745 pois inside aoi from original 11212 pois.\n",
      "--- Source 47/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each paradas_tp_ruta.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for paradas_tp_ruta.\n",
      "Starting range k = 1 of 14 for paradas_tp_ruta.\n",
      "Starting range k = 2 of 14 for paradas_tp_ruta.\n",
      "Starting range k = 3 of 14 for paradas_tp_ruta.\n",
      "Starting range k = 4 of 14 for paradas_tp_ruta.\n",
      "Starting range k = 5 of 14 for paradas_tp_ruta.\n",
      "Starting range k = 6 of 14 for paradas_tp_ruta.\n",
      "Starting range k = 7 of 14 for paradas_tp_ruta.\n",
      "Starting range k = 8 of 14 for paradas_tp_ruta.\n",
      "Starting range k = 9 of 14 for paradas_tp_ruta.\n",
      "Starting range k = 10 of 14 for paradas_tp_ruta.\n",
      "Starting range k = 11 of 14 for paradas_tp_ruta.\n",
      "Starting range k = 12 of 14 for paradas_tp_ruta.\n",
      "Starting range k = 13 of 14 for paradas_tp_ruta.\n",
      "Starting range k = 14 of 14 for paradas_tp_ruta.\n",
      "Finished time analysis for paradas_tp_ruta.\n",
      "--- Source 47/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 48/51: paradas_tp_metro. \n",
      "--- Source 48/51 (1.1) - Reading pois dir.\n",
      "--- Source 48/51 (1.2) - Keeping 63 pois inside aoi from original 124 pois.\n",
      "--- Source 48/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each paradas_tp_metro.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for paradas_tp_metro.\n",
      "Starting range k = 1 of 1 for paradas_tp_metro.\n",
      "Finished time analysis for paradas_tp_metro.\n",
      "--- Source 48/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 49/51: paradas_tp_tren. \n",
      "--- Source 49/51 (1.1) - Reading pois dir.\n",
      "--- Source 49/51 (1.2) - Keeping 2 pois inside aoi from original 10 pois.\n",
      "--- Source 49/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each paradas_tp_tren.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for paradas_tp_tren.\n",
      "Starting range k = 1 of 1 for paradas_tp_tren.\n",
      "Finished time analysis for paradas_tp_tren.\n",
      "--- Source 49/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 50/51: ciclovias. \n",
      "--- Source 50/51 (1.1) - Reading pois dir.\n",
      "--- Source 50/51 (1.2) - Keeping 1868 pois inside aoi from original 6656 pois.\n",
      "--- Source 50/51 (1.3) - Calculating nodes proximity for unique ID case.\n",
      "Found and assigned nearest node osmid to each ciclovias.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for ciclovias.\n",
      "Calculating proximity data for geometry of interest (goi) 1 of 171 for ciclovias.\n",
      "0% done.\n",
      "First geometry of interest (goi)'s time.\n",
      "First batch count.\n",
      "Calculating proximity data for geometry of interest (goi) 18 of 171 for ciclovias.\n",
      "10% done.\n",
      "Calculating proximity data for geometry of interest (goi) 35 of 171 for ciclovias.\n",
      "20% done.\n",
      "Calculating proximity data for geometry of interest (goi) 52 of 171 for ciclovias.\n",
      "30% done.\n",
      "Calculating proximity data for geometry of interest (goi) 69 of 171 for ciclovias.\n",
      "40% done.\n",
      "Calculating proximity data for geometry of interest (goi) 86 of 171 for ciclovias.\n",
      "50% done.\n",
      "Calculating proximity data for geometry of interest (goi) 103 of 171 for ciclovias.\n",
      "60% done.\n",
      "Calculating proximity data for geometry of interest (goi) 120 of 171 for ciclovias.\n",
      "70% done.\n",
      "Calculating proximity data for geometry of interest (goi) 137 of 171 for ciclovias.\n",
      "80% done.\n",
      "Calculating proximity data for geometry of interest (goi) 154 of 171 for ciclovias.\n",
      "90% done.\n",
      "Calculating proximity data for geometry of interest (goi) 171 of 171 for ciclovias.\n",
      "100% done.\n",
      "Finished time analysis for ciclovias.\n",
      "--- Source 50/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "--------------------------------------------------------------------------------\n",
      "--- Starting nodes proximity to pois for source 51/51: estaciones_bicicletas. \n",
      "--- Source 51/51 (1.1) - Reading pois dir.\n",
      "--- Source 51/51 (1.2) - Keeping 241 pois inside aoi from original 326 pois.\n",
      "--- Source 51/51 (1.3) - Calculating nodes proximity for regular case.\n",
      "Found and assigned nearest node osmid to each estaciones_bicicletas.\n",
      "Calculated length for 0 edges that had no length data.\n",
      "Starting time analysis for estaciones_bicicletas.\n",
      "Starting range k = 1 of 2 for estaciones_bicicletas.\n",
      "Starting range k = 2 of 2 for estaciones_bicicletas.\n",
      "Finished time analysis for estaciones_bicicletas.\n",
      "--- Source 51/51 (1.4) - Appended 14537 nodes to nodes analysis.\n",
      "14537\n",
      "CALCULATING PROXIMITY AND HQSL FOR AREA OF ANALYSIS 1/3: unidadesvecinales.\n",
      "--- STARTING PART 2: NODES DATA TO unidadesvecinales.\n",
      "--- Area of analysis 1/3 (2.1) - Loaded area of analysis gdf.\n",
      "--- Area of analysis 1/3 (2.2) - Calculated mean proximity for 343 polygons.\n",
      "--- Area of analysis 1/3 (2.3) - Giving final format and saving unidadesvecinales proximity data.\n",
      "--- Area of analysis 1/3 (2.3) - Saved unidadesvecinales proximity data locally.\n",
      "--- STARTING PART 3 (HQSL) FOR unidadesvecinales.\n",
      "--- Area of analysis 1/3 (3.1) - Loading areal data.\n",
      "--- Area of analysis 1/3 (3.2) - Joining _priv and _pub pois in unidadesvecinales.\n",
      "--- Area of analysis 1/3 (3.3) - Processing variables analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [00:01<00:00, 48.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Area of analysis 1/3 (3.4) - Calculating HQSL.\n",
      "--- Area of analysis 1/3 (3.5) - Saving HQSL index locally.\n",
      "CALCULATING PROXIMITY AND HQSL FOR AREA OF ANALYSIS 2/3: zonascensales.\n",
      "--- STARTING PART 2: NODES DATA TO zonascensales.\n",
      "--- Area of analysis 2/3 (2.1) - Loaded area of analysis gdf.\n",
      "--- Area of analysis 2/3 (2.2) - Calculated mean proximity for 522 polygons.\n",
      "--- Area of analysis 2/3 (2.3) - Giving final format and saving zonascensales proximity data.\n",
      "--- Area of analysis 2/3 (2.3) - Saved zonascensales proximity data locally.\n",
      "--- STARTING PART 3 (HQSL) FOR zonascensales.\n",
      "--- Area of analysis 2/3 (3.1) - Loading areal data.\n",
      "--- Area of analysis 2/3 (3.2) - Joining _priv and _pub pois in zonascensales.\n",
      "--- Area of analysis 2/3 (3.3) - Processing variables analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [00:01<00:00, 35.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Area of analysis 2/3 (3.4) - Calculating HQSL.\n",
      "--- Area of analysis 2/3 (3.5) - Saving HQSL index locally.\n",
      "CALCULATING PROXIMITY AND HQSL FOR AREA OF ANALYSIS 3/3: hex.\n",
      "--- STARTING PART 2: NODES DATA TO hex.\n",
      "--- Area of analysis 3/3 (2.1) - Created 10121 hexagons at resolution 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gds/lib/python3.9/site-packages/geopandas/geodataframe.py:1525: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  super().__setitem__(key, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Area of analysis 3/3 (2.2) - Calculated mean proximity for 10121 hexagons at resolution 10.\n",
      "--- Area of analysis 3/3 (2.2) - Merged 10121 hexagons to poly_proximity gdf.\n",
      "--- Area of analysis 3/3 (2.2) - Calculated mean proximity for 10121 polygons.\n",
      "--- Area of analysis 3/3 (2.3) - Giving final format and saving hex proximity data.\n",
      "--- Area of analysis 3/3 (2.3) - Saved hex proximity data locally.\n",
      "--- STARTING PART 3 (HQSL) FOR hex.\n",
      "--- Area of analysis 3/3 (3.1) - Loading areal data.\n",
      "--- Area of analysis 3/3 (3.2) - Joining _priv and _pub pois in hex.\n",
      "--- Area of analysis 3/3 (3.3) - Processing variables analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [05:21<00:00,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Area of analysis 3/3 (3.4) - Calculating HQSL.\n",
      "--- Area of analysis 3/3 (3.5) - Saving HQSL index locally.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################################################\n",
    "########################################################## SCRIPT START ##############################################################\n",
    "######################################################################################################################################\n",
    "\n",
    "# Area of interest (aoi)\n",
    "print(\"--- Downloading area of interest.\")\n",
    "query = f\"SELECT * FROM {aoi_schema}.{aoi_table} WHERE \\\"city\\\" LIKE \\'{city}\\'\"\n",
    "aoi = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "aoi = aoi.set_crs(\"EPSG:4326\")\n",
    "\n",
    "# Network\n",
    "if osmnx_network:\n",
    "    print(\"--- Downloading OSMnx network.\")\n",
    "    G, nodes, edges = aup.graph_from_hippo(aoi, network_schema, edges_table, nodes_table, projected_crs)\n",
    "else:\n",
    "    print(\"--- Converting local data to OSMnx format network.\")\n",
    "    #G, nodes, edges = create_filtered_navigable_network(public_space_quality_dir, projected_crs, filtering_column, filtering_value)\n",
    "\n",
    "    # Load edges\n",
    "    network_edges = gpd.read_file(gral_dir+'calidad_ep/red_buena_calidad_single_parts.gpkg')\n",
    "    network_edges = network_edges.set_crs(projected_crs)\n",
    "    # Load nodes\n",
    "    network_nodes = gpd.read_file(gral_dir +'calidad_ep/red_buena_calidad_nodes.shp')\n",
    "    network_nodes = network_nodes.set_crs(\"EPSG:32719\")\n",
    "    # Create navigable network\n",
    "    nodes, edges = aup.create_network(network_nodes, network_edges,\"EPSG:32719\")\n",
    "    nodes = nodes.drop_duplicates(subset=['osmid'])\n",
    "    # Filter navigable network\n",
    "    edges_filt = edges.loc[edges[filtering_column] >= filtering_value]\n",
    "    # Create G\n",
    "    nodes_gdf = nodes.copy()\n",
    "    nodes_gdf.set_index('osmid',inplace=True)\n",
    "    edges_gdf = edges_filt.copy()\n",
    "    edges_gdf.set_index(['u','v','key'],inplace=True)\n",
    "    nodes_gdf['x'] = nodes_gdf['geometry'].x\n",
    "    nodes_gdf['y'] = nodes_gdf['geometry'].y\n",
    "    G = ox.graph_from_gdfs(nodes_gdf, edges_gdf)\n",
    "    nodes = nodes_gdf.copy()\n",
    "    edges = edges_gdf.copy()\n",
    "\n",
    "# Main function\n",
    "for walking_speed in walking_speed_list:\n",
    "    print('--'*45)\n",
    "    str_walk_speed = str(walking_speed).replace('.','_')\n",
    "    \n",
    "    # Proceed to main function\n",
    "    print(f\"--- Running Script for speed: {walking_speed}km/hr.\")\n",
    "    #main(source_list, aoi, G, nodes, edges, walking_speed, local_save)\n",
    "\n",
    "    ############################################################### PART 1 ###############################################################\n",
    "    #################################################### FIND NODES PROXIMITY TO POIS ####################################################\n",
    "    # ------------------- This step loads each source of interest, calculates nodes proximity and saves it to nodes_analysis\n",
    "\n",
    "    print(f\"STARTING PART 1: NODES PROXIMITY TO POINTS OF INTEREST.\")\n",
    "\n",
    "    k = len(source_list)\n",
    "    i = 1\n",
    "    all_source_cols =[]\n",
    "\n",
    "    for source in source_list:\n",
    "\n",
    "        source_cols =[]\n",
    "\n",
    "        # ----------\n",
    "        # UNIQUE ID CONSIDERATION\n",
    "        # Check if current source has a unique ID that needs to be considered in the process\n",
    "        if source in unique_id_sources:\n",
    "            unique_id = True\n",
    "        else:\n",
    "            unique_id = False\n",
    "        # ----------\n",
    "\n",
    "        print(\"--\"*40)\n",
    "        print(f\"--- Starting nodes proximity to pois for source {i}/{k}: {source}. \")\n",
    "\n",
    "        # 1.1) Read pois from pois dir\n",
    "        print(f\"--- Source {i}/{k} (1.1) - Reading pois dir.\")\n",
    "        # Directory where pois to be examined are located\n",
    "        pois_dir = all_pois_dir + f'{source}.gpkg'\n",
    "        # Load all pois from directory\n",
    "        pois = gpd.read_file(pois_dir)\n",
    "\n",
    "        # ----------\n",
    "        # UNIQUE ID AND SMALL PARKS CONSIDERATION\n",
    "        if unique_id:\n",
    "            if source == 'ep_plaza_small':\n",
    "                # For small parks, area is relevant to sub-divide process (below 2000m2 --> pois_time(), above 2000m2 --> id_pois_time())\n",
    "                pois = pois[['area_ha','ID','geometry']]\n",
    "            else:\n",
    "                # For the rest, keep already existing unique ID and geometry\n",
    "                pois = pois[['ID','geometry']]\n",
    "        else:\n",
    "            # If not unique_ID, ID col is source name (irrelevant), keeps geometry\n",
    "            pois['ID'] = source\n",
    "            pois = pois[['ID','geometry']]\n",
    "        # ----------\n",
    "\n",
    "        # Format\n",
    "        try:\n",
    "            pois = pois.to_crs(\"EPSG:4326\")\n",
    "        except:\n",
    "            pois = pois.set_crs(\"EPSG:4326\")\n",
    "\n",
    "        # 1.2) Clip pois to aoi\n",
    "        source_pois = gpd.sjoin(pois, aoi)\n",
    "\n",
    "        # ----------\n",
    "        # SMALL PARKS CONSIDERATION\n",
    "        if source == 'ep_plaza_small':\n",
    "            # For small parks, area is relevant to sub-divide process\n",
    "            source_pois = source_pois[['area_ha','ID','geometry']]\n",
    "        else:\n",
    "            source_pois = source_pois[['ID','geometry']]\n",
    "        # ----------\n",
    "\n",
    "        print(f\"--- Source {i}/{k} (1.2) - Keeping {len(source_pois)} pois inside aoi from original {len(pois)} pois.\")\n",
    "\n",
    "        if save_space:\n",
    "            del pois\n",
    "\n",
    "        # 1.3) Calculate nodes proximity\n",
    "        # ----------\n",
    "        # UNIQUE ID AND SMALL PARKS CONSIDERATION\n",
    "        if unique_id:\n",
    "            #################################################### SMALL PARKS ONLY [SECTION STARTS]\n",
    "            if source == 'ep_plaza_small':\n",
    "                print(f\"--- Source {i}/{k} (1.3) - Calculating nodes proximity for special case.\")\n",
    "\n",
    "                # pois_time() [for public spaces below 2000m2]\n",
    "                # For VERY small public spaces (below 2000m2), the proximity analysis will consider any poi derived from the geometry of interest (goi, polygon) because anyway it is small.\n",
    "                # Because we just care about one poi only (any), this step filters and drops duplicate IDs, keeping the first occurrence.\n",
    "                very_small_source_pois = source_pois.loc[source_pois['area_ha']<0.2].copy().drop_duplicates(subset='ID')\n",
    "                # Calculate time data from nodes to source for very_small_source_pois (Has 1 pois for each goi)\n",
    "                print(f\"--- Calculating very small {source} nodes proximity with function pois_time().\")\n",
    "                source_nodes_time_1 = aup.pois_time(G, nodes, edges, very_small_source_pois, source,'length',\n",
    "                                                    walking_speed, count_pois, projected_crs)\n",
    "                if save_space:\n",
    "                    del very_small_source_pois\n",
    "                \n",
    "                # id_pois_time() [for public spaces above 2000m2]\n",
    "                # For larger public spaces (above 2000m2), having several accesses becomes relevant, and goi IDs becomes necessary (needs id_pois_time() function)\n",
    "                small_source_pois = source_pois.loc[source_pois['area_ha']>=0.2].copy()\n",
    "                # Calculate time data from nodes to source for small_source_pois (Has n pois for each goi, needs goi_id)\n",
    "                print(f\"--- Calculating not that small {source} nodes proximity with function id_pois_time().\")\n",
    "                source_nodes_time_2 = aup.id_pois_time(G, nodes, edges, small_source_pois, source,'length',\n",
    "                                                       walking_speed, goi_id='ID', count_pois=count_pois, projected_crs=projected_crs)\n",
    "                if save_space:\n",
    "                    del small_source_pois\n",
    "\n",
    "                # Now merge source_nodes_time_1 results with source_nodes_time_2 results\n",
    "                if count_pois[0]:\n",
    "                    source_nodes_time_all = source_nodes_time_1.merge(source_nodes_time_2[['osmid', 'time_'+source, f'{source}_{count_pois[1]}min']],on='osmid')\n",
    "                else:\n",
    "                    source_nodes_time_all = source_nodes_time_1.merge(source_nodes_time_2[['osmid', 'time_'+source]],on='osmid')\n",
    "\n",
    "                if save_space:\n",
    "                    del source_nodes_time_1\n",
    "                    del source_nodes_time_2\n",
    "                \n",
    "                # For time data, find *min* time between both source_nodes_time.\n",
    "                time_cols = [f'time_{source}_x', f'time_{source}_y']\n",
    "                source_nodes_time_all[f'time_{source}'] = source_nodes_time_all[time_cols].min(axis=1)\n",
    "                source_nodes_time_all.drop(columns=time_cols,inplace=True)\n",
    "\n",
    "                # For count data, find *sum* of counted pois for both source_nodes_time\n",
    "                if count_pois[0]:\n",
    "                    count_cols = [f'{source}_{count_pois[1]}min_x',f'{source}_{count_pois[1]}min_y']\n",
    "                    source_nodes_time_all[f'{source}_{count_pois[1]}min'] = source_nodes_time_all[count_cols].sum(axis=1)\n",
    "                    source_nodes_time_all.drop(columns=count_cols,inplace=True)\n",
    "\n",
    "                # Finally, rename result\n",
    "                source_nodes_time = source_nodes_time_all.copy()\n",
    "                \n",
    "                if save_space:\n",
    "                    del source_nodes_time_all\n",
    "            #################################################### SMALL PARKS ONLY [SECTION ENDS]\n",
    "\n",
    "            else:\n",
    "                print(f\"--- Source {i}/{k} (1.3) - Calculating nodes proximity for unique ID case.\")\n",
    "                # Function id_pois_time() consideres the unique ID belonging to each geometry of interest (goi).\n",
    "                source_nodes_time = aup.id_pois_time(G, nodes, edges, source_pois, source, 'length', walking_speed, \n",
    "                                                    goi_id='ID', count_pois=count_pois, projected_crs=projected_crs)\n",
    "        else:\n",
    "            print(f\"--- Source {i}/{k} (1.3) - Calculating nodes proximity for regular case.\")\n",
    "            # Function pois_time() calculates proximity data from nodes to source (all) without considering any unique ID.\n",
    "            source_nodes_time = aup.pois_time(G, nodes, edges, source_pois, source,'length',walking_speed, \n",
    "                                              count_pois, projected_crs)\n",
    "        # ----------\n",
    "\n",
    "        if save_space:\n",
    "            del source_pois\n",
    "\n",
    "        #### Changes when comparing to Script 23, 23b and notebook 04b:\n",
    "        # Previously we formated nodes analysis as tidy format in order to be able to loop-upload nodes proximity data.\n",
    "        # That was relevant as new data was flowing each day. However, that's no longer needed.\n",
    "        # Instead, data is formated directly and added to nodes_analysis\n",
    "        ####\n",
    "\n",
    "        # 1.4) New nodes_analysis format (Not tidy data)\n",
    "        # Rename time column\n",
    "        source_nodes_time.rename(columns={'time_'+source:f'{source}_time'}, inplace=True)\n",
    "        # Register time column\n",
    "        source_cols.append(f'{source}_time') # Current source only\n",
    "        all_source_cols.append(f'{source}_time') # All sources, this list will be used in PART 2.\n",
    "\n",
    "        # Rename and format count column\n",
    "        if count_pois[0]:\n",
    "            source_nodes_time.rename(columns={f'{source}_{count_pois[1]}min':f'{source}_count_{count_pois[1]}min'}, inplace=True)\n",
    "            source_nodes_time[f'{source}_count_{count_pois[1]}min'] = source_nodes_time[f'{source}_count_{count_pois[1]}min'].astype(int)\n",
    "            # Register count column\n",
    "            source_cols.append(f'{source}_count_{count_pois[1]}min') # Current source only\n",
    "            all_source_cols.append(f'{source}_count_{count_pois[1]}min') # All sources, this list will be used in PART 2.\n",
    "\n",
    "        # Create or append to nodes_analysis\n",
    "        if i == 1:\n",
    "            nodes_analysis = source_nodes_time[['osmid','geometry']+source_cols]\n",
    "            print(f\"--- Source {i}/{k} (1.4) - Created nodes analysis with {len(source_nodes_time)} for the first time.\")\n",
    "        else:\n",
    "            nodes_analysis = nodes_analysis.merge(source_nodes_time[['osmid']+source_cols], on='osmid', how='left')\n",
    "            print(f\"--- Source {i}/{k} (1.4) - Appended {len(source_nodes_time)} nodes to nodes analysis.\")\n",
    "\n",
    "        print(len(nodes_analysis))\n",
    "        \n",
    "        if save_space:\n",
    "            del source_nodes_time\n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    ############################################################### PART 2 ###############################################################\n",
    "    #################################################### NODES DATA TO AREA OF ANALYSIS ##################################################\n",
    "    # Avoid overestimating universities\n",
    "    nodes_analysis.loc[nodes_analysis.universidad_count_15min > 3, 'universidad_count_15min'] = 3\n",
    "\n",
    "    area_dict = {'unidadesvecinales':'COD_UNICO_',\n",
    "                 'zonascensales':'GEOCODI',\n",
    "                 'hex':'hex_id'\n",
    "                 }\n",
    "    \n",
    "    k = len(area_dict.keys())\n",
    "    i = 1\n",
    "\n",
    "    for area_analysis in area_dict.keys():\n",
    "\n",
    "        print(f\"CALCULATING PROXIMITY AND HQSL FOR AREA OF ANALYSIS {i}/{k}: {area_analysis}.\")\n",
    "\n",
    "        print(f\"--- STARTING PART 2: NODES DATA TO {area_analysis}.\")\n",
    "\n",
    "        # 2.1 --------------- LOAD AND FORMAT AREA OF ANALYSIS GDF\n",
    "        # ------------------- This step loads the current area of analysis and prepares it as an empty container\n",
    "\n",
    "        code_column = area_dict[area_analysis]\n",
    "        \n",
    "        # Load area of analysis gdf\n",
    "        if area_analysis == 'unidadesvecinales':\n",
    "            gdf = gpd.read_file(areas_dir+\"santiago_unidadesvecinales_zonaurbana.geojson\")\n",
    "            gdf = gdf[[code_column,'geometry']].copy()\n",
    "            print(f\"--- Area of analysis {i}/{k} (2.1) - Loaded area of analysis gdf.\")\n",
    "\n",
    "        elif area_analysis == 'zonascensales':\n",
    "            gdf = gpd.read_file(areas_dir+\"zonas_censales_hogares_RM.shp\")\n",
    "            gdf = gdf[[code_column,'geometry']].copy()\n",
    "            print(f\"--- Area of analysis {i}/{k} (2.1) - Loaded area of analysis gdf.\")\n",
    "\n",
    "        elif area_analysis == 'hex':\n",
    "            # For this script, will only use res=10\n",
    "            res = 10\n",
    "\n",
    "            gdf = aup.create_hexgrid(aoi, res)\n",
    "            gdf.rename(columns={f'hex_id_{res}':'hex_id'}, inplace=True)\n",
    "            gdf['res'] = res\n",
    "            gdf = gdf[[code_column,'res','geometry']].copy()  \n",
    "            print(f\"--- Area of analysis {i}/{k} (2.1) - Created {len(gdf)} hexagons at resolution {res}.\")\n",
    "        \n",
    "        # Set gdf CRS\n",
    "        try:\n",
    "            gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "        except:\n",
    "            gdf = gdf.set_crs(\"EPSG:4326\")\n",
    "        \n",
    "        # Explode area of analysis gdf\n",
    "        gdf = gdf.explode(ignore_index=True)\n",
    "\n",
    "        # Clip area of analysis gdf to area of interest \n",
    "        # (Data available within area of interest only, not clipping causes problems when computing neighbors data.)\n",
    "        gdf_cut = gpd.sjoin(gdf, aoi[['geometry']])\n",
    "        gdf_cut.drop(columns=['index_right'],inplace=True)\n",
    "        gdf = gdf_cut.copy()\n",
    "\n",
    "        # 2.2 --------------- GROUP DATA BY AREA OF ANALYSIS\n",
    "        # ------------------- This groups proximity data by area of analysis\n",
    "\n",
    "        if area_analysis == 'hex':\n",
    "\n",
    "            hex_gdf = gdf.copy()\n",
    "            poly_proximity = gpd.GeoDataFrame()\n",
    "\n",
    "            for r in hex_gdf.res.unique():\n",
    "\n",
    "                # Calculate mean proximity within area of analysis\n",
    "                hex_tmp = hex_gdf[hex_gdf.res == r].copy()\n",
    "                hex_tmp = aup.group_by_hex_mean(nodes_analysis, hex_tmp, r, all_source_cols, 'hex_id')\n",
    "                hex_tmp = hex_tmp.drop(columns=['res_x','res_y'])\n",
    "                hex_tmp['res'] = r\n",
    "                print(f\"--- Area of analysis {i}/{k} (2.2) - Calculated mean proximity for {len(hex_tmp)} hexagons at resolution {r}.\")\n",
    "\n",
    "                # Merge to poly_proximity gdf\n",
    "                poly_proximity = pd.concat([poly_proximity, hex_tmp], \n",
    "                                           ignore_index = True, \n",
    "                                           axis = 0)\n",
    "                print(f\"--- Area of analysis {i}/{k} (2.2) - Merged {len(hex_tmp)} hexagons to poly_proximity gdf.\")\n",
    "\n",
    "                del hex_tmp\n",
    "        \n",
    "        # If not hex\n",
    "        else:\n",
    "            r = 0 # no resolution needed for polygons different from h3 hexagons\n",
    "            poly_proximity = aup.group_by_hex_mean(nodes_analysis, gdf, r, all_source_cols, code_column)\n",
    "        print(f\"--- Area of analysis {i}/{k} (2.2) - Calculated mean proximity for {len(poly_proximity)} polygons.\")\n",
    "\n",
    "\n",
    "        # 2.3 --------------- FINAL FORMAT AND SAVE\n",
    "        # ------------------- This step gives final formating to proximity data and saves it localy\n",
    "        print(f\"--- Area of analysis {i}/{k} (2.3) - Giving final format and saving {area_analysis} proximity data.\")\n",
    "\n",
    "        poly_proximity = poly_proximity.set_geometry('geometry')\n",
    "        try:\n",
    "            poly_proximity = poly_proximity.to_crs(\"EPSG:4326\")\n",
    "        except:\n",
    "            poly_proximity = poly_proximity.set_crs(\"EPSG:4326\")\n",
    "        \n",
    "        poly_proximity['city'] = 'Santiago'\n",
    "\n",
    "        if local_save:\n",
    "            area_proximity_table = f\"santiago_{area_analysis}proximity_{str_walk_speed}_kmh.gpkg\"\n",
    "            poly_proximity.to_file(local_save_dir + area_proximity_table, driver='GPKG')\n",
    "            print(f\"--- Area of analysis {i}/{k} (2.3) - Saved {area_analysis} proximity data locally.\")\n",
    "\n",
    "    ########################################################## PART 3 ####################################################################\n",
    "    ########################################################### HQSL #####################################################################\n",
    "\n",
    "        print(f\"--- STARTING PART 3 (HQSL) FOR {area_analysis}.\")\n",
    "\n",
    "        prox_gdf = poly_proximity.copy()\n",
    "\n",
    "        # 3.1 --------------- AREAL DATA\n",
    "        # ------------------- This step loads areal data (Not processed through proximity analysis).\n",
    "        print(f\"--- Area of analysis {i}/{k} (3.1) - Loading areal data.\")\n",
    "\n",
    "        if area_analysis == 'hex':\n",
    "            poly_areal = gpd.read_file(areal_dir+f'{area_analysis}_areal_res{res}.gpkg')\n",
    "        else:\n",
    "            poly_areal = gpd.read_file(areal_dir+f'{area_analysis}_areal.gpkg')\n",
    "            \n",
    "        poly_areal = poly_areal.rename(columns={'oficinas_sum':'oficinas_count',\n",
    "                                                'pct_social_viv':'social_viv_count',\n",
    "                                                'viv_sum':'houses_count',\n",
    "                                                'pct_hotel':'hotel_count',\n",
    "                                                'ndvi_mean':'ndvi_count'})\n",
    "        \n",
    "        # Clip poly_aereal gdf to area of interest \n",
    "        # (Data available within area of interest only, not clipping causes problems when computing neighbors data.)\n",
    "        poly_areal_cut = gpd.sjoin(poly_areal, aoi[['geometry']])\n",
    "        poly_areal_cut.drop(columns=['index_right'],inplace=True)\n",
    "        poly_areal = poly_areal_cut.copy()\n",
    "        \n",
    "        # 3.2 --------------- DATA TREATMENT\n",
    "        # ------------------- This step prepares proximity data and merges it with areal data\n",
    "        print(f\"--- Area of analysis {i}/{k} (3.2) - Joining _priv and _pub pois in {area_analysis}.\")\n",
    "        \n",
    "        join_pois_list = ['hospital','clinica','consult_ado', 'museos','vacunatorio','eq_deportivo',]\n",
    "        \n",
    "        for source in join_pois_list:\n",
    "            # join count columns for private and public in one encompassing column\n",
    "            prox_gdf[f\"{source}_count_15min\"] = prox_gdf[f\"{source}_priv_count_15min\"] + prox_gdf[f\"{source}_pub_count_15min\"]\n",
    "            # remove 0 values from time\n",
    "            prox_gdf.loc[prox_gdf[f\"{source}_pub_time\"]==0] = np.nan\n",
    "            prox_gdf.loc[prox_gdf[f\"{source}_priv_time\"]==0] = np.nan\n",
    "            # assign general minimum time\n",
    "            prox_gdf[f\"{source}_time\"] = prox_gdf[[f\"{source}_pub_time\", f\"{source}_priv_time\"]].min(axis=1)\n",
    "            # remove duplicate info columns\n",
    "            prox_gdf = prox_gdf.drop(columns=[f\"{source}_pub_count_15min\", f\"{source}_priv_count_15min\",\n",
    "                                              f\"{source}_pub_time\", f\"{source}_priv_time\"])\n",
    "            # fill na with 0 for future processing\n",
    "            prox_gdf['hospital_time'].fillna(0, inplace=True)\n",
    "\n",
    "        # Merge areal and proximity data\n",
    "        poly_analysis = poly_areal.merge(prox_gdf.drop(columns='geometry'), on=code_column, how='left')\n",
    "        poly_analysis = poly_analysis.explode(ignore_index=True)\n",
    "        poly_analysis = poly_analysis.dissolve(by=code_column)\n",
    "        poly_analysis = poly_analysis.reset_index()\n",
    "\n",
    "        # 3.3 --------------- HQSL Function - Variables analysis\n",
    "        # ------------------- This step scales data\n",
    "        print(f\"--- Area of analysis {i}/{k} (3.3) - Processing variables analysis.\")\n",
    "        # ------------------------------\n",
    "        # use scale functions for each column\n",
    "        for j in tqdm(range(len(weight_dict.keys())),position=0,leave=True):\n",
    "            # gather specific source\n",
    "            source = list(weight_dict.keys())[j]\n",
    "            # iterate over columns\n",
    "            for col_name in poly_analysis.columns:\n",
    "                # select column with count information -- refers to the amount of opportunities available at 15 min\n",
    "                if source in col_name and 'count' in col_name:\n",
    "                    if f'{source}_time' in poly_analysis.columns:\n",
    "                        poly_analysis[f'{source}_time'].fillna(0, inplace=True)\n",
    "                    poly_analysis[col_name].fillna(0, inplace=True)\n",
    "\n",
    "                    # source scaling\n",
    "                    poly_analysis[f'{source}_scaled'] = poly_analysis[col_name].apply(lambda x:scale_source_fn(x,\n",
    "                                                                                                               source,\n",
    "                                                                                                               weight_dict,\n",
    "                                                                                                               area_analysis,\n",
    "                                                                                                               poly_analysis[col_name].mean(),\n",
    "                                                                                                               poly_analysis[col_name].std()))\n",
    "                    # treat 0 time values -- hexagons without nodes \n",
    "                    if area_analysis == 'hex':\n",
    "                        if weight_dict[source] != 'specific':\n",
    "                            # assign nan values to hexagons without nodes to avoid affecting the mean calculation process\n",
    "                            #if source in join_pois_list:\n",
    "                            #    hex_analysis.loc[hex_analysis.supermercado_time==0,f'{source}_scaled'] = np.nan\n",
    "                            if source == 'hotel' or source == 'oficinas':\n",
    "                                continue\n",
    "                            else:\n",
    "                                poly_analysis.loc[poly_analysis[f'{source}_time']==0,f'{source}_scaled'] = np.nan\n",
    "                                \n",
    "                            # calculate mean count value\n",
    "                            poly_analysis.loc[poly_analysis[f'{source}_time']==0, f'{source}_scaled'] = poly_analysis.loc[poly_analysis[f'{source}_time']==0].apply(lambda x: neighbour_mean(x['hex_id'],\n",
    "                                                                                                                                                                                             'hex_id',\n",
    "                                                                                                                                                                                             poly_analysis,\n",
    "                                                                                                                                                                                             f'{source}_scaled'), axis=1)\n",
    "        # 3.4 --------------- HQSL Function - HQSL Index calculation\n",
    "        # ------------------- This step calculates HQSL\n",
    "        print(f\"--- Area of analysis {i}/{k} (3.4) - Calculating HQSL.\")\n",
    "\n",
    "        # ------------------------------\n",
    "        hex_ind = indicator_fn(poly_analysis, parameters_dict, code_column)\n",
    "        hex_social_fn = social_fn(poly_analysis, parameters_dict, code_column)\n",
    "        hex_hqsl = hqsl_fn(hex_social_fn, parameters_dict, code_column)\n",
    "        \n",
    "        hex_idx = hex_ind.merge(hex_social_fn.drop(columns='geometry'), on=code_column)\n",
    "        hex_idx = hex_idx.merge(hex_hqsl.drop(columns='geometry'), on=code_column)\n",
    "\n",
    "        # 3.5 --------------- SAVING\n",
    "        # ------------------- This step saves HQSL result.\n",
    "        if area_analysis == 'hex':\n",
    "            hex_idx['res'] = res\n",
    "        \n",
    "        hex_idx = hex_idx.dropna()\n",
    "                                \n",
    "        if local_save:\n",
    "            print(f\"--- Area of analysis {i}/{k} (3.5) - Saving HQSL index locally.\")\n",
    "            hex_idx.to_file(gral_dir +'output/'+ f'santiago_{area_analysis}analysis_{str_walk_speed}_kmh.gpkg', driver='GPKG')\n",
    "        \n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c489c-49cc-4913-adfc-e30b2dfa29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_nodes_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d3915-f5e9-4f24-a285-82f7506c89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a2725-911f-4d28-b792-11ac06c2d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_nodes_time.loc[source_nodes_time.osmid==3427646296549]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7717a25-b1ad-4d26-9531-b14108f79644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDS-10.0",
   "language": "python",
   "name": "gds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
