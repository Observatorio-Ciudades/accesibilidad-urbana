{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34da882-1610-4e69-b6c2-dccff1960031",
   "metadata": {},
   "source": [
    "# NDVI regular cities running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d7dbc-eec1-455b-b03c-18ffbab57304",
   "metadata": {},
   "source": [
    "This notebook tries to __run all processable missing cities__ (identified on notebook 02_missing_ndvi_temp_cities). Even though all missing cities will run download_raster_from_pc(), __only cities with no full_month processing (just specific_dates processing) will be processed to hexs and uploaded to the database.__\n",
    "\n",
    "* NOTE: Cities where no full_month processing is used couldn't be previously processed not because of the availability of tiles in specific_dates, but because of a __(fixed) bug in available_datasets()__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94261724-ea33-49a2-9555-07f5d11b310c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Processing summary\n",
    "#### __Cities with no full_month processing, just specific_date processing:__\n",
    "__Logs 2025-09-24__\n",
    "* __Tuxtla__ (2 tiles) started with 40% missing, could not process 2/2021 and ended up with _Missing more than 50 percent of data points_.\n",
    "* __Piedad__ (2 tiles) started with 39% missing, and ended with 39% missing, [UPLOADED TO DB]\n",
    "* __Cordoba__ (2 tiles) started with 19% missing, ended with 26%,  [UPLOADED TO DB]\n",
    "* __Orizaba__ (2 tiles) started with 25% missing, ended with 26%,  [UPLOADED TO DB]\n",
    "* __Morelia__ (3 tiles) started with 18% missing, could not process 5/2018 and ended up with _Multiple missing months together_.\n",
    "* __Cancun__ (3 tiles) started with 11% missing, ended with 32% , [UPLOADED TO DB]\n",
    "* __Playa__ (3 tiles) started with 18% missing, could not process 10/2019 and ended up with _Multiple missing months together_.\n",
    "* __Culiacan__ (3 tiles) started with 1% missing, ended with 19%  [UPLOADED TO DB, BUT ERROR IN RES 11, FROM HEX SOURCE]\n",
    "\n",
    "__Logs 2025-09-25__\n",
    "* __Guaymas__ (3 tiles) started with 1% missing, ended with 8%  [UPLOADED TO DB]\n",
    "\n",
    "__Logs 2025-09-25__ \"202509 - Dev -  Joining all rasters in a month\" folder\n",
    "* __Tampico__ (3 tiles) started with 36% missing, could not process by the end of 2019 and ended up with _Missing more than 50 percent of data points_.\n",
    "* __Campeche__ (4 tiles) started with 10% missing, could not process by the all at start of 2018 and ended up with _Multiple missing months together_.\n",
    "* __Celaya__ (4 tiles) started with 10% missing, ended with 24% [UPLOADED TO DB]\n",
    "* __Guanajuato__ (4 tiles) started with 22% missing, ended with 26% [UPLOADED TO DB]\n",
    "* __Leon__ (4 tiles) started with 22% missing, ended with 26% [UPLOADED TO DB]\n",
    "\n",
    "__Logs 2025-09-26__ \"202509 - Dev -  Joining all rasters in a month\" folder\n",
    "* __Irapuato__ (4 tiles) started with 22% missing, ended with 39% [UPLOADED TO DB]\n",
    "* __SLP__ (4 tiles) started with 6% missing, ended with 26% [UPLOADED TO DB]\n",
    "* __Merida__ (4 tiles) started with 17%, could not process 6 consecutive months mid-2018 and ended up with _Multiple missing months together_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21166f-6b16-4bbb-919a-46fbff441c66",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f97a65-60c1-42dd-bdbd-33111538ab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/accesibilidad-urbana/\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "current_path = Path().resolve()\n",
    "for parent in current_path.parents:\n",
    "    if parent.name == \"accesibilidad-urbana\":\n",
    "        module_path = str(parent)+'/'\n",
    "        break\n",
    "print(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e469adc-da66-4153-87e0-f5d7df6ae108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import aup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9f1b86-41ff-419b-8bb2-3136750c3985",
   "metadata": {},
   "source": [
    "## __Notebook config__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc6ab21-6522-4934-b7f4-5526623d40c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mexicali']\n"
     ]
    }
   ],
   "source": [
    "# Missing processing from\n",
    "city_list = ['Mexicali']\n",
    "# Saving\n",
    "save_output_database = True\n",
    "save_output_locally = True\n",
    "\n",
    "print(city_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72343e1e-ebd2-4af2-8cff-ecfaa1778180",
   "metadata": {},
   "source": [
    "## __NDVI config__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af39201-438a-4556-804f-4375734a1509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/accesibilidad-urbana/data/processed/tmp_ndvi/\n"
     ]
    }
   ],
   "source": [
    "band_name_dict = {'nir08':[False], #If GSD(resolution) of band is different, set True.\n",
    "                   'red':[False], #If GSD(resolution) of band is different, set True.\n",
    "                   'eq':['(nir08-red)/(nir08+red)']}\n",
    "query_sat = {\"eo:cloud_cover\": {\"lt\": 15},\n",
    "          \"platform\": {\"in\": [\"landsat-8\", \"landsat-9\"]}}\n",
    "index_analysis = 'ndvi'\n",
    "tmp_dir = module_path + f'data/processed/tmp_{index_analysis}/'\n",
    "res = [8,11]\n",
    "freq = 'MS'\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2023-12-31'\n",
    "satellite = 'landsat-c2-l2'\n",
    "\n",
    "print(tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0809d1e0-24b3-41a8-bd21-0b5f2b3f256d",
   "metadata": {},
   "source": [
    "## __Secondary functions -__ raster_to_save_hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abbcb73d-01ad-4446-999d-de878748ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_to_hex_save(hex_gdf_i, df_len, index_analysis, tmp_dir, city, r, save, local_save=False, i=0):\n",
    "    print(f'Translating raster to hexagon for res: {r}')\n",
    "\n",
    "    hex_raster_analysis, df_raster_analysis = aup.raster_to_hex_analysis(hex_gdf_i, df_len, index_analysis,\n",
    "                                                                tmp_dir, city, r)\n",
    "    print('Finished assigning raster data to hexagons')\n",
    "    print(f'df nan values: {df_raster_analysis[index_analysis].isna().sum()}')\n",
    "    if df_raster_analysis[index_analysis].isna().sum() > 0:\n",
    "        raise NanValues('NaN values are still present after processing')\n",
    "    \n",
    "    # local save (test)\n",
    "    if local_save:\n",
    "        # Create folder to store local save\n",
    "        localsave_dir = tmp_dir+'local_save/'\n",
    "        if os.path.exists(localsave_dir) == False:\n",
    "            os.mkdir(localsave_dir)\n",
    "\n",
    "        # Local save\n",
    "        #hex_raster_analysis.to_file(tmp_dir+'local_save/'+f'{city}_{index_analysis}_HexRes{r}_v{i}.geojson')\n",
    "        df_raster_analysis.to_csv(localsave_dir+f'{city}_{index_analysis}_HexRes{r}_v{i}.csv')\n",
    "\n",
    "    # Save - upload to database\n",
    "    if save:\n",
    "        upload_chunk = 150000\n",
    "        print(f'Starting upload for res: {r}')\n",
    "\n",
    "        if r == 8:\n",
    "            # df upload\n",
    "            #aup.df_to_db_slow(df_raster_analysis, f'{index_analysis}_complete_dataset_hex',\n",
    "            #                'raster_analysis', if_exists='append', chunksize=upload_chunk)\n",
    "            # gdf upload\n",
    "            aup.gdf_to_db_slow(hex_raster_analysis, f'{index_analysis}_analysis_hex',\n",
    "                            'raster_analysis', if_exists='append')\n",
    "\n",
    "        else:\n",
    "            # df upload\n",
    "            #limit_len = 5000000\n",
    "            #if len(df_raster_analysis)>limit_len:\n",
    "            #    c_upload = len(df_raster_analysis)/limit_len\n",
    "            #    for k in range(int(c_upload)+1):\n",
    "            #        print(f\"Starting range k = {k} of {int(c_upload)}\")\n",
    "            #        df_inter_upload = df_raster_analysis.iloc[int(limit_len*k):int(limit_len*(1+k))].copy()\n",
    "            #        aup.df_to_db(df_inter_upload,f'{index_analysis}_complete_dataset_hex',\n",
    "            #                        'raster_analysis', if_exists='append')\n",
    "            #else:\n",
    "            #    aup.df_to_db(df_raster_analysis,f'{index_analysis}_complete_dataset_hex',\n",
    "            #                        'raster_analysis', if_exists='append')\n",
    "            # gdf upload\n",
    "            aup.gdf_to_db_slow(hex_raster_analysis, f'{index_analysis}_analysis_hex',\n",
    "                            'raster_analysis', if_exists='append')\n",
    "        print(f'Finished uploading data for res{r}')\n",
    "        \n",
    "    # delete variables\n",
    "    del df_raster_analysis\n",
    "    del hex_raster_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a364733-d324-45a7-859f-9982d0cc6a58",
   "metadata": {},
   "source": [
    "## __Main function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a62284-1677-41df-9f29-31b565aaac43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING Mexicali.\n",
      "Mexicali - Created hex_city.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                | 0/72 [00:00<?, ?it/s]<string>:1: RuntimeWarning: overflow encountered in add\n",
      "<string>:1: RuntimeWarning: overflow encountered in add\n",
      "<string>:1: RuntimeWarning: overflow encountered in add\n",
      "<string>:1: RuntimeWarning: overflow encountered in add\n",
      "  1%|██                                                                                                                                                  | 1/72 [08:56<10:35:14, 536.82s/it]<string>:1: RuntimeWarning: overflow encountered in add\n",
      "<string>:1: RuntimeWarning: overflow encountered in add\n",
      "<string>:1: RuntimeWarning: overflow encountered in add\n",
      "<string>:1: RuntimeWarning: overflow encountered in add\n",
      "  3%|████▏                                                                                                                                                | 2/72 [15:33<8:50:02, 454.33s/it]<string>:1: RuntimeWarning: overflow encountered in add\n",
      "<string>:1: RuntimeWarning: overflow encountered in add\n",
      "<string>:1: RuntimeWarning: overflow encountered in add\n",
      "  4%|██████▏                                                                                                                                              | 3/72 [21:43<7:58:19, 415.94s/it]<string>:1: RuntimeWarning: overflow encountered in add\n",
      "<string>:1: RuntimeWarning: overflow encountered in add\n",
      "<string>:1: RuntimeWarning: overflow encountered in add\n",
      "<string>:1: RuntimeWarning: overflow encountered in add\n",
      "  6%|████████▎                                                                                                                                            | 4/72 [28:53<7:57:43, 421.52s/it]<string>:1: RuntimeWarning: overflow encountered in add\n",
      "<string>:1: RuntimeWarning: overflow encountered in add\n",
      "<string>:1: RuntimeWarning: overflow encountered in add\n",
      "<string>:1: RuntimeWarning: overflow encountered in add\n",
      "<string>:1: RuntimeWarning: overflow encountered in add\n",
      "  7%|██████████▎                                                                                                                                          | 5/72 [36:22<8:01:45, 431.42s/it]"
     ]
    }
   ],
   "source": [
    "for city in city_list:\n",
    "    print(f\"STARTING {city}.\")\n",
    "    ############################### CREATE AREA OF INTEREST\n",
    "    ### Create city area of interest with biggest hexs\n",
    "    big_res = min(res)\n",
    "    schema_hex = 'hexgrid'\n",
    "    table_hex = f'hexgrid_{big_res}_city_2020'\n",
    "    \n",
    "    # Download hexagons with type=urban\n",
    "    type = 'urban'\n",
    "    query = f\"SELECT hex_id_{big_res},geometry FROM {schema_hex}.{table_hex} WHERE \\\"city\\\" = '{city}\\' AND \\\"type\\\" = '{type}\\'\"\n",
    "    hex_urban = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "    \n",
    "    # Download hexagons with type=rural within 500m buffer\n",
    "    poly = hex_urban.to_crs(\"EPSG:6372\").buffer(500).reset_index()\n",
    "    poly = poly.to_crs(\"EPSG:4326\")\n",
    "    poly_wkt = poly.dissolve().geometry.to_wkt()[0]\n",
    "    type = 'rural'\n",
    "    query = f\"SELECT hex_id_{big_res},geometry FROM {schema_hex}.{table_hex} WHERE \\\"city\\\" = '{city}\\' AND \\\"type\\\" = '{type}\\' AND (ST_Intersects(geometry, \\'SRID=4326;{poly_wkt}\\'))\"\n",
    "    hex_rural = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "    \n",
    "    # Concatenate urban and rural hex\n",
    "    hex_city = pd.concat([hex_urban, hex_rural])\n",
    "\n",
    "    print(f\"{city} - Created hex_city.\")\n",
    "    \n",
    "    ############################### DOWNLOAD AND INTERPOLATE RASTERS\n",
    "    try:\n",
    "        df_len = aup.download_raster_from_pc(hex_city, index_analysis, city, freq,\n",
    "                                     start_date, end_date, tmp_dir, band_name_dict, \n",
    "                                     query=query_sat, satellite = satellite,\n",
    "                                     compute_unavailable_dates=True)\n",
    "        print(f\"{city} - Created df_len.\")\n",
    "    except:\n",
    "        print(f\"{city} - Failed df_len.\")\n",
    "        continue\n",
    "\n",
    "    ############################### RASTERS TO HEX\n",
    "    # Do NOT process and upload if used full_month processing since it is still a WIP.\n",
    "    if 'full_month' in df_len.download_method.unique():\n",
    "        full_months = len(df_len.loc[df_len.download_method == 'full_month'].copy())\n",
    "        print(f\"---------------------------------------\")\n",
    "        print(f\"{city} - Has {full_months} months that used 'full_month' processing.\")\n",
    "        print(f\"{city} - NOT PROCESSING TO HEXS AND NOT SAVING TO DATABASE.\")\n",
    "        print(f\"---------------------------------------\")\n",
    "        continue\n",
    "    \n",
    "    ### hex preprocessing\n",
    "    print(f\"{city} - Started loading hexagons at different resolutions.\")\n",
    "    \n",
    "    # Create res_list\n",
    "    res_list=[]\n",
    "    for r in range(res[0],res[-1]+1):\n",
    "        res_list.append(r)\n",
    "    \n",
    "    # Load hexgrids\n",
    "    hex_gdf = hex_city.copy()\n",
    "    hex_gdf.rename(columns={f'hex_id_{big_res}':'hex_id'}, inplace=True)\n",
    "    hex_gdf['res'] = big_res\n",
    "    \n",
    "    print(f\"{city} Loaded hexgrid res {big_res}.\")\n",
    "    \n",
    "    for r in res_list:\n",
    "        # biggest resolution already loaded\n",
    "        if r == big_res:\n",
    "            continue\n",
    "        \n",
    "        # Load hexgrid\n",
    "        table_hex = f'hexgrid_{r}_city_2020'\n",
    "        query = f\"SELECT hex_id_{r},geometry FROM {schema_hex}.{table_hex} WHERE \\\"city\\\"=\\'{city}\\' AND  (ST_Intersects(geometry, \\'SRID=4326;{poly_wkt}\\'))\"\n",
    "        hex_tmp = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "        # Format hexgrid\n",
    "        hex_tmp.rename(columns={f'hex_id_{r}':'hex_id'}, inplace=True)\n",
    "        hex_tmp['res'] = r\n",
    "        # Concatenate to hex_gdf\n",
    "        hex_gdf = pd.concat([hex_gdf, hex_tmp])\n",
    "    \n",
    "        print(f\"{city} - Loaded hexgrid res {r}.\")\n",
    "    \n",
    "        del hex_tmp\n",
    "    \n",
    "    print(f\"{city} - Finished creating hexagons at different resolutions.\")\n",
    "    \n",
    "    # Raster to hex function for each resolution (saves output)\n",
    "    skip_these_res = [8,9,10,11]\n",
    "    for r in list(hex_gdf.res.unique()):\n",
    "\n",
    "        if r in skip_these_res:\n",
    "            continue\n",
    "    \n",
    "        print(f\"---------------------------------------\")\n",
    "        print(f\"{city} - STARTING processing for resolution {r}.\")\n",
    "    \n",
    "        processing_chunk = 20000 # Use 20,000 max, crashed on DELL laptop with 50,000\n",
    "    \n",
    "        # filters hexagons at specified resolution\n",
    "        hex_gdf_res = hex_gdf.loc[hex_gdf.res==r].copy()\n",
    "        hex_gdf_res = hex_gdf_res.reset_index(drop=True)\n",
    "    \n",
    "        if len(hex_gdf_res)>processing_chunk:\n",
    "            print(f'hex_gdf_res len: {len(hex_gdf_res)} is bigger than processing chunk: {processing_chunk}')\n",
    "            c_processing = len(hex_gdf_res)/processing_chunk\n",
    "            print(f'There are {round(c_processing)} processes')\n",
    "            for i in range(int(c_processing)+1):\n",
    "                print(f'Processing from {i*processing_chunk} to {(i+1)*processing_chunk}')\n",
    "                hex_gdf_i = hex_gdf_res.iloc[int(processing_chunk*i):int(processing_chunk*(1+i))].copy()\n",
    "                raster_to_hex_save(hex_gdf_i, df_len, index_analysis, tmp_dir, city, r, \n",
    "                                   save = save_output_database, \n",
    "                                   local_save = save_output_locally, \n",
    "                                   i = i\n",
    "                                  )\n",
    "        else:\n",
    "            print('hex_gdf len smaller than processing chunk')\n",
    "            hex_gdf_i = hex_gdf_res.copy()\n",
    "            raster_to_hex_save(hex_gdf_i, df_len, index_analysis, tmp_dir, city, r, \n",
    "                               save = save_output_database, \n",
    "                               local_save = save_output_locally, \n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820ca42-5ae9-4e15-af33-402782c3b4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDS-10.0",
   "language": "python",
   "name": "gds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
