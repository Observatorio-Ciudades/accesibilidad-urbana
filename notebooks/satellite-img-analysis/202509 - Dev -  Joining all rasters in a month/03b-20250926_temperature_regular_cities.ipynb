{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34da882-1610-4e69-b6c2-dccff1960031",
   "metadata": {},
   "source": [
    "# Temperature regular cities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80d5290-59b2-4a75-b5f0-8ef1be58abe2",
   "metadata": {},
   "source": [
    "__Similar to Notebook 03a, but for temperature__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d7dbc-eec1-455b-b03c-18ffbab57304",
   "metadata": {},
   "source": [
    "This notebook tries to __run all processable missing cities__ (identified on notebook 02_missing_ndvi_temp_cities). \n",
    "* Canceled, uploaded all, but originaly: Even though all missing cities will run download_raster_from_pc(), __only cities with no full_month processing (just specific_dates processing) will be processed to hexs and uploaded to the database.__\n",
    "\n",
    "* NOTE: Cities where no full_month processing is used couldn't be previously processed not because of the availability of tiles in specific_dates, but because of a __(fixed) bug in available_datasets()__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dacee8a-8b03-41bf-b8d1-4dc38d7fa833",
   "metadata": {},
   "source": [
    "## Processing summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94261724-ea33-49a2-9555-07f5d11b310c",
   "metadata": {},
   "source": [
    "__Alienware Logs 2025-09-29__ \"202509 - Dev -  Joining all rasters in a month\" folder __[10% clouds]__\n",
    "* __Reynosa__ (2 tiles) started with 35% missing, could not process by the all at start of 2018 and ended up with __Multiple missing months together__.\n",
    "* __Morelia__ (3 tiles) started with 24% missing, could not process by the all at mid 2018 and ended up with __Multiple missing months together__.\n",
    "* * --> __Has full_month processing__\n",
    "* __Culiacan__ (3 tiles) started with 4% missing, and ended with 24% missing, [UPLOADED TO DB, BUT ERROR IN RES 11, FROM HEX SOURCE]\n",
    "* __Tampico__ (3 tiles) started with 56% missing, __Missing more than 50 percent of data points__.\n",
    "* __Monclova__ (3 tiles) started with 19% missing, could not process by the all at the end of 2018 and ended up with __Multiple missing months together__.\n",
    "* __Los Mochis__ (3 tiles) started with 4% missing, and ended with 32% missing, [FINISHED, NOT UPLOADED]\n",
    "* * --> __Has full_month processing__\n",
    "* __Hermosillo__ (3 tiles) started with 6% missing, and ended with 28% missing, [FINISHED, NOT UPLOADED]\n",
    "* * --> __Has full month processing__\n",
    "* __Nogales__ (3 tiles) started with 4% missing, and ended with 18% missing, [UPLOADED TO DB]\n",
    "* __Campeche__ (4 tiles) started with 14% missing, could not process by the all at start of 2018 and ended up with __Multiple missing months together__.\n",
    "* __Celaya__ (4 tiles) started with 12% missing, and ended with 31% missing, [UPLOADED TO DB]\n",
    "* __Uruapan__ (4 tiles) started with 38% missing, could not process at mid 2018 and ended up with __Multiple missing months together__.\n",
    "* __Tlaxcala__ (4 tiles) started with 19% missing, could not process by the all at the end of 2018 and ended up with __Multiple missing months together__.\n",
    "* __Xalapa__ (4 tiles) started with 19% missing, could not process 10/2021 and ended up __Missing more than 50 percent of data points__.\n",
    "* __Mexicali__ (6 tiles) started with 0% missing, and ended with 32% missing, [FINISHED, NOT UPLOADED]\n",
    "* * --> __Has full month processing__\n",
    "* __Ensenada__ (7 tiles) started with 10% missing, and ended with 28% missing, [UPLOADED TO DB]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b7da8-b23c-4bfd-be86-313573b87161",
   "metadata": {},
   "source": [
    "__Alienware Logs 2025-09-30__ \"202509 - Dev -  Joining all rasters in a month\" folder __[12% clouds]__\n",
    "* __Reynosa__ (2 tiles) started with __less (32%)__ missing, could not process by the all at start of 2018 and ended up with __Multiple missing months together__.\n",
    "* * --> __NOTE: Always missing 1 of 2 tiles.__\n",
    "* __Morelia__ (3 tiles) started with 24% missing, could not process by mid 2018 and ended up with __Multiple missing months together__.\n",
    "* * --> __Found new months__\n",
    "* * --> __Has full_month processing__\n",
    "* __Tampico__ (3 tiles) started with __less (47%)__ missing, __No df_len, Multiple missing months together__.\n",
    "* __Monclova__ (3 tiles) started with 19% missing, could not process by the all at the end of 2018 and ended up with __Multiple missing months together__.\n",
    "* * --> __Always missing tile '027042'__\n",
    "* __Campeche__ (4 tiles) started with __less (10%)__ missing, could not process by the all at start of 2018 and ended up with __Multiple missing months together__.\n",
    "* * --> __Fails even on months that have all available tiles__\n",
    "* __Uruapan__ (4 tiles) started with __less (32%)__ missing, could not process by the at mid __2020__ and ended up with __Multiple missing months together__.\n",
    "* * --> __Found new months__\n",
    "* __Tlaxcala__ (4 tiles) started with __less (11%)__ missing, could not process by the all at the end of 2018 and ended up with __Multiple missing months together__.\n",
    "* * --> __Found new months__\n",
    "* * --> __Has full_month processing__\n",
    "* __Xalapa__ (4 tiles) started with __less (15%)__ missing, could not process __3/2022__ and ended up __Missing more than 50 percent of data points__.\n",
    "* * --> __Found new months__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a77096-a9d8-4ee7-9f66-9b8ec0b8791f",
   "metadata": {},
   "source": [
    "__Alienware Logs 2025-09-01__ \"202509 - Dev -  Joining all rasters in a month\" folder __[15% clouds]__\n",
    "* __Reynosa__ (2 tiles) started with __less (24%)__ missing, could not process by the all at start of 2018 and ended up with __Multiple missing months together__.\n",
    "* * --> __NOTE: Always missing 1 of 2 tiles.__\n",
    "* __Morelia__ (3 tiles) started with __less (18%)__ missing, could not process by mid 2018 and ended up with __Multiple missing months together__.\n",
    "* * --> __Found new months__\n",
    "* * --> __Has full_month processing__\n",
    "* __Tampico__ (3 tiles) started with __less (36%)__ missing, failed 6 continous mid-end 2019 and ended up __Missing more than 50 percent of data points__.\n",
    "* * --> __Found new months__\n",
    "* __Monclova__ (3 tiles) started with __less (17%)__ missing, could not process by the all at the end of 2018 and ended up with __Multiple missing months together__.\n",
    "* * --> __Almost always missing tile '027042'__\n",
    "* __Campeche__ (4 tiles) started with 10% missing, could not process by the all at start of 2018 and ended up with __Multiple missing months together__.\n",
    "* * --> __Fails even on months that have all available tiles__\n",
    "* __Uruapan__ (4 tiles) started with __less (31%)__ missing, could not process by the at mid __2020__ and ended up with __Multiple missing months together__.\n",
    "* * --> __Found new months__\n",
    "* __Tlaxcala__ (4 tiles) started with __less (7%)__ missing, could not process by the all at the end of 2018 and ended up with __Multiple missing months together__.\n",
    "* * --> __Found new months__\n",
    "* * --> __Has full_month processing__\n",
    "* __Xalapa__ (4 tiles) started with __less (12%)__ missing, could not process __6/2022__ and ended up __Missing more than 50 percent of data points__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21166f-6b16-4bbb-919a-46fbff441c66",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f97a65-60c1-42dd-bdbd-33111538ab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/observatorio/Documents/repos/accesibilidad-urbana/\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "current_path = Path().resolve()\n",
    "for parent in current_path.parents:\n",
    "    if parent.name == \"accesibilidad-urbana\":\n",
    "        module_path = str(parent)+'/'\n",
    "        break\n",
    "print(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e469adc-da66-4153-87e0-f5d7df6ae108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import aup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9f1b86-41ff-419b-8bb2-3136750c3985",
   "metadata": {},
   "source": [
    "## __Notebook config__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc6ab21-6522-4934-b7f4-5526623d40c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Culiacan']\n"
     ]
    }
   ],
   "source": [
    "# Missing processing from\n",
    "city_list = ['Culiacan']\n",
    "\n",
    "# Processing\n",
    "raster_to_hex = True\n",
    "skip_these_res = [8,9,10] #For example, in case of fixing particular upload\n",
    "\n",
    "# Saving\n",
    "save_output_database = True\n",
    "save_output_locally = True\n",
    "\n",
    "print(city_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72343e1e-ebd2-4af2-8cff-ecfaa1778180",
   "metadata": {},
   "source": [
    "## __Temperature config__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af39201-438a-4556-804f-4375734a1509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/observatorio/Documents/repos/accesibilidad-urbana/data/processed/tmp_temperature/\n"
     ]
    }
   ],
   "source": [
    "band_name_dict = {'lwir11':[False],\n",
    "                  'eq':[\"((lwir11*0.00341802) + 149.0)-273.15\"]}\n",
    "query_sat = {\"eo:cloud_cover\": {\"lt\": 10},\n",
    "          \"platform\": {\"in\": [\"landsat-8\", \"landsat-9\"]}}\n",
    "index_analysis = 'temperature'\n",
    "tmp_dir = module_path + f'data/processed/tmp_{index_analysis}/'\n",
    "res = [8,11]\n",
    "freq = 'MS'\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2023-12-31'\n",
    "satellite = 'landsat-c2-l2'\n",
    "\n",
    "print(tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0809d1e0-24b3-41a8-bd21-0b5f2b3f256d",
   "metadata": {},
   "source": [
    "## __Secondary functions -__ raster_to_save_hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abbcb73d-01ad-4446-999d-de878748ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_to_hex_save(hex_gdf_i, df_len, index_analysis, tmp_dir, city, r, save, local_save=False, i=0):\n",
    "    print(f'Translating raster to hexagon for res: {r}')\n",
    "\n",
    "    hex_raster_analysis, df_raster_analysis = aup.raster_to_hex_analysis(hex_gdf_i, df_len, index_analysis,\n",
    "                                                                tmp_dir, city, r)\n",
    "    print('Finished assigning raster data to hexagons')\n",
    "    print(f'df nan values: {df_raster_analysis[index_analysis].isna().sum()}')\n",
    "    if df_raster_analysis[index_analysis].isna().sum() > 0:\n",
    "        raise NanValues('NaN values are still present after processing')\n",
    "    \n",
    "    # local save (test)\n",
    "    if local_save:\n",
    "        # Create folder to store local save\n",
    "        localsave_dir = tmp_dir+'local_save/'\n",
    "        if os.path.exists(localsave_dir) == False:\n",
    "            os.mkdir(localsave_dir)\n",
    "\n",
    "        # Local save\n",
    "        #hex_raster_analysis.to_file(tmp_dir+'local_save/'+f'{city}_{index_analysis}_HexRes{r}_v{i}.geojson')\n",
    "        df_raster_analysis.to_csv(localsave_dir+f'{city}_{index_analysis}_HexRes{r}_v{i}.csv')\n",
    "\n",
    "    # Save - upload to database\n",
    "    if save:\n",
    "        upload_chunk = 150000\n",
    "        print(f'Starting upload for res: {r}')\n",
    "\n",
    "        if r == 8:\n",
    "            # df upload\n",
    "            #aup.df_to_db_slow(df_raster_analysis, f'{index_analysis}_complete_dataset_hex',\n",
    "            #                'raster_analysis', if_exists='append', chunksize=upload_chunk)\n",
    "            # gdf upload\n",
    "            aup.gdf_to_db_slow(hex_raster_analysis, f'{index_analysis}_analysis_hex',\n",
    "                            'raster_analysis', if_exists='append')\n",
    "\n",
    "        else:\n",
    "            # df upload\n",
    "            #limit_len = 5000000\n",
    "            #if len(df_raster_analysis)>limit_len:\n",
    "            #    c_upload = len(df_raster_analysis)/limit_len\n",
    "            #    for k in range(int(c_upload)+1):\n",
    "            #        print(f\"Starting range k = {k} of {int(c_upload)}\")\n",
    "            #        df_inter_upload = df_raster_analysis.iloc[int(limit_len*k):int(limit_len*(1+k))].copy()\n",
    "            #        aup.df_to_db(df_inter_upload,f'{index_analysis}_complete_dataset_hex',\n",
    "            #                        'raster_analysis', if_exists='append')\n",
    "            #else:\n",
    "            #    aup.df_to_db(df_raster_analysis,f'{index_analysis}_complete_dataset_hex',\n",
    "            #                        'raster_analysis', if_exists='append')\n",
    "            # gdf upload\n",
    "            aup.gdf_to_db_slow(hex_raster_analysis, f'{index_analysis}_analysis_hex',\n",
    "                            'raster_analysis', if_exists='append')\n",
    "        print(f'Finished uploading data for res{r}')\n",
    "        \n",
    "    # delete variables\n",
    "    del df_raster_analysis\n",
    "    del hex_raster_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a364733-d324-45a7-859f-9982d0cc6a58",
   "metadata": {},
   "source": [
    "## __Main function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730b7380-e21d-488c-8bd7-ab1570e67d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING Culiacan.\n",
      "Culiacan - Created hex_city.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:00<00:00, 805.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Culiacan - Created df_len.\n",
      "Culiacan - Started loading hexagons at different resolutions.\n",
      "Culiacan Loaded hexgrid res 8.\n",
      "Culiacan - Loaded hexgrid res 9.\n",
      "Culiacan - Loaded hexgrid res 10.\n",
      "Culiacan - Loaded hexgrid res 11.\n",
      "Culiacan - Finished creating hexagons at different resolutions.\n",
      "Skipped saving for res 8.\n",
      "Skipped saving for res 9.\n",
      "Skipped saving for res 10.\n",
      "---------------------------------------\n",
      "Culiacan - STARTING processing for resolution 11.\n",
      "hex_gdf_res len: 145793 is bigger than processing chunk: 100000\n",
      "There are 1 processes\n",
      "Processing from 0 to 100000\n",
      "Translating raster to hexagon for res: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                            | 0/6 [00:00<?, ?it/s]\n",
      " 17%|██████████████████████                                                                                                              | 1/6 [01:09<05:45, 69.16s/it]\u001b[A\n",
      "\n",
      "  0%|                                                                                                                                           | 0/12 [01:09<?, ?it/s]\u001b[A\u001b[A\n",
      " 33%|████████████████████████████████████████████                                                                                        | 2/6 [02:19<04:38, 69.64s/it]\n",
      "  0%|                                                                                                                                           | 0/12 [01:09<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████████████████████████████████████████████████                                                                  | 3/6 [03:29<03:29, 69.80s/it]\n",
      "\n",
      "  0%|                                                                                                                                           | 0/12 [01:09<?, ?it/s]\u001b[A\u001b[A\n",
      " 67%|████████████████████████████████████████████████████████████████████████████████████████                                            | 4/6 [04:38<02:19, 69.75s/it]\n",
      "  0%|                                                                                                                                           | 0/12 [01:09<?, ?it/s]\u001b[A\n",
      " 83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████                      | 5/6 [05:48<01:09, 69.85s/it]\n",
      "\n",
      "  0%|                                                                                                                                           | 0/12 [01:10<?, ?it/s]\u001b[A\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [06:58<00:00, 69.79s/it]\n",
      "  0%|                                                                                                                                           | 0/12 [01:09<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished assigning raster data to hexagons\n",
      "df nan values: 0\n",
      "Starting upload for res: 11\n",
      "Finished uploading data for res11\n",
      "Processing from 100000 to 200000\n",
      "Translating raster to hexagon for res: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                            | 0/6 [00:00<?, ?it/s]\n",
      " 17%|██████████████████████                                                                                                              | 1/6 [00:32<02:41, 32.34s/it]\u001b[A\n",
      "\n",
      "  0%|                                                                                                                                           | 0/12 [00:32<?, ?it/s]\u001b[A\u001b[A\n",
      " 33%|████████████████████████████████████████████                                                                                        | 2/6 [01:06<02:13, 33.29s/it]\n",
      "  0%|                                                                                                                                           | 0/12 [00:33<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████████████████████████████████████████████████                                                                  | 3/6 [01:41<01:42, 34.08s/it]\n",
      "\n",
      "  0%|                                                                                                                                           | 0/12 [00:35<?, ?it/s]\u001b[A\u001b[A\n",
      " 67%|████████████████████████████████████████████████████████████████████████████████████████                                            | 4/6 [02:16<01:08, 34.40s/it]\n",
      "  0%|                                                                                                                                           | 0/12 [00:34<?, ?it/s]\u001b[A\n",
      " 83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████                      | 5/6 [02:50<00:34, 34.42s/it]\n",
      "\n",
      "  0%|                                                                                                                                           | 0/12 [00:34<?, ?it/s]\u001b[A\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [03:22<00:00, 33.82s/it]\n",
      "  0%|                                                                                                                                           | 0/12 [00:32<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished assigning raster data to hexagons\n",
      "df nan values: 0\n",
      "Starting upload for res: 11\n",
      "Finished uploading data for res11\n"
     ]
    }
   ],
   "source": [
    "for city in city_list:\n",
    "    print(f\"STARTING {city}.\")\n",
    "    ############################### CREATE AREA OF INTEREST\n",
    "    ### Create city area of interest with biggest hexs\n",
    "    big_res = min(res)\n",
    "    schema_hex = 'hexgrid'\n",
    "    table_hex = f'hexgrid_{big_res}_city_2020'\n",
    "    \n",
    "    # Download hexagons with type=urban\n",
    "    type = 'urban'\n",
    "    query = f\"SELECT hex_id_{big_res},geometry FROM {schema_hex}.{table_hex} WHERE \\\"city\\\" = '{city}\\' AND \\\"type\\\" = '{type}\\'\"\n",
    "    hex_urban = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "    \n",
    "    # Download hexagons with type=rural within 500m buffer\n",
    "    poly = hex_urban.to_crs(\"EPSG:6372\").buffer(500).reset_index()\n",
    "    poly = poly.to_crs(\"EPSG:4326\")\n",
    "    poly_wkt = poly.dissolve().geometry.to_wkt()[0]\n",
    "    type = 'rural'\n",
    "    query = f\"SELECT hex_id_{big_res},geometry FROM {schema_hex}.{table_hex} WHERE \\\"city\\\" = '{city}\\' AND \\\"type\\\" = '{type}\\' AND (ST_Intersects(geometry, \\'SRID=4326;{poly_wkt}\\'))\"\n",
    "    hex_rural = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "    \n",
    "    # Concatenate urban and rural hex\n",
    "    hex_city = pd.concat([hex_urban, hex_rural])\n",
    "\n",
    "    print(f\"{city} - Created hex_city.\")\n",
    "    \n",
    "    ############################### DOWNLOAD AND INTERPOLATE RASTERS\n",
    "    try:\n",
    "        df_len = aup.download_raster_from_pc(hex_city, index_analysis, city, freq,\n",
    "                                     start_date, end_date, tmp_dir, band_name_dict, \n",
    "                                     query=query_sat, satellite = satellite,\n",
    "                                     compute_unavailable_dates=True)\n",
    "        print(f\"{city} - Created df_len.\")\n",
    "    except:\n",
    "        print(f\"{city} - Failed df_len.\")\n",
    "        continue\n",
    "\n",
    "    ############################### RASTERS TO HEX\n",
    "    if raster_to_hex:\n",
    "        # Do NOT process and upload if used full_month processing since it is still a WIP.\n",
    "        #if 'full_month' in df_len.download_method.unique():\n",
    "        #    full_months = len(df_len.loc[df_len.download_method == 'full_month'].copy())\n",
    "        #    print(f\"---------------------------------------\")\n",
    "        #    print(f\"{city} - Has {full_months} months that used 'full_month' processing.\")\n",
    "        #    print(f\"{city} - NOT PROCESSING TO HEXS AND NOT SAVING TO DATABASE.\")\n",
    "        #    print(f\"---------------------------------------\")\n",
    "        #    continue\n",
    "        \n",
    "        ### hex preprocessing\n",
    "        print(f\"{city} - Started loading hexagons at different resolutions.\")\n",
    "        \n",
    "        # Create res_list\n",
    "        res_list=[]\n",
    "        for r in range(res[0],res[-1]+1):\n",
    "            res_list.append(r)\n",
    "        \n",
    "        # Load hexgrids\n",
    "        hex_gdf = hex_city.copy()\n",
    "        hex_gdf.rename(columns={f'hex_id_{big_res}':'hex_id'}, inplace=True)\n",
    "        hex_gdf['res'] = big_res\n",
    "        \n",
    "        print(f\"{city} Loaded hexgrid res {big_res}.\")\n",
    "        \n",
    "        for r in res_list:\n",
    "            # biggest resolution already loaded\n",
    "            if r == big_res:\n",
    "                continue\n",
    "            \n",
    "            # Load hexgrid\n",
    "            table_hex = f'hexgrid_{r}_city_2020'\n",
    "            query = f\"SELECT hex_id_{r},geometry FROM {schema_hex}.{table_hex} WHERE \\\"city\\\"=\\'{city}\\' AND  (ST_Intersects(geometry, \\'SRID=4326;{poly_wkt}\\'))\"\n",
    "            hex_tmp = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "            # Format hexgrid\n",
    "            hex_tmp.rename(columns={f'hex_id_{r}':'hex_id'}, inplace=True)\n",
    "            hex_tmp['res'] = r\n",
    "            # Concatenate to hex_gdf\n",
    "            hex_gdf = pd.concat([hex_gdf, hex_tmp])\n",
    "        \n",
    "            print(f\"{city} - Loaded hexgrid res {r}.\")\n",
    "        \n",
    "            del hex_tmp\n",
    "        \n",
    "        print(f\"{city} - Finished creating hexagons at different resolutions.\")\n",
    "        \n",
    "        # Raster to hex function for each resolution (saves output)\n",
    "        for r in list(hex_gdf.res.unique()):\n",
    "    \n",
    "            if r in skip_these_res:\n",
    "                print(f\"Skipped saving for res {r}.\")\n",
    "                continue\n",
    "        \n",
    "            print(f\"---------------------------------------\")\n",
    "            print(f\"{city} - STARTING processing for resolution {r}.\")\n",
    "    \n",
    "            # 20,000 max. on DELL laptop, crashed on DELL laptop with 50,000\n",
    "            # 100,000 works on Alienware laptop.\n",
    "            processing_chunk = 100000\n",
    "        \n",
    "            # filters hexagons at specified resolution\n",
    "            hex_gdf_res = hex_gdf.loc[hex_gdf.res==r].copy()\n",
    "            hex_gdf_res = hex_gdf_res.reset_index(drop=True)\n",
    "        \n",
    "            if len(hex_gdf_res)>processing_chunk:\n",
    "                print(f'hex_gdf_res len: {len(hex_gdf_res)} is bigger than processing chunk: {processing_chunk}')\n",
    "                c_processing = len(hex_gdf_res)/processing_chunk\n",
    "                print(f'There are {round(c_processing)} processes')\n",
    "                for i in range(int(c_processing)+1):\n",
    "                    print(f'Processing from {i*processing_chunk} to {(i+1)*processing_chunk}')\n",
    "                    hex_gdf_i = hex_gdf_res.iloc[int(processing_chunk*i):int(processing_chunk*(1+i))].copy()\n",
    "                    raster_to_hex_save(hex_gdf_i, df_len, index_analysis, tmp_dir, city, r, \n",
    "                                       save = save_output_database, \n",
    "                                       local_save = save_output_locally, \n",
    "                                       i = i\n",
    "                                      )\n",
    "            else:\n",
    "                print('hex_gdf len smaller than processing chunk')\n",
    "                hex_gdf_i = hex_gdf_res.copy()\n",
    "                raster_to_hex_save(hex_gdf_i, df_len, index_analysis, tmp_dir, city, r, \n",
    "                                   save = save_output_database, \n",
    "                                   local_save = save_output_locally, \n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820ca42-5ae9-4e15-af33-402782c3b4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
