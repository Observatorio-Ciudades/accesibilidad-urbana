{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "056c69b3-2f41-494d-97fb-4753127738ce",
   "metadata": {},
   "source": [
    "# 18-proximity-analysis-for-aoi.py Script pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3dc367-4b24-4c76-be29-79d3f763ffef",
   "metadata": {},
   "source": [
    "__Notebooks starting with \"prox b-\" follow the progress made on \"prox a-\" notebooks.__\n",
    "\n",
    "This notebook takes as input: \n",
    "* Area of interest\n",
    "* Points of interest\n",
    "* Population data\n",
    "* Eje-amenity-source-code dictionary\n",
    "* Weights dictionary\n",
    "* Output hex resolution\n",
    "\n",
    "And creates a proximity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187cd46a-5e58-4292-a4f9-34f9f19d9e7e",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779e2272-7f0c-432b-bdd0-a68b7feeb000",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/lib/python3.10/site-packages/osmnx/utils.py:192: UserWarning: The `utils.config` function is deprecated and will be removed in a future release. Instead, use the `settings` module directly to configure a global setting's value. For example, `ox.settings.log_console=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    import aup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f60b5ae-9e37-46d9-96b9-fe643d5b4212",
   "metadata": {},
   "source": [
    "## Step 0 : Notebook config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a36e13-2043-4a31-b4e2-400828c1ddc6",
   "metadata": {},
   "source": [
    "### Notebook config - Basic analysis data (Required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986d4d9-fe79-4ef1-8c3a-285d24e83db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of area of interest\n",
    "city = 'Aguascalientes'\n",
    "# Resolutions of hexgrid output\n",
    "res_list = [8,9]\n",
    "# Save final output to db?\n",
    "save = True\n",
    "# Save disk space by deleting used data that will not be used after?\n",
    "save_space = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d607e-83d2-453d-ab43-42e62fe5b217",
   "metadata": {},
   "source": [
    "### Notebook config - Input data Directories (Required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ce0667-f763-48b8-9ca9-0ca3bdfabf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of area of interest (Required)\n",
    "aoi_dir = \"../../data/external/prox_latam/aoi_ags.gpkg\"\n",
    "# Location of points of interest (Required)\n",
    "pois_dir = \"../../data/external/prox_latam/pois_ags.gpkg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b51a76-ffbc-4ade-a8cc-4b3e9feb23b3",
   "metadata": {},
   "source": [
    "### Notebook config - Add population data to analysis? (Optional, required if pop_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459256b9-9701-44c9-a61b-6c2fb0b3cf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hexgrid with population data?\n",
    "pop_output = True\n",
    "# Location of pop data file (Optional, if pop_output = True)\n",
    "pop_dir = \"../../data/external/prox_latam/pop_gdf_ags.gpkg\"\n",
    "# Insert name of column with pop data (Optional, if pop_output = True)\n",
    "pop_column = 'pobtot'\n",
    "# Insert name of pop gdf index column (Optional, if pop_output = True)\n",
    "pop_index_column = 'cvegeo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5edaed2-3a06-41b0-b581-6a3329628234",
   "metadata": {},
   "source": [
    "### Notebook config - Local save? (Optional, required if local_save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f79d2a-6dfd-4de6-bcc3-5548c470b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each step's data to local?\n",
    "local_save = True\n",
    "# STEP 1: Create osmnx network (Based on Script 07-download_osmnx)\n",
    "nodes_save_dir = '../../data/external/temporal_fromjupyter/01_nodes_concat.gpkg'\n",
    "edges_save_dir = '../../data/external/temporal_fromjupyter/01_edges_concat.gpkg'\n",
    "# STEP 2: Calculate distance from pois to nearest osmnx node in tidy data (Based on Script 01_denue_to_nodes)\n",
    "nearest_save_dir = '../../data/external/temporal_fromjupyter/02_pois_distance_node_concat.gpkg'\n",
    "# STEP 3: Calculate distance from each node to nearest source amenity (Based on Script 02_distance_amenities)\n",
    "nodesproximity_save_dir = '../../data/external/temporal_fromjupyter/03_nodes_proximity_2020_concat.gpkg'\n",
    "# STEP 4: Group and analyze (from distance data in nodes to proximity in hexagons) (Based on Script 15-15-min-cities)\n",
    "proximityanalysis_save_dir = '../../data/external/temporal_fromjupyter/04_proximityanalysis_hexres9_concat.gpkg'\n",
    "# STEP 4: Saves pop hexgrid to local [only used if pop_output = True and local_save = True]\n",
    "pop_save_dir = '../../data/external/temporal_fromjupyter/04_hex_pop_concat.gpkg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a0bd4-0574-42c7-94b3-a2c61e68515d",
   "metadata": {},
   "source": [
    "### Notebook config - Parameters dicc (Required, must follow eje-amenity-sources-codes structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f04f2127-b094-4842-8597-5628dd7e9da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#--------------- PREPARE DATA ---------- SET PARAMETER\n",
    "    #This step sets the ejes, amenidades, sources and codes for further analysis\n",
    "            #{Eje (e):\n",
    "            #            {Amenity (a):\n",
    "            #                          {Sources (s):\n",
    "            #                                           [Codes (c)]\n",
    "            #                           }\n",
    "            #             }\n",
    "            #}\n",
    "parameters = {'Escuelas':{'Preescolar':{'denue_preescolar':[611111, 611112]},\n",
    "                          'Primaria':{'denue_primaria':[611121, 611122]},\n",
    "                          'Secundaria':{'denue_secundaria':[611131, 611132]}\n",
    "                         },\n",
    "             'Servicios comunitarios':{'Salud':{'clues_primer_nivel':[8610]},\n",
    "                                       'Guarderías':{'denue_guarderias':[624411, 624412]},\n",
    "                                       'Asistencia social':{'denue_dif':[931610]}\n",
    "                                      },\n",
    "              'Comercio':{'Alimentos':{'denue_supermercado':[462111],\n",
    "                                       'denue_abarrotes':[461110], \n",
    "                                       'denue_carnicerias': [461121, 461122, 461123],\n",
    "                                       'sip_mercado':[4721]},\n",
    "                          'Personal':{'denue_peluqueria':[812110]},\n",
    "                          'Farmacias':{'denue_farmacias':[464111, 464112]},\n",
    "                          'Hogar':{'denue_ferreteria_tlapaleria':[467111],\n",
    "                                   'denue_art_limpieza':[467115]},\n",
    "                          'Complementarios':{'denue_ropa':[463211, 463212, 463213, 463215, 463216, 463218],\n",
    "                                             'denue_calzado':[463310], \n",
    "                                             'denue_muebles':[466111, 466112, 466113, 466114],\n",
    "                                             'denue_lavanderia':[812210],\n",
    "                                             'denue_revistas_periodicos':[465313],\n",
    "                                             'denue_pintura':[467113]}\n",
    "                         },\n",
    "              'Entretenimiento':{'Social':{'denue_restaurante_insitu':[722511, 722512, 722513, 722514, 722519],\n",
    "                                           'denue_restaurante_llevar':[722516, 722518, 722517],\n",
    "                                           'denue_bares':[722412],\n",
    "                                           'denue_cafe':[722515]},\n",
    "                                 'Actividad física':{'sip_cancha':[93110],\n",
    "                                                     'sip_unidad_deportiva':[93111],\n",
    "                                                     'sip_espacio_publico':[9321],\n",
    "                                                     'denue_parque_natural':[712190]},\n",
    "                                 'Cultural':{'denue_cines':[512130],\n",
    "                                             'denue_museos':[712111, 712112]}\n",
    "                                } \n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfa0210-9abc-4bb9-a5e7-aa4687c3b720",
   "metadata": {},
   "source": [
    "### Notebook config - Weights (Required, must follow amenity-sources-('max' or 'min') structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d70caead-c426-4a33-941f-39ace6740c96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If need to measure nearest source for amenity, doesn't matter which, choose 'min'\n",
    "# If need to measure access to all of the different sources in an amenity, choose 'max'\n",
    "\n",
    "source_weight = {'Escuelas':{'Preescolar':'max', #There is only one source, no effect.\n",
    "                             'Primaria':'max',  #There is only one source, no effect.\n",
    "                             'Secundaria':'max'},  #There is only one source, no effect.\n",
    "                 'Servicios comunitarios':{'Salud':'max',  #There is only one source, no effect.\n",
    "                                           'Guarderías':'max', #There is only one source, no effect.\n",
    "                                           'Asistencia social':'max'},  #There is only one source, no effect.\n",
    "                 'Comercio':{'Alimentos':'min', # /////////////////////////////////////////////////////// Will choose min time to source because measuring access to nearest food source, doesn't matter which.\n",
    "                             'Personal':'max', #There is only one source, no effect.\n",
    "                             'Farmacias':'max', #There is only one source, no effect.\n",
    "                             'Hogar':'min', # ////////////////////////////////////////////////////////// Will choose min time to source because measuring access to nearest source, doesn't matter which.\n",
    "                             'Complementarios':'min'}, # /////////////////////////////////////////////// Will choose min time to source because measuring access to nearest source, doesn't matter which.\n",
    "                 'Entretenimiento':{'Social':'max', # ////////////////////////////////////////////////// Will choose max time to source because measuring access to all of them.\n",
    "                                    'Actividad física':'min', # //////////////////////////////////////// Will choose min time to source because measuring access to nearest source, doesn't matter which.\n",
    "                                    'Cultural':'min'} # //////////////////////////////////////////////// Will choose min time to source because measuring access to nearest source, doesn't matter which.\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963c0f12-c90c-410c-8b63-c5a152e8a065",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Download OSMnx network (G, nodes and edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e9fdc4-2432-431a-817c-b6cfe0765ab2",
   "metadata": {},
   "source": [
    "### Step 1: Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "737c706e-a9d8-4372-acb2-f3df0f7fe733",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_osmnx(aoi):\n",
    "    #Read area of interest as a polygon\n",
    "    poly = aoi.geometry\n",
    "\n",
    "    # Extracts coordinates from polygon as DataFrame\n",
    "    coord_val = poly.bounds\n",
    "\n",
    "    # Gets coordinates for bounding box\n",
    "    n = coord_val.maxy.max()\n",
    "    s = coord_val.miny.min()\n",
    "    e = coord_val.maxx.max()\n",
    "    w = coord_val.minx.min()\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    print(f\"Extracted min and max coordinates from the municipality. Polygon N:{round(n,5)}, S:{round(s,5)}, E{round(e,5)}, W{round(w,5)}.\")\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Downloads OSMnx graph from bounding box\n",
    "    G = ox.graph_from_bbox(n, s, e, w, network_type=\"all_private\")\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    print(\"Downloaded data from OSMnx.\")\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #Transforms graph to nodes and edges Geodataframe\n",
    "    nodes, edges = ox.graph_to_gdfs(G)\n",
    "\n",
    "    #Resets index to access osmid as a column\n",
    "    nodes.reset_index(inplace=True)\n",
    "\n",
    "    #Resets index to acces u and v as columns\n",
    "    edges.reset_index(inplace=True)\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    print(f\"Converted OSMnx graph to {len(nodes)} nodes and {len(edges)} edges GeoDataFrame.\")\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Defines columns of interest for nodes and edges\n",
    "    nodes_columns = [\"osmid\", \"x\", \"y\", \"street_count\", \"geometry\"]\n",
    "    edges_columns = [\n",
    "        \"osmid\",\n",
    "        \"v\",\n",
    "        \"u\",\n",
    "        \"key\",\n",
    "        \"oneway\",\n",
    "        \"lanes\",\n",
    "        \"name\",\n",
    "        \"highway\",\n",
    "        \"maxspeed\",\n",
    "        \"length\",\n",
    "        \"geometry\",\n",
    "        \"bridge\",\n",
    "        \"ref\",\n",
    "        \"junction\",\n",
    "        \"tunnel\",\n",
    "        \"access\",\n",
    "        \"width\",\n",
    "        \"service\",\n",
    "    ]\n",
    "\n",
    "    # if column doesn't exist it creates it as nan\n",
    "    for c in nodes_columns:\n",
    "        if c not in nodes.columns:\n",
    "            nodes[c] = np.nan\n",
    "            #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "            print(f\"Added column {c} for nodes.\")\n",
    "            #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    for c in edges_columns:\n",
    "        if c not in edges.columns:\n",
    "            edges[c] = np.nan\n",
    "            #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "            print(f\"Added column {c} for edges.\")\n",
    "            #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Filters GeoDataFrames for relevant columns\n",
    "    nodes = nodes[nodes_columns]\n",
    "    edges = edges[edges_columns]\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    print(\"Filtered columns.\")\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Converts columns with lists to strings to allow saving to local and further processes.\n",
    "    for col in nodes.columns:\n",
    "        if any(isinstance(val, list) for val in nodes[col]):\n",
    "            nodes[col] = nodes[col].astype('string')\n",
    "            #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "            print(f\"Column: {col} in nodes gdf, has a list in it, the column data was converted to string.\")\n",
    "            #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    for col in edges.columns:\n",
    "        if any(isinstance(val, list) for val in edges[col]):\n",
    "            edges[col] = edges[col].astype('string')\n",
    "            #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "            print(f\"Column: {col} in nodes gdf, has a list in it, the column data was converted to string.\")\n",
    "            #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return G,nodes,edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d4093-a34f-4e86-ac44-bdbf6a488887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read area of interest (aoi)\n",
    "aoi = gpd.read_file(aoi_dir)\n",
    "aoi = aoi.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Show\n",
    "aoi.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1df9b7a6-4309-4896-b36f-eab858cf2e38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted min and max coordinates from the municipality. Polygon N:22.10033, S:21.62227, E-102.06451, W-102.59887.\n",
      "Downloaded data from OSMnx.\n",
      "Converted OSMnx graph to 60233 nodes and 142263 edges GeoDataFrame.\n",
      "Filtered columns.\n",
      "Column: osmid in nodes gdf, has a list in it, the column data was converted to string.\n",
      "Column: lanes in nodes gdf, has a list in it, the column data was converted to string.\n",
      "Column: name in nodes gdf, has a list in it, the column data was converted to string.\n",
      "Column: highway in nodes gdf, has a list in it, the column data was converted to string.\n",
      "Column: maxspeed in nodes gdf, has a list in it, the column data was converted to string.\n",
      "Column: ref in nodes gdf, has a list in it, the column data was converted to string.\n",
      "Saved space by deleting used data.\n"
     ]
    }
   ],
   "source": [
    "# Download osmnx network (G, nodes and edges from bounding box of aoi)\n",
    "G, nodes, edges = download_osmnx(aoi)\n",
    "\n",
    "if local_save:\n",
    "    nodes.to_file(nodes_save_dir, driver='GPKG')\n",
    "    edges.to_file(edges_save_dir, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f180d5-54c7-4948-ac9e-85f8c83914ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Calculate distance from each poi to nearest node (osmid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c662dc-97c4-44d2-825b-0fb5f39b6c52",
   "metadata": {},
   "source": [
    "### Step 2 - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443cf187-620e-429e-922e-eabc360f374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read points of interest (pois)\n",
    "pois = gpd.read_file(pois_dir)\n",
    "pois = pois.set_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674c9a1-c2ae-4ceb-8550-7645ba7c7776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show\n",
    "print(pois.shape)\n",
    "pois.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73ddda5b-81ac-4271-afcd-2aa7efa9fce8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated distances from pois to nearest node.\n"
     ]
    }
   ],
   "source": [
    "# Filter pois for aoi\n",
    "pois_aoi = gpd.sjoin(pois,aoi,how='inner')\n",
    "pois = pois_aoi[['code','geometry']]\n",
    "\n",
    "# Format to calculate nearest\n",
    "nodes_gdf = nodes.set_crs(\"EPSG:4326\")\n",
    "edges_gdf = edges.set_crs(\"EPSG:4326\")\n",
    "nodes_gdf = nodes_gdf.set_index('osmid')\n",
    "edges_gdf = edges_gdf.set_index([\"u\", \"v\", \"key\"])\n",
    "\n",
    "# Calculate nearest\n",
    "nearest = aup.find_nearest(G, nodes_gdf, pois, return_distance= True)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "print(\"Calculated distances from pois to nearest node.\")\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Saves space in disk if instructed\n",
    "if save_space:\n",
    "    del nodes\n",
    "    del edges\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    print(\"Saved space by deleting used data.\")\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "if local_save:\n",
    "    nearest.to_file(nearest_save_dir, driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "823666bc-7e15-427b-a3b6-4444d3a88207",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20792, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>geometry</th>\n",
       "      <th>osmid</th>\n",
       "      <th>distance_node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>611111</td>\n",
       "      <td>POINT (-102.27464 21.90191)</td>\n",
       "      <td>961580633</td>\n",
       "      <td>16.377978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>611111</td>\n",
       "      <td>POINT (-102.26601 21.85971)</td>\n",
       "      <td>2253747737</td>\n",
       "      <td>71.553289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     code                     geometry       osmid  distance_node\n",
       "0  611111  POINT (-102.27464 21.90191)   961580633      16.377978\n",
       "1  611111  POINT (-102.26601 21.85971)  2253747737      71.553289"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show\n",
    "print(nearest.shape)\n",
    "nearest.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da178102-647c-4d3a-b322-8bb9d937cfcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Calculate distance to nearest source amenity for each node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00571593-b366-454e-929d-de2800954258",
   "metadata": {},
   "source": [
    "### Step 3 - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3f17f00-fa07-44fe-a153-7bc9344c6b26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a total of 306 points of interest of the source denue_preescolar for analisis.\n",
      "Starting range k = 0 of 1 for source denue_preescolar.\n",
      "Starting range k = 1 of 1 for source denue_preescolar.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_preescolar.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_preescolar to nodes_sources.\n",
      "Loaded a total of 309 points of interest of the source denue_primaria for analisis.\n",
      "Starting range k = 0 of 1 for source denue_primaria.\n",
      "Starting range k = 1 of 1 for source denue_primaria.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_primaria.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_primaria to nodes_sources.\n",
      "Loaded a total of 70 points of interest of the source denue_secundaria for analisis.\n",
      "Starting range k = 0 of 0 for source denue_secundaria.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_secundaria.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_secundaria to nodes_sources.\n",
      "Loaded a total of 132 points of interest of the source clues_primer_nivel for analisis.\n",
      "Starting range k = 0 of 0 for source clues_primer_nivel.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source clues_primer_nivel.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source clues_primer_nivel to nodes_sources.\n",
      "Loaded a total of 167 points of interest of the source denue_guarderias for analisis.\n",
      "Starting range k = 0 of 0 for source denue_guarderias.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_guarderias.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_guarderias to nodes_sources.\n",
      "Loaded a total of 103 points of interest of the source denue_dif for analisis.\n",
      "Starting range k = 0 of 0 for source denue_dif.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_dif.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_dif to nodes_sources.\n",
      "Loaded a total of 59 points of interest of the source denue_supermercado for analisis.\n",
      "Starting range k = 0 of 0 for source denue_supermercado.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_supermercado.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_supermercado to nodes_sources.\n",
      "Loaded a total of 3957 points of interest of the source denue_abarrotes for analisis.\n",
      "Starting range k = 0 of 19 for source denue_abarrotes.\n",
      "Starting range k = 1 of 19 for source denue_abarrotes.\n",
      "Starting range k = 2 of 19 for source denue_abarrotes.\n",
      "Starting range k = 3 of 19 for source denue_abarrotes.\n",
      "Starting range k = 4 of 19 for source denue_abarrotes.\n",
      "Starting range k = 5 of 19 for source denue_abarrotes.\n",
      "Starting range k = 6 of 19 for source denue_abarrotes.\n",
      "Starting range k = 7 of 19 for source denue_abarrotes.\n",
      "Starting range k = 8 of 19 for source denue_abarrotes.\n",
      "Starting range k = 9 of 19 for source denue_abarrotes.\n",
      "Starting range k = 10 of 19 for source denue_abarrotes.\n",
      "Starting range k = 11 of 19 for source denue_abarrotes.\n",
      "Starting range k = 12 of 19 for source denue_abarrotes.\n",
      "Starting range k = 13 of 19 for source denue_abarrotes.\n",
      "Starting range k = 14 of 19 for source denue_abarrotes.\n",
      "Starting range k = 15 of 19 for source denue_abarrotes.\n",
      "Starting range k = 16 of 19 for source denue_abarrotes.\n",
      "Starting range k = 17 of 19 for source denue_abarrotes.\n",
      "Starting range k = 18 of 19 for source denue_abarrotes.\n",
      "Starting range k = 19 of 19 for source denue_abarrotes.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_abarrotes.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_abarrotes to nodes_sources.\n",
      "Loaded a total of 728 points of interest of the source denue_carnicerias for analisis.\n",
      "Starting range k = 0 of 3 for source denue_carnicerias.\n",
      "Starting range k = 1 of 3 for source denue_carnicerias.\n",
      "Starting range k = 2 of 3 for source denue_carnicerias.\n",
      "Starting range k = 3 of 3 for source denue_carnicerias.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_carnicerias.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_carnicerias to nodes_sources.\n",
      "Loaded a total of 30 points of interest of the source sip_mercado for analisis.\n",
      "Starting range k = 0 of 0 for source sip_mercado.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source sip_mercado.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source sip_mercado to nodes_sources.\n",
      "Loaded a total of 2549 points of interest of the source denue_peluqueria for analisis.\n",
      "Starting range k = 0 of 12 for source denue_peluqueria.\n",
      "Starting range k = 1 of 12 for source denue_peluqueria.\n",
      "Starting range k = 2 of 12 for source denue_peluqueria.\n",
      "Starting range k = 3 of 12 for source denue_peluqueria.\n",
      "Starting range k = 4 of 12 for source denue_peluqueria.\n",
      "Starting range k = 5 of 12 for source denue_peluqueria.\n",
      "Starting range k = 6 of 12 for source denue_peluqueria.\n",
      "Starting range k = 7 of 12 for source denue_peluqueria.\n",
      "Starting range k = 8 of 12 for source denue_peluqueria.\n",
      "Starting range k = 9 of 12 for source denue_peluqueria.\n",
      "Starting range k = 10 of 12 for source denue_peluqueria.\n",
      "Starting range k = 11 of 12 for source denue_peluqueria.\n",
      "Starting range k = 12 of 12 for source denue_peluqueria.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_peluqueria.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_peluqueria to nodes_sources.\n",
      "Loaded a total of 386 points of interest of the source denue_farmacias for analisis.\n",
      "Starting range k = 0 of 1 for source denue_farmacias.\n",
      "Starting range k = 1 of 1 for source denue_farmacias.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_farmacias.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_farmacias to nodes_sources.\n",
      "Loaded a total of 635 points of interest of the source denue_ferreteria_tlapaleria for analisis.\n",
      "Starting range k = 0 of 3 for source denue_ferreteria_tlapaleria.\n",
      "Starting range k = 1 of 3 for source denue_ferreteria_tlapaleria.\n",
      "Starting range k = 2 of 3 for source denue_ferreteria_tlapaleria.\n",
      "Starting range k = 3 of 3 for source denue_ferreteria_tlapaleria.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_ferreteria_tlapaleria.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_ferreteria_tlapaleria to nodes_sources.\n",
      "Loaded a total of 406 points of interest of the source denue_art_limpieza for analisis.\n",
      "Starting range k = 0 of 2 for source denue_art_limpieza.\n",
      "Starting range k = 1 of 2 for source denue_art_limpieza.\n",
      "Starting range k = 2 of 2 for source denue_art_limpieza.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_art_limpieza.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_art_limpieza to nodes_sources.\n",
      "Loaded a total of 2060 points of interest of the source denue_ropa for analisis.\n",
      "Starting range k = 0 of 10 for source denue_ropa.\n",
      "Starting range k = 1 of 10 for source denue_ropa.\n",
      "Starting range k = 2 of 10 for source denue_ropa.\n",
      "Starting range k = 3 of 10 for source denue_ropa.\n",
      "Starting range k = 4 of 10 for source denue_ropa.\n",
      "Starting range k = 5 of 10 for source denue_ropa.\n",
      "Starting range k = 6 of 10 for source denue_ropa.\n",
      "Starting range k = 7 of 10 for source denue_ropa.\n",
      "Starting range k = 8 of 10 for source denue_ropa.\n",
      "Starting range k = 9 of 10 for source denue_ropa.\n",
      "Starting range k = 10 of 10 for source denue_ropa.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_ropa.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_ropa to nodes_sources.\n",
      "Loaded a total of 347 points of interest of the source denue_calzado for analisis.\n",
      "Starting range k = 0 of 1 for source denue_calzado.\n",
      "Starting range k = 1 of 1 for source denue_calzado.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_calzado.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_calzado to nodes_sources.\n",
      "Loaded a total of 418 points of interest of the source denue_muebles for analisis.\n",
      "Starting range k = 0 of 2 for source denue_muebles.\n",
      "Starting range k = 1 of 2 for source denue_muebles.\n",
      "Starting range k = 2 of 2 for source denue_muebles.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_muebles.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_muebles to nodes_sources.\n",
      "Loaded a total of 396 points of interest of the source denue_lavanderia for analisis.\n",
      "Starting range k = 0 of 1 for source denue_lavanderia.\n",
      "Starting range k = 1 of 1 for source denue_lavanderia.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_lavanderia.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_lavanderia to nodes_sources.\n",
      "Loaded a total of 39 points of interest of the source denue_revistas_periodicos for analisis.\n",
      "Starting range k = 0 of 0 for source denue_revistas_periodicos.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_revistas_periodicos.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_revistas_periodicos to nodes_sources.\n",
      "Loaded a total of 155 points of interest of the source denue_pintura for analisis.\n",
      "Starting range k = 0 of 0 for source denue_pintura.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_pintura.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_pintura to nodes_sources.\n",
      "Loaded a total of 4926 points of interest of the source denue_restaurante_insitu for analisis.\n",
      "Starting range k = 0 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 1 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 2 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 3 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 4 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 5 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 6 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 7 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 8 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 9 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 10 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 11 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 12 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 13 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 14 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 15 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 16 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 17 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 18 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 19 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 20 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 21 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 22 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 23 of 24 for source denue_restaurante_insitu.\n",
      "Starting range k = 24 of 24 for source denue_restaurante_insitu.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_restaurante_insitu.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_restaurante_insitu to nodes_sources.\n",
      "Loaded a total of 1055 points of interest of the source denue_restaurante_llevar for analisis.\n",
      "Starting range k = 0 of 5 for source denue_restaurante_llevar.\n",
      "Starting range k = 1 of 5 for source denue_restaurante_llevar.\n",
      "Starting range k = 2 of 5 for source denue_restaurante_llevar.\n",
      "Starting range k = 3 of 5 for source denue_restaurante_llevar.\n",
      "Starting range k = 4 of 5 for source denue_restaurante_llevar.\n",
      "Starting range k = 5 of 5 for source denue_restaurante_llevar.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_restaurante_llevar.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_restaurante_llevar to nodes_sources.\n",
      "Loaded a total of 278 points of interest of the source denue_bares for analisis.\n",
      "Starting range k = 0 of 1 for source denue_bares.\n",
      "Starting range k = 1 of 1 for source denue_bares.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_bares.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_bares to nodes_sources.\n",
      "Loaded a total of 957 points of interest of the source denue_cafe for analisis.\n",
      "Starting range k = 0 of 4 for source denue_cafe.\n",
      "Starting range k = 1 of 4 for source denue_cafe.\n",
      "Starting range k = 2 of 4 for source denue_cafe.\n",
      "Starting range k = 3 of 4 for source denue_cafe.\n",
      "Starting range k = 4 of 4 for source denue_cafe.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_cafe.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_cafe to nodes_sources.\n",
      "Loaded a total of 25 points of interest of the source sip_cancha for analisis.\n",
      "Starting range k = 0 of 0 for source sip_cancha.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source sip_cancha.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source sip_cancha to nodes_sources.\n",
      "Loaded a total of 4 points of interest of the source sip_unidad_deportiva for analisis.\n",
      "Starting range k = 0 of 0 for source sip_unidad_deportiva.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source sip_unidad_deportiva.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source sip_unidad_deportiva to nodes_sources.\n",
      "Loaded a total of 272 points of interest of the source sip_espacio_publico for analisis.\n",
      "Starting range k = 0 of 1 for source sip_espacio_publico.\n",
      "Starting range k = 1 of 1 for source sip_espacio_publico.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source sip_espacio_publico.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source sip_espacio_publico to nodes_sources.\n",
      "Loaded a total of 0 points of interest of the source denue_parque_natural for analisis.\n",
      "0 points of interest of the source denue_parque_natural found, time is NaN.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_parque_natural to nodes_sources.\n",
      "Loaded a total of 9 points of interest of the source denue_cines for analisis.\n",
      "Starting range k = 0 of 0 for source denue_cines.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_cines.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_cines to nodes_sources.\n",
      "Loaded a total of 14 points of interest of the source denue_museos for analisis.\n",
      "Starting range k = 0 of 0 for source denue_museos.\n",
      "Calculated time from nodes to pois for a total of 60233 nodes for source denue_museos.\n",
      "Applying final formating to calculated nodes.\n",
      "Added time for source denue_museos to nodes_sources.\n"
     ]
    }
   ],
   "source": [
    "# Create sources - code dicc out of main parameters dicc\n",
    "sources = {}\n",
    "for eje in parameters.keys():\n",
    "    for amenity in parameters[eje]:\n",
    "        for source in parameters[eje][amenity]:\n",
    "            sources[source] = parameters[eje][amenity][source]\n",
    "\n",
    "# Format\n",
    "pois_distance_node = nearest.copy()\n",
    "edges_gdf['length'].fillna(edges_gdf['length'].mean(),inplace=True)\n",
    "\n",
    "# ELEMENTS NEEDED OUTSIDE THE LOOP - nodes_analysis is a nodes_gdf out of loop used in aup.calculate_distance_nearest_poi in each loop\n",
    "nodes_analysis = nodes_gdf.reset_index().copy()\n",
    "# ELEMENTS NEEDED OUTSIDE THE LOOP - nodes_sources is a nodes_gdf out of loop used to accumulate a final gdf with the minimal distance from each node to each source\n",
    "nodes_sources = gpd.GeoDataFrame()\n",
    "# ELEMENTS NEEDED OUTSIDE THE LOOP - Count\n",
    "i = 0\n",
    "\n",
    "# Loop that calculates distance from each node to each source (source by source)\n",
    "for s in sources:\n",
    "    \n",
    "    # Locate pois data for current source\n",
    "    source_gdf = gpd.GeoDataFrame()\n",
    "    for cod in sources[s]:\n",
    "        source_tmp = pois_distance_node[pois_distance_node['code']==cod]\n",
    "        source_gdf = pd.concat([source_gdf,source_tmp])\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    print(f\"Loaded a total of {len(source_gdf)} points of interest of the source {s} for analisis.\")\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Data for current amenity loop - df_temp: Each column will be a batch of procesed nodes.\n",
    "    df_temp = nodes_gdf.copy()\n",
    "    # Data for current amenity loop - nodes_distance: Minimum time/distance found in all batches will be added from df_min (within if/elif/else) to nodes_distance and finally to nodes_sources (outside loop)\n",
    "    nodes_distance = nodes_gdf.copy()\n",
    "    \n",
    "    # In case there are no amenities of a certain type in the city\n",
    "    if len(source_gdf) == 0:\n",
    "        nodes_time = nodes_distance.copy()\n",
    "        nodes_time['time'] = 0\n",
    "        #-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        print(f\"0 points of interest of the source {s} found, time is NaN.\")\n",
    "        #-------------------------------------------------------------------------------------------------------------------------------------------------       \n",
    "    \n",
    "    # Elif, divide in batches processing (200 if the total number of pois is an exact multiple of 250, 250 otherwise)\n",
    "    elif len(source_gdf) % 250:\n",
    "        batch_size = len(source_gdf)/200\n",
    "        for k in range(int(batch_size)+1):\n",
    "            #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "            print(f\"Starting range k = {k} of {int(batch_size)} for source {s}.\")\n",
    "            #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "            source_process = source_gdf.iloc[int(200*k):int(200*(1+k))].copy()\n",
    "            nodes_distance_prep = aup.calculate_distance_nearest_poi(source_process, nodes_analysis, edges_gdf, s, 'osmid', wght='length')\n",
    "            \n",
    "            #A middle gdf is created whose columns will be the name of the amenity and the batch number it belongs to\n",
    "            df_int = pd.DataFrame()\n",
    "            df_int['dist_'+str(k)+s] = nodes_distance_prep['dist_'+s]\n",
    "            \n",
    "            #The middle gdf is merged into the previously created temporary gdf to store the data by node, each batch in a column.\n",
    "            df_temp = df_temp.merge(df_int, left_index=True, right_index=True)\n",
    "            \n",
    "        # Once finished, drop the non-distance values from the temporary gdf\n",
    "        df_temp.drop(['x', 'y', 'street_count','geometry'], inplace = True, axis=1)\n",
    "        \n",
    "        #We apply the min function to find the minimum value. This value is sent to a new df_min\n",
    "        df_min = pd.DataFrame()\n",
    "        df_min['dist_'+s] = df_temp.min(axis=1)\n",
    "        \n",
    "        #We merge df_min which contains the shortest distance to the POI with nodes_distance which will store all final data\n",
    "        nodes_distance = nodes_distance.merge(df_min, left_index=True, right_index=True)\n",
    "        \n",
    "        #Final data gets converted to time, assuming a walking speed of 4km/hr\n",
    "        nodes_time = nodes_distance.copy()\n",
    "        nodes_time['time'] = (nodes_time['dist_'+s]*60)/4000\n",
    "        \n",
    "        #-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        print(f\"Calculated time from nodes to pois for a total of {len(nodes_distance)} nodes for source {s}.\")\n",
    "        #------------------------------------------------------------------------------------------------------------------------------------------------- \n",
    "\n",
    "    # Else, divide in batches processing (200 if the total number of pois is an exact multiple of 250, 250 otherwise)   \n",
    "    else:\n",
    "        batch_size = len(source_gdf)/250\n",
    "        for k in range(int(batch_size)+1):\n",
    "            #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "            print(f\"Starting range k = {k} of {int(batch_size)} for source {s}.\")\n",
    "            #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "            source_process = source_gdf.iloc[int(250*k):int(250*(1+k))].copy()\n",
    "            nodes_distance_prep = aup.calculate_distance_nearest_poi(source_process, nodes_analysis, edges_gdf, s, 'osmid', wght='length')\n",
    "            \n",
    "            #A middle gdf is created whose columns will be the name of the amenity and the batch number it belongs to\n",
    "            df_int = pd.DataFrame()\n",
    "            df_int['dist_'+str(k)+s] = nodes_distance_prep['dist_'+s]\n",
    "            \n",
    "            #The middle gdf is merged into the previously created temporary gdf to store the data by node, each batch in a column.\n",
    "            df_temp = df_temp.merge(df_int, left_index=True, right_index=True)\n",
    "            \n",
    "        # Once finished, drop the non-distance values from the temporary gdf\n",
    "        df_temp.drop(['x', 'y', 'street_count','geometry'], inplace = True, axis=1)\n",
    "        \n",
    "        #We apply the min function to find the minimum value. This value is sent to a new df_min\n",
    "        df_min = pd.DataFrame()\n",
    "        df_min['dist_'+s] = df_temp.min(axis=1)\n",
    "        \n",
    "        #We merge df_min which contains the shortest distance to the POI with nodes_distance which will store all final data\n",
    "        nodes_distance = nodes_distance.merge(df_min, left_index=True, right_index=True)\n",
    "        \n",
    "        #Final data gets converted to time, assuming a walking speed of 4km/hr\n",
    "        nodes_time = nodes_distance.copy()\n",
    "        nodes_time['time'] = (nodes_time['dist_'+s]*60)/4000\n",
    "    \n",
    "        #-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        print(f\"Calculated time from nodes to pois for a total of {len(nodes_distance)} nodes for source {s}.\")\n",
    "        #-------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    print(\"Applying final formating to calculated nodes.\")\n",
    "    #----------------------------------------------------------------------------------------------------------------------------------------------------- \n",
    "    \n",
    "    #Format nodes_distance\n",
    "    nodes_time['source'] = s\n",
    "    nodes_time['city'] = city\n",
    "    nodes_time.reset_index(inplace=True)\n",
    "    nodes_time = nodes_time.set_crs(\"EPSG:4326\")\n",
    "    nodes_time = nodes_time[['osmid','time','source','city','x','y','geometry']]\n",
    "    \n",
    "    #If it is the first round nodes_sources is created equal to nodes_distance (all nodes, one source)\n",
    "    #If it is the second or more, the new nodes_distance is merged.\n",
    "    #This way we obtain the final gdf of interest that will contain the minimum disstance to each type of amenity by column.\n",
    "    if i == 0:\n",
    "        nodes_sources = nodes_time.copy()\n",
    "    else:\n",
    "        nodes_sources = pd.concat([nodes_sources,nodes_time])\n",
    "        \n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    print(f\"Added time for source {s} to nodes_sources.\")\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "# Saves space in disk if instructed\n",
    "if save_space:\n",
    "    del nearest\n",
    "    del nodes_gdf\n",
    "    del nodes_analysis\n",
    "    del source_tmp\n",
    "    del source_gdf\n",
    "    del df_temp\n",
    "    del nodes_distance\n",
    "    del nodes_time\n",
    "    del source_process\n",
    "    del nodes_distance_prep\n",
    "    del df_int\n",
    "    del df_min\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    print(\"Saved space by deleting used data.\")\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "262d4afc-b15f-4938-a796-c4ff47435e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if local_save:\n",
    "    nodes_sources.to_file(nodesproximity_save_dir, driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c687ab63-6049-40b8-8491-e6c5d794798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show\n",
    "print(nodes_sources.shape)\n",
    "nodes_sources.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d70338-f735-4bde-829b-d4cbd1e011cb",
   "metadata": {},
   "source": [
    "## Step 4 - Analize distance data by nodes to proximity in hexagons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194efe1a-f301-490b-b809-21a84be4e37d",
   "metadata": {},
   "source": [
    "### Step 4 - Code - Pop Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6d441c6-0b0a-43f0-b62e-d39691101193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_popdata_hexgrid(aoi,pop_dir,pop_column,pop_index_column,res_list):\n",
    "    \n",
    "    pop_gdf = gpd.read_file(pop_dir)\n",
    "    \n",
    "    # Format and isolate data of interest\n",
    "    pop_gdf = pop_gdf.to_crs(\"EPSG:4326\")\n",
    "    pop_gdf.columns = pop_gdf.columns.str.lower()\n",
    "    block_pop = pop_gdf[[pop_index_column,pop_column,'geometry']]\n",
    "\n",
    "    # Extract point from polygon\n",
    "    block_pop = block_pop.to_crs(\"EPSG:6372\")\n",
    "    block_pop = block_pop.set_index(pop_index_column)\n",
    "    point_within_polygon = gpd.GeoDataFrame(geometry=block_pop.representative_point())\n",
    "\n",
    "    # Add census data to points\n",
    "    centroid_block_pop = point_within_polygon.merge(block_pop, right_index=True, left_index=True) \n",
    "\n",
    "    # Format centroid with pop data\n",
    "    centroid_block_pop.drop(columns=['geometry_y'], inplace=True)\n",
    "    centroid_block_pop.rename(columns={'geometry_x':'geometry'}, inplace=True)\n",
    "    centroid_block_pop = gpd.GeoDataFrame(centroid_block_pop, geometry='geometry')\n",
    "    centroid_block_pop = centroid_block_pop.to_crs(\"EPSG:4326\")\n",
    "    centroid_block_pop = centroid_block_pop.reset_index()\n",
    "    centroid_block_pop.rename(columns={pop_column:'pobtot'},inplace=True)\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    print(f\"Converted to centroids with {centroid_block_pop.pobtot.sum()} \" + f\"pop vs {block_pop.pobtot.sum()} pop in original gdf.\")\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # create buffer for aoi to include outer blocks when creating hexgrid\n",
    "    aoi_buffer = aoi.copy()\n",
    "    aoi_buffer = aoi_buffer.dissolve()\n",
    "    aoi_buffer = aoi_buffer.to_crs(\"EPSG:6372\").buffer(2500)\n",
    "    aoi_buffer = gpd.GeoDataFrame(geometry=aoi_buffer)\n",
    "    aoi_buffer = aoi_buffer.to_crs(\"EPSG:4326\")\n",
    "\n",
    "    hex_socio_gdf = gpd.GeoDataFrame()\n",
    "\n",
    "    for res in res_list:\n",
    "        # Generate hexagon gdf\n",
    "        hex_gdf = aup.create_hexgrid(aoi_buffer, res)\n",
    "        hex_gdf = hex_gdf.set_crs(\"EPSG:4326\")\n",
    "\n",
    "        # Format - Remove res from index name and add column with res\n",
    "        hex_gdf.rename(columns={f'hex_id_{res}':'hex_id'},inplace=True)\n",
    "        hex_gdf['res'] = res\n",
    "        #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        print(f\"Created hex_grid with {res} resolution\")\n",
    "        #-----------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "        # Group pop data\n",
    "        # def socio_points_to_polygon(gdf_polygon,gdf_socio,cve_column,string_columns,wgt_dict=None,avg_column=None):\n",
    "        # Agruments:\n",
    "            #gdf_polygon (geopandas.GeoDataFrame): GeoDataFrame polygon where sociodemographic data will be grouped\n",
    "            #gdf_socio (geopandas.GeoDataFrame): GeoDataFrame points with sociodemographic data\n",
    "            #cve_column (str): Column name with polygon id in gdf_polygon.\n",
    "            #string_columns (list): List with column names for string data in gdf_socio.\n",
    "            #wgt_dict {dict, optional): Dictionary with average column names and weight column names for weighted average. Defaults to None.\n",
    "            #avg_column (list, optional): List with column names with average data. Defaults to None.\n",
    "\n",
    "        string_columns = [pop_index_column]\n",
    "        hex_socio_df = aup.socio_points_to_polygon(hex_gdf, centroid_block_pop,'hex_id', string_columns)\n",
    "\n",
    "        #-----------------------------------------------------------------------------------------------------------------------------------------------------     \n",
    "        print(f\"Agregated socio data to hex with a total of {hex_socio_df.pobtot.sum()} population for resolution {res}.\")\n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------------------- \n",
    "\n",
    "        # Hexagons to GeoDataFrame\n",
    "        hex_socio_gdf_tmp = hex_gdf.merge(hex_socio_df, on='hex_id')\n",
    "\n",
    "        hectares = hex_socio_gdf_tmp.to_crs(\"EPSG:6372\").area / 10000\n",
    "        hex_socio_gdf_tmp['dens_pob_ha'] = hex_socio_gdf_tmp['pobtot'] / hectares\n",
    "\n",
    "        #-----------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "        print(f\"Calculated an average density of {hex_socio_gdf_tmp.dens_pob_ha.mean()}\")\n",
    "        #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        hex_socio_gdf = pd.concat([hex_socio_gdf,hex_socio_gdf_tmp])    \n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    print(f\"Finished calculating population by hexgrid for res {res_list}.\")\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return hex_socio_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f214da1e-b554-4c31-85cf-fe14d987b829",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to centroids with 1043905 pop vs 1043905 pop in original gdf.\n",
      "Created hex_grid with 8 resolution\n",
      "Agregated socio data to hex with a total of 1043905.0 population for resolution 8.\n",
      "Calculated an average density of 39.75306755993063\n",
      "Created hex_grid with 9 resolution\n",
      "Agregated socio data to hex with a total of 1043905.0 population for resolution 9.\n",
      "Calculated an average density of 57.5081759708286\n",
      "Finished calculating population by hexgrid for res [8, 9].\n"
     ]
    }
   ],
   "source": [
    "if pop_output:\n",
    "    hex_socio_gdf = create_popdata_hexgrid(aoi,pop_dir,pop_column,pop_index_column,res_list)\n",
    "    if local_save:\n",
    "        hex_socio_gdf.to_file(pop_save_dir, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51378d1a-38bd-474f-b029-5465af2a1aa8",
   "metadata": {},
   "source": [
    "### Step 4 - Code - Transform nodes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02515bf3-c366-474f-9354-10cd6417dea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed nodes data.\n"
     ]
    }
   ],
   "source": [
    "nodes = nodes_sources.copy()\n",
    "\n",
    "#--------------- PREPARE DATA\n",
    "#--------------- PREPARE DATA ---------- DELETE DUPLICATES AND CLEAN NODES\n",
    "#This step keeps osmid, geometry and metropolis (without duplicates, keeping only one point for each node) to store times to each amenity source by node in following loop.\n",
    "nodes_geom = nodes.drop_duplicates(subset='osmid', keep=\"last\")[['osmid','geometry','city']].copy()\n",
    "\n",
    "#--------------- PREPARE DATA ---------- REORGANIZE NODES DATA\n",
    "#This step organizes data by nodes by changing (time to source amenities) from rows (1 column with source amenity name + 1 column with time data) \n",
    "#to columns (1 column with time data named after its source amenity)\n",
    "nodes_analysis = nodes_geom.copy()\n",
    "\n",
    "for source_amenity in list(nodes.source.unique()):\n",
    "    nodes_tmp = nodes.loc[nodes.source == source_amenity,['osmid','time']]\n",
    "    nodes_tmp = nodes_tmp.rename(columns={'time':source_amenity})\n",
    "    # Search for amenities that aren't present in the city (with all values marked as 0) and change them to NaN\n",
    "    if nodes_tmp[source_amenity].mean() == 0:\n",
    "        nodes_tmp[source_amenity] = np.nan\n",
    "    nodes_analysis = nodes_analysis.merge(nodes_tmp, on='osmid')\n",
    "\n",
    "if save_space:\n",
    "    del nodes\n",
    "    del nodes_sources\n",
    "    del nodes_geom\n",
    "    del nodes_tmp\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    print(\"Saved space by deleting used data.\")\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "print(\"Transformed nodes data.\")\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c35c5bd-2198-43fa-97bf-7b2ee18cb5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show\n",
    "print(nodes_analysis.shape)\n",
    "nodes_analysis.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106f530-d24e-4288-b987-c4724bb2121d",
   "metadata": {},
   "source": [
    "### Step 4 - Code - Create definitions (previously idx_15min) dicc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16f7e2fb-50e6-40ca-8021-89012b0bae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create definitions dicc out of main parameters dicc:\n",
    "definitions = {}\n",
    "for eje in parameters.keys():\n",
    "    # Temporary dicc stores amenity:[source_list] for each eje\n",
    "    tmp_dicc = {}\n",
    "    \n",
    "    for amenity in parameters[eje]:\n",
    "        items_lst = []\n",
    "        items = list(parameters[eje][amenity].items())\n",
    "        \n",
    "        for item in items:\n",
    "            items_lst.append(item[0])\n",
    "            \n",
    "        tmp_dicc[amenity] = items_lst\n",
    "     \n",
    "    # Each eje gets assigned its own tmp_dicc\n",
    "    definitions[eje] = tmp_dicc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64d955-1c54-4f76-a852-8ce023e8d62b",
   "metadata": {},
   "source": [
    "### Step 4 - Code - Fill for missing amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00693d26-bbdb-4f1e-9f32-8ccefee9ab80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished missing source amenities analysis. 0 not present source amenities were added as np.nan columns\n"
     ]
    }
   ],
   "source": [
    "#--------------- PREPARE DATA ---------- FILL MISSING COLUMNS (In case there is a source amenity not available in a city)\n",
    "all_sources = []\n",
    "\n",
    "# Gather all possible sources\n",
    "for eje in definitions.keys():\n",
    "    for amenity in definitions[eje].values():\n",
    "        for source in amenity:\n",
    "            all_sources.append(source)\n",
    "            \n",
    "# If source not in currently analized city, fill column with np.nan\n",
    "column_list = list(nodes_analysis.columns)\n",
    "missing_sourceamenities = []\n",
    "\n",
    "for s in all_sources:\n",
    "        if s not in column_list:\n",
    "            nodes_analysis[s] = np.nan\n",
    "            #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "            print(f\"{s} source amenity is not present in {city}.\")\n",
    "            #---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "            missing_sourceamenities.append(s)\n",
    "            \n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "print(f\"Finished missing source amenities analysis. {len(missing_sourceamenities)} not present source amenities were added as np.nan columns\")\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070d56b6-ec15-4942-aeed-9b3b2c4aa546",
   "metadata": {},
   "source": [
    "### Step 4 - Code - Amenities analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "caacd424-d67c-4450-ac75-e26a694f6c74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting proximity to amenities analysis by node.\n",
      "Calculated proximity to amenities data by node.\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "print(\"Starting proximity to amenities analysis by node.\")\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#--------------- PROCESS DATA \n",
    "#--------------- PROCESS DATA ---------- Max time calculation\n",
    "#This step calculates times by amenity\n",
    "\n",
    "column_max_all = [] # list with all max index column names\n",
    "column_max_ejes = [] # list with ejes index column names\n",
    "\n",
    "#Goes through each eje in dictionary:\n",
    "for e in definitions.keys():\n",
    "\n",
    "    #Appends to 3 lists currently examined eje\n",
    "    column_max_all.append('max_'+ e.lower())\n",
    "    column_max_ejes.append('max_'+ e.lower())\n",
    "    column_max_amenities = [] # list with amenities in current eje\n",
    "\n",
    "    #Goes through each amenity of current eje:\n",
    "    for a in definitions[e].keys():\n",
    "\n",
    "        #Appends to 2 lists currently examined amenity:\n",
    "        column_max_all.append('max_'+ a.lower())\n",
    "        column_max_amenities.append('max_'+ a.lower())\n",
    "\n",
    "        #Calculates time to currently examined amenity:\n",
    "        #If weight is less than number of sources of amenity, choose minimum time to sources.\n",
    "        if source_weight[e][a] == 'min': \n",
    "            nodes_analysis['max_'+ a.lower()] = nodes_analysis[definitions[e][a]].min(axis=1)\n",
    "        #Else, choose maximum time to sources.\n",
    "        else:\n",
    "            nodes_analysis['max_'+ a.lower()] = nodes_analysis[definitions[e][a]].max(axis=1)\n",
    "\n",
    "    #Calculates time to currently examined eje (max time of its amenities):\n",
    "    nodes_analysis['max_'+ e.lower()] = nodes_analysis[column_max_amenities].max(axis=1) \n",
    "\n",
    "index_column = 'max_time' # column name for maximum time data\n",
    "\n",
    "#Add to column_max_all list the attribute 'max_time'\n",
    "column_max_all.append(index_column)\n",
    "\n",
    "#Assigns \"max_time\" the max time for all ejes\n",
    "nodes_analysis[index_column] = nodes_analysis[column_max_ejes].max(axis=1)     \n",
    "\n",
    "#Add to column_max_all list the attributes 'osmid' and 'geometry' to filter nodes_analysis with the column_max_all list.\n",
    "column_max_all.append('osmid')\n",
    "column_max_all.append('geometry')\n",
    "nodes_analysis_filter = nodes_analysis[column_max_all].copy()\n",
    "\n",
    "if save_space:\n",
    "    del nodes_analysis\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "print(\"Calculated proximity to amenities data by node.\")\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46877181-5cd4-4c77-9d16-c0cee63a02d8",
   "metadata": {},
   "source": [
    "### Step 4 - Code - Group by hex (mean data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96ef9f64-4fcd-4b7f-9e3c-d985cfce031a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pop hexgrid of resolution 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15373/1860612382.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hex_pop.rename(columns={'hex_id':f'hex_id_{res}'},inplace=True)\n",
      "/tmp/ipykernel_15373/1860612382.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hex_pop.rename(columns={'hex_id':f'hex_id_{res}'},inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped nodes data by hexagons res 8\n",
      "Saved grouped data by hexagons res 8\n",
      "Loaded pop hexgrid of resolution 9\n",
      "Grouped nodes data by hexagons res 9\n",
      "Saved grouped data by hexagons res 9\n"
     ]
    }
   ],
   "source": [
    "hex_idx = gpd.GeoDataFrame()\n",
    "\n",
    "for res in res_list:\n",
    "    \n",
    "    #/////////////////////////////////////////////////////////////////////////// HEXGRID DEPENDS ON POP DATA BEING CALCULATED OR NOT ///////////////////////////////////////////////////////////////////////////\n",
    "    # If pop_output is true, loads previously created hexgrid with pop data\n",
    "    if pop_output:\n",
    "        # Load hexgrid\n",
    "        hex_pop = hex_socio_gdf.loc[hex_socio_gdf['res'] == res]\n",
    "        \n",
    "        #Function group_by_hex_mean requires ID to include resolution\n",
    "        hex_pop.rename(columns={'hex_id':f'hex_id_{res}'},inplace=True)\n",
    "        \n",
    "        # Create hex_tmp\n",
    "        hex_pop = hex_pop.set_crs(\"EPSG:4326\")\n",
    "        hex_tmp = hex_pop[[f'hex_id_{res}','geometry']].copy()\n",
    "        #-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        print(f\"Loaded pop hexgrid of resolution {res}\")\n",
    "        #-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    # If pop_output is false, creates hexgrid\n",
    "    else:\n",
    "        # Create hexgrid (which already has ID_res)\n",
    "        hexgrid = aup.create_hexgrid(aoi,res)\n",
    "        \n",
    "        # Create hex_tmp\n",
    "        hexgrid = hexgrid.set_crs(\"EPSG:4326\")\n",
    "        hex_tmp = hexgrid.copy()\n",
    "        #-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        print(f\"Created hexgrid of resolution {res}\")\n",
    "        #-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    #/////////////////////////////////////////////////////////////////////////// HEXGRID DEPENDS ON POP DATA BEING CALCULATED OR NOT ///////////////////////////////////////////////////////////////////////////\n",
    "    \n",
    "    # group data by hex\n",
    "    hex_res_idx = aup.group_by_hex_mean(nodes_analysis_filter, hex_tmp, res, index_column)\n",
    "    hex_res_idx = hex_res_idx.loc[hex_res_idx[index_column]>0].copy()\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    print(f\"Grouped nodes data by hexagons res {res}\")\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    #////////////////////////////////////////////////////////////////////////////////// ADD POP DATA IF POP DATA IS CONSIDERED /////////////////////////////////////////////////////////////////////////////////\n",
    "    # Add pop data\n",
    "    if pop_output:\n",
    "        pop_list = [f'hex_id_{res}','pobtot','dens_pob_ha']\n",
    "        hex_res_pop = pd.merge(hex_res_idx, hex_pop[pop_list], on=f'hex_id_{res}')\n",
    "    else:\n",
    "        hex_res_pop = hex_res_idx.copy()\n",
    "    #////////////////////////////////////////////////////////////////////////////////// ADD POP DATA IF POP DATA IS CONSIDERED /////////////////////////////////////////////////////////////////////////////////    \n",
    "    \n",
    "    # After funtion group_by_hex_mean we can remove res from ID and set as a column\n",
    "    hex_res_pop.rename(columns={f'hex_id_{res}':'hex_id'},inplace=True)\n",
    "    hex_res_pop['res'] = res\n",
    "\n",
    "    hex_idx = hex_idx.append(hex_res_pop)\n",
    "        \n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "    print(f\"Saved grouped data by hexagons res {res}\")\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "if save_space:\n",
    "    if pop_output:\n",
    "        del hex_socio_gdf #pop_output=True\n",
    "        del hex_pop #pop_output=True\n",
    "        del hex_tmp\n",
    "        del nodes_analysis_filter\n",
    "        del hex_res_idx\n",
    "        del hex_res_pop #pop_output=True\n",
    "    else:\n",
    "        del hexgrid #pop_output=False\n",
    "        del hex_tmp\n",
    "        del nodes_analysis_filter\n",
    "        del hex_res_idx\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    print(\"Saved space by deleting used data.\")\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeea7d8-21ad-4804-9ffb-58eb72fffb6e",
   "metadata": {},
   "source": [
    "### Step 4 - Code - Relculate ejes max times by hexagon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e097688-1269-4859-864a-7fff68a3c97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished recalculating times in hexagons\n"
     ]
    }
   ],
   "source": [
    "#--------------- PROCESS DATA ---------- RE-CALCULATE MAX TIMES BY HEXAGON (ALL)\n",
    "# This step recalculates max time to each eje from max times to calculated amenities and max_time from max eje\n",
    "column_max_ejes = [] # list with ejes index column names\n",
    "\n",
    "#Goes (again) through each eje in dictionary:\n",
    "for e in definitions.keys():\n",
    "\n",
    "    column_max_ejes.append('max_'+ e.lower())\n",
    "    column_max_amenities = [] # list with amenities in current eje\n",
    "\n",
    "    #Goes (again) through each amenity of current eje:    \n",
    "    for a in definitions[e].keys():\n",
    "\n",
    "        column_max_amenities.append('max_'+ a.lower())\n",
    "\n",
    "    #Re-calculates time to currently examined eje (max time of its amenities):        \n",
    "    hex_idx['max_'+ e.lower()] = hex_idx[column_max_amenities].max(axis=1)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "print('Finished recalculating times in hexagons')\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed4f409-6466-47fc-a95e-043c22d0f1aa",
   "metadata": {},
   "source": [
    "### Step 4 - Code - Calculate index and add aditional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b496bb68-4e8a-4e64-a229-ca6b26c3a726",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished calculating index, mean and median time\n"
     ]
    }
   ],
   "source": [
    "#--------------- PROCESS DATA ---------- INDEX, MEDIAN AND MEAN CALCULATION (ALL)\n",
    "\n",
    "#Define function\n",
    "def apply_sigmoidal(x):\n",
    "    if x == -1:\n",
    "        return -1\n",
    "    elif x > 1000:\n",
    "        return 0\n",
    "    else:\n",
    "        val = aup.sigmoidal_function(0.1464814753435666, x, 30)\n",
    "        return val\n",
    "\n",
    "#Apply function to amenities columns without ejes ---------------------------------------------///// En lugar de pasarle la lista de amenidades, se crea la lista de amenidades desde column_max_all\n",
    "max_amenities_cols = [i for i in column_max_all if i not in column_max_ejes]\n",
    "max_amenities_cols.remove('max_time')\n",
    "max_amenities_cols.remove('osmid')\n",
    "max_amenities_cols.remove('geometry')\n",
    "\n",
    "idx_amenities_cols = [] # list with idx amenity column names\n",
    "for ac in max_amenities_cols:\n",
    "    idx_col = ac.replace('max','idx')\n",
    "    hex_idx[idx_col] = hex_idx[ac].apply(apply_sigmoidal)\n",
    "    idx_amenities_cols.append(idx_col)\n",
    "\n",
    "# Add final data\n",
    "hex_idx[index_column] = hex_idx[column_max_ejes].max(axis=1)\n",
    "hex_idx['mean_time'] = hex_idx[max_amenities_cols].mean(axis=1)\n",
    "hex_idx['median_time'] = hex_idx[max_amenities_cols].median(axis=1)\n",
    "hex_idx['idx_sum'] = hex_idx[idx_amenities_cols].sum(axis=1)\n",
    "hex_idx['city'] = city\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "print('Finished calculating index, mean and median time')\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001a6e1-aee0-4459-8fba-7a8f211432f4",
   "metadata": {},
   "source": [
    "### Step 4 - Code - Final format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd82f9ad-3a0a-42ea-bd19-cb59f6730f99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished final format\n"
     ]
    }
   ],
   "source": [
    "#--------------- FINAL FORMAT ----------\n",
    "#--------------- FINAL FORMAT ---------- REORDER COLUMNS\n",
    "\n",
    "# First elements of ordered list - ID and geometry\n",
    "first_elements = ['hex_id','res','geometry']\n",
    "\n",
    "# Second elements of ordered list - max_ejes and max_amenities removing max_time, osmid and geometry.\n",
    "column_max_ejes_amenities = column_max_all.copy()\n",
    "column_max_ejes_amenities.remove('max_time')\n",
    "column_max_ejes_amenities.remove('osmid')\n",
    "column_max_ejes_amenities.remove('geometry')\n",
    "\n",
    "# Third elements of ordered list are listed in idx_amenities_cols\n",
    "\n",
    "# Fourth elements of ordered list - Mean, median, max and idx\n",
    "fourth_elements = ['mean_time', 'median_time', 'max_time', 'idx_sum']\n",
    "\n",
    "# Fifth elements - If pop is calculated - Pop data\n",
    "fifth_elements = ['pobtot', 'dens_pob_ha']\n",
    "\n",
    "# Last element - City data\n",
    "last_element = ['city']\n",
    "\n",
    "if pop_output:\n",
    "    final_column_ordered_list = first_elements + column_max_ejes_amenities + idx_amenities_cols + fourth_elements + fifth_elements + last_element\n",
    "else:\n",
    "    final_column_ordered_list = first_elements + column_max_ejes_amenities + idx_amenities_cols + fourth_elements + last_element\n",
    "    \n",
    "hex_idx_city = hex_idx[final_column_ordered_list]\n",
    "\n",
    "if save_space:\n",
    "    del hex_idx\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "print('Finished final format')\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3014da94-129d-4cad-b693-b0418420c085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if local_save:\n",
    "    hex_idx_city.to_file(proximityanalysis_save_dir, driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e1c42-e9d6-4187-9705-58b17124d66c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDS-10.0",
   "language": "python",
   "name": "gds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
