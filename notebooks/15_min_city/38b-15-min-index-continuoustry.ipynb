{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60aaac8",
   "metadata": {},
   "source": [
    "# Ciudades de 15 minutos - resolución 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae7cf7-c7f7-4aac-af04-8f6b3e5e0503",
   "metadata": {},
   "source": [
    "Notebook para la prueba de la metodología de ciudades de 15 minutos. El acercamiento del Observatorio de Ciudades se basa en agrupar las amenidades en 4 ejes:\n",
    "* Educación\n",
    "* Servicios comunitarios\n",
    "* Comercio\n",
    "* Entretenimiento\n",
    "\n",
    "Cada uno de estos ejes se subdivide en amenidades específicas:\n",
    "\n",
    "* Educación\n",
    "    * Preescolar\n",
    "    * Primaria\n",
    "    * Secundaria\n",
    "* Servicios comunitarios:\n",
    "    * Centro de salud - lo traducimos como primer contacto a salud (que incluye farmacias con médicos)\n",
    "    * Gobierno - oficinas de gobierno\n",
    "    * Asistencia social - DIF\n",
    "    * Cuidados - Guarderías\n",
    "* Comercio:\n",
    "    * Alimentos - sitios para la adquisición de alimentos\n",
    "    * Comercio personal - peluquerías y venta de ropa\n",
    "    * Farmacias\n",
    "    * Hogar - Ferretería y tlapalería y artículos de limpieza\n",
    "    * Complementario - sitios de comercio complementario como venta de ropa, calzado, muebles, lavandería, pintura y revistas\n",
    "* Entretenimiento\n",
    "    * Actividad física - espacios de recreación al aire libre como parques, canchas, unidades deportivas o parques naturales\n",
    "    * Social - sitios de esparcimiento social como restaurantes, bares y cafés\n",
    "    * Cultural - espacios de recreación cultural como museos o cines\n",
    "\n",
    "Para calcular si un hexágono cumple o no con lo neceasrio para ser ciudad de 15 minutos se toma el tiempo máximo a una de las amenidades y esa se registra en el hexágono, si ese tiempo es menor a 15, se considera que cumple, de lo contrario no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41393dfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/lib/python3.10/site-packages/osmnx/utils.py:192: UserWarning: The `utils.config` function is deprecated and will be removed in a future release. Instead, use the `settings` module directly to configure a global setting's value. For example, `ox.settings.log_console=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    import aup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba12f761-cb55-43fb-bffb-3e9de9d3fdfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(city, save=False, save_disk_space = True):\n",
    "    \n",
    "    print('STARTING ANALYSIS FOR {}'.format(city))\n",
    "    \n",
    "    #--------------- DOWNLOAD DATA\n",
    "    #Download hexagons with pop data (Based on cvegeo)\n",
    "    hex_pop = gpd.GeoDataFrame()\n",
    "    hex_schema = 'censo'\n",
    "    hex_table = 'hex_censo_mza_2020_res9'\n",
    "    \n",
    "    query = f\"SELECT * FROM {hex_schema}.{hex_table} WHERE \\\"metropolis\\\" LIKE \\'{city}\\'\"\n",
    "    hex_pop = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "    \n",
    "    pob_tot = hex_pop.pobtot.sum()\n",
    "    print('Downloaded hex data with a total of {} persons'.format(pob_tot))\n",
    "        \n",
    "    #Download nodes (Based on city)\n",
    "    nodes_schema = 'prox_analysis'\n",
    "    nodes_table = 'nodes_proximity_2020'\n",
    "    \n",
    "    query = f\"SELECT * FROM {nodes_schema}.{nodes_table} WHERE \\\"metropolis\\\" LIKE \\'{city}\\'\"\n",
    "    nodes = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "    \n",
    "    print('Downloaded a total of {} nodes'.format(nodes.shape[0]))\n",
    "    \n",
    "    #--------------- PREPARE DATA\n",
    "    #--------------- PREPARE DATA ---------- DELETE DUPLICATES AND CLEAN NODES\n",
    "    #This step keeps osmid, geometry and metropolis (without duplicates, keeping only one point for each node) to store times to each amenity source by node in following loop.\n",
    "    nodes_geom = nodes.drop_duplicates(subset='osmid', keep=\"last\")[['osmid','geometry','metropolis']].copy()\n",
    "    \n",
    "    #--------------- PREPARE DATA ---------- REORGANIZE NODES DATA\n",
    "    #This step organizes data by nodes by changing (time to source amenities) from rows (1 column with source amenity name + 1 column with time data) \n",
    "    #to columns (1 column with time data named after its source amenity)\n",
    "    nodes_analysis = nodes_geom.copy()\n",
    "\n",
    "    for source_amenity in list(nodes.amenity.unique()):\n",
    "        nodes_tmp = nodes.loc[nodes.amenity == source_amenity,['osmid','time']]\n",
    "        nodes_tmp = nodes_tmp.rename(columns={'time':source_amenity})\n",
    "        # Search for amenities that aren't present in the city (with all values marked as 0) and change them to NaN\n",
    "        if nodes_tmp[source_amenity].mean() == 0:\n",
    "            nodes_tmp[source_amenity] = np.nan\n",
    "        nodes_analysis = nodes_analysis.merge(nodes_tmp, on='osmid')\n",
    "\n",
    "    if save_disk_space:\n",
    "        del nodes_geom\n",
    "        del nodes_tmp\n",
    "        \n",
    "    print(\"Transformed nodes data\")\n",
    "        \n",
    "    #--------------- PREPARE DATA ---------- SET PARAMETER DEFINITIONS\n",
    "    #This step sets the ejes, amenidades, sources and weights for further analysis\n",
    "    #{Eje (e):\n",
    "    #         {Amenidad (a):\n",
    "    #                       {Source (s))}}}\n",
    "\n",
    "    idx_15_min = {'Escuelas':{'Preescolar':['denue_preescolar'],\n",
    "                             'Primaria':['denue_primaria'],\n",
    "                             'Secundaria':['denue_secundaria']},\n",
    "                 'Servicios comunitarios':{'Salud':['clues_primer_nivel'],\n",
    "                                           'Guarderías':['denue_guarderias'],\n",
    "                                           'Asistencia social':['denue_dif']},\n",
    "                  'Comercio':{'Alimentos':['denue_supermercado','denue_abarrotes',\n",
    "                                           'denue_carnicerias','sip_mercado'],\n",
    "                              'Personal':['denue_peluqueria'],\n",
    "                              'Farmacias':['denue_farmacias'],\n",
    "                              'Hogar':['denue_ferreteria_tlapaleria','denue_art_limpieza'],\n",
    "                              'Complementarios':['denue_ropa','denue_calzado','denue_muebles',\n",
    "                                                 'denue_lavanderia','denue_revistas_periodicos',\n",
    "                                                 'denue_pintura']},\n",
    "                  'Entretenimiento':{'Social':['denue_restaurante_insitu','denue_restaurante_llevar',\n",
    "                                               'denue_bares','denue_cafe'],\n",
    "                                    'Actividad física':['sip_cancha','sip_unidad_deportiva',\n",
    "                                                        'sip_espacio_publico','denue_parque_natural'],\n",
    "                                    'Cultural':['denue_cines','denue_museos']} \n",
    "                 }\n",
    "\n",
    "    #If weight of amenity is less than number of sources, the algorith chooses the minimum time to source. Else (if equall or greater), chooses max time.\n",
    "    wegiht_idx = {'Escuelas':{'Preescolar':1,\n",
    "                            'Primaria':1,\n",
    "                            'Secundaria':1},\n",
    "                'Servicios comunitarios':{'Salud':1,\n",
    "                                        'Guarderías':1,\n",
    "                                        'Asistencia social':1},\n",
    "                'Comercio':{'Alimentos':1,\n",
    "                            'Personal':1,\n",
    "                            'Farmacias':1,\n",
    "                            'Hogar':1,\n",
    "                            'Complementarios':1},\n",
    "                'Entretenimiento':{'Social':4,\n",
    "                                    'Actividad física':1,\n",
    "                                    'Cultural':1}\n",
    "                }\n",
    "    \n",
    "    #--------------- PREPARE DATA ---------- FILL MISSING COLUMNS (In case there is a source amenity not available in a city)\n",
    "    sources = []\n",
    "\n",
    "    # Gather all possible sources\n",
    "    for eje in idx_15_min.keys():\n",
    "        for amenity in idx_15_min[eje].values():\n",
    "            for source in amenity:\n",
    "                sources.append(source)\n",
    "\n",
    "    # If source not in currently analized city, fill column with np.nan\n",
    "    column_list = list(nodes_analysis.columns)\n",
    "    missing_sourceamenities = []\n",
    "\n",
    "    for s in sources:\n",
    "            if s not in column_list:\n",
    "                nodes_analysis[s] = np.nan\n",
    "                missing_sourceamenities.append(s)\n",
    "\n",
    "    print(\"There are {} non present source amenities in {}\".format(len(missing_sourceamenities),city))\n",
    "    \n",
    "    #--------------- PROCESS DATA \n",
    "    #--------------- PROCESS DATA ---------- Max time calculation\n",
    "    #This step calculates times by amenity\n",
    "\n",
    "    column_max_all = [] # list with all max index column names\n",
    "    column_max_ejes = [] # list with ejes index column names\n",
    "\n",
    "    #Goes through each eje in dictionary:\n",
    "    for e in idx_15_min.keys():\n",
    "\n",
    "        #Appends to 3 lists currently examined eje\n",
    "        column_max_all.append('max_'+ e.lower())\n",
    "        column_max_ejes.append('max_'+ e.lower())\n",
    "        column_max_amenities = [] # list with amenities in current eje\n",
    "\n",
    "        #Goes through each amenity of current eje:\n",
    "        for a in idx_15_min[e].keys():\n",
    "\n",
    "            #Appends to 2 lists currently examined amenity:\n",
    "            column_max_all.append('max_'+ a.lower())\n",
    "            column_max_amenities.append('max_'+ a.lower())\n",
    "\n",
    "            #Calculates time to currently examined amenity:\n",
    "            #If weight is less than number of sources of amenity, choose minimum time to sources.\n",
    "            if wegiht_idx[e][a] < len(idx_15_min[e][a]): \n",
    "                nodes_analysis['max_'+ a.lower()] = nodes_analysis[idx_15_min[e][a]].min(axis=1)\n",
    "            #Else, choose maximum time to sources.\n",
    "            else:\n",
    "                nodes_analysis['max_'+ a.lower()] = nodes_analysis[idx_15_min[e][a]].max(axis=1)\n",
    "\n",
    "        #Calculates time to currently examined eje (max time of its amenities):\n",
    "        nodes_analysis['max_'+ e.lower()] = nodes_analysis[column_max_amenities].max(axis=1) \n",
    "\n",
    "    index_column = 'max_time' # column name for maximum time data\n",
    "\n",
    "    #Add to column_max_all list the attribute 'max_time'\n",
    "    column_max_all.append(index_column)\n",
    "\n",
    "    #Assigns \"max_time\" the max time for all ejes\n",
    "    nodes_analysis[index_column] = nodes_analysis[column_max_ejes].max(axis=1)     \n",
    "    \n",
    "    #Add to column_max_all list the attributes 'osmid' and 'geometry' to filter nodes_analysis with the column_max_all list.\n",
    "    column_max_all.append('osmid')\n",
    "    column_max_all.append('geometry')\n",
    "    nodes_analysis_filter = nodes_analysis[column_max_all].copy()\n",
    "\n",
    "    if save_disk_space:\n",
    "        del nodes_analysis\n",
    "          \n",
    "    print('Calculated proximity to amenities data by node')\n",
    "        \n",
    "    #--------------- PROCESS DATA ---------- GROUP TIMES BY HEXAGONS\n",
    "    # group data by hex\n",
    "    res = 9\n",
    "    hex_tmp = hex_pop[['hex_id_9','geometry']]\n",
    "    hex_res_9_idx = aup.group_by_hex_mean(nodes_analysis_filter, hex_tmp, res, index_column)\n",
    "    hex_res_9_idx = hex_res_9_idx.loc[hex_res_9_idx[index_column]>0].copy()\n",
    "\n",
    "    if save_disk_space:\n",
    "        del hex_tmp\n",
    "        del nodes_analysis_filter\n",
    "        \n",
    "    print('Grouped nodes data by hexagons')\n",
    "          \n",
    "    #--------------- PROCESS DATA ---------- RE-CALCULATE MAX TIMES BY HEXAGON\n",
    "    # This step recalculates max time to each eje from max times to calculated amenities and max_time from max eje\n",
    "    column_max_ejes = [] # list with ejes index column names\n",
    "\n",
    "    #Goes (again) through each eje in dictionary:\n",
    "    for e in idx_15_min.keys():\n",
    "\n",
    "        column_max_ejes.append('max_'+ e.lower())\n",
    "        column_max_amenities = [] # list with amenities in current eje\n",
    "\n",
    "        #Goes (again) through each amenity of current eje:    \n",
    "        for a in idx_15_min[e].keys():\n",
    "\n",
    "            column_max_amenities.append('max_'+ a.lower())\n",
    "\n",
    "        #Re-calculates time to currently examined eje (max time of its amenities):        \n",
    "        hex_res_9_idx['max_'+ e.lower()] = hex_res_9_idx[column_max_amenities].max(axis=1)\n",
    "\n",
    "    hex_res_9_idx[index_column] = hex_res_9_idx[column_max_ejes].max(axis=1)\n",
    "\n",
    "    #Add to column_max_all list the attribute 'max_time'\n",
    "    column_max_ejes.append(index_column)\n",
    "          \n",
    "    print('Finished recalculating times in hexagons')\n",
    "    \n",
    "    #--------------- PROCESS DATA ---------- INDEX, MEDIAN AND MEAN CALCULATION\n",
    "    # This step adds data\n",
    "    \n",
    "    #Define function\n",
    "    def apply_sigmoidal(x):\n",
    "        if x == -1:\n",
    "            return -1\n",
    "        elif x > 1000:\n",
    "            return 0\n",
    "        else:\n",
    "            val = aup.sigmoidal_function(0.1464814753435666, x, 30)\n",
    "            return val\n",
    "    \n",
    "    #Apply function\n",
    "    amenities_col = ['max_preescolar','max_primaria','max_secundaria',\n",
    "               'max_salud','max_guarderías','max_asistencia social',\n",
    "               'max_alimentos','max_personal','max_farmacias','max_hogar',\n",
    "               'max_complementarios','max_social','max_actividad física',\n",
    "               'max_cultural']\n",
    "    for ac in amenities_col:\n",
    "        idx_col = ac.replace('max','idx')\n",
    "        hex_res_9_idx[idx_col] = hex_res_9_idx[ac].apply(apply_sigmoidal)\n",
    "    \n",
    "    #Add data\n",
    "    idx_colname = []\n",
    "    for ac in amenities_col:\n",
    "        idx_col = ac.replace('max','idx')\n",
    "        idx_colname.append(idx_col)\n",
    "    \n",
    "    hex_res_9_idx['mean_time'] = hex_res_9_idx[amenities_col].mean(axis=1)\n",
    "    hex_res_9_idx['median_time'] = hex_res_9_idx[amenities_col].median(axis=1)\n",
    "    hex_res_9_idx['idx_sum'] = hex_res_9_idx[idx_colname].sum(axis=1)\n",
    "          \n",
    "    print('Finished calculating index, mean and median time')\n",
    "    \n",
    "    #--------------- PROCESS DATA ---------- ADD POP AND CITY DATA\n",
    "    # calculate population density\n",
    "    hex_pop = hex_pop.to_crs(\"EPSG:6372\")\n",
    "    hex_pop['dens_pobha'] = hex_pop['pobtot'] / (hex_pop.area/10000)\n",
    "    hex_pop = hex_pop.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Add pop data\n",
    "    pop_list = ['hex_id_9','pobtot','dens_pobha']\n",
    "    hex_res_9_idx = pd.merge(hex_res_9_idx, hex_pop[pop_list], on='hex_id_9')\n",
    "\n",
    "    if save_disk_space:\n",
    "        del hex_pop\n",
    "    \n",
    "    # Add city data\n",
    "    hex_res_9_idx['city'] = city\n",
    "          \n",
    "    print('Finished adding pop and city data')\n",
    "    \n",
    "    #--------------- FINAL FORMAT ----------\n",
    "    #--------------- FINAL FORMAT ---------- REORDER COLUMNS    \n",
    "    #Final format\n",
    "    final_column_ordered_list = ['hex_id_9', 'geometry', \n",
    "                             'max_escuelas', 'max_preescolar', 'max_primaria', 'max_secundaria',\n",
    "                             'max_servicios comunitarios', 'max_salud', 'max_guarderías', 'max_asistencia social',\n",
    "                             'max_comercio', 'max_alimentos', 'max_personal', 'max_farmacias', 'max_hogar', 'max_complementarios',\n",
    "                             'max_entretenimiento', 'max_social', 'max_actividad física', 'max_cultural', \n",
    "                             'idx_preescolar', 'idx_primaria', 'idx_secundaria',\n",
    "                             'idx_salud', 'idx_guarderías', 'idx_asistencia social',\n",
    "                             'idx_alimentos', 'idx_personal', 'idx_farmacias', 'idx_hogar', 'idx_complementarios',\n",
    "                             'idx_social', 'idx_actividad física', 'idx_cultural',\n",
    "                             'mean_time', 'median_time', 'max_time', 'idx_sum',\n",
    "                             'pobtot', 'dens_pobha','city']\n",
    "\n",
    "    hex_res_9_idx_city = hex_res_9_idx[final_column_ordered_list]\n",
    "\n",
    "    if save_disk_space:\n",
    "        del hex_res_9_idx\n",
    "          \n",
    "    print('Finished final format')\n",
    "        \n",
    "    #--------------- SAVE TO DB ----------\n",
    "    if save:\n",
    "        #Load previously loaded data\n",
    "        #prox_schema = 'prox_analysis'\n",
    "        #prox_table = 'proximityanalysis_hexres9'\n",
    "        #query = f\"SELECT * FROM {prox_schema}.{prox_table}\"\n",
    "        #prox_all = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "        #print('Loaded already processed data from db')    \n",
    "\n",
    "        #Concatenate data\n",
    "        #dfs = [hex_res_9_idx_city,prox_all]\n",
    "        #prox = pd.concat(dfs)\n",
    "        \n",
    "        #if save_disk_space:\n",
    "        #    del hex_res_9_idx_city\n",
    "        #    del prox_all\n",
    "            \n",
    "        #print('Concatented {} data to already processed data from db'.format(city))\n",
    "\n",
    "        #Upload data\n",
    "        aup.gdf_to_db_slow(hex_res_9_idx_city,\"proximityanalysis_hexres9\", 'prox_analysis', if_exists='append')\n",
    "        print('Uploaded {} data to db'.format(city))\n",
    "    \n",
    "    print('FINISHED ANALYSIS FOR {}'.format(city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22a10795-ddfd-49db-aa0f-43799ecbb466",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "['Aguascalientes', 'Ensenada', 'Mexicali', 'Tijuana', 'La Paz', 'Campeche', 'Laguna', 'Monclova', 'Piedras Negras', 'Saltillo', 'Colima', 'Tecoman', 'Tapachula', 'Tuxtla', 'Chihuahua', 'Delicias', 'Juarez', 'Parral', 'ZMVM', 'Durango', 'Celaya', 'Guanajuato', 'Leon', 'Moroleon', 'San Francisco', 'Acapulco', 'Chilpancingo', 'Pachuca', 'Tula', 'Tulancingo', 'Guadalajara', 'Ocotlan', 'Vallarta', 'Tianguistenco', 'Toluca', 'Piedad', 'Morelia', 'Zamora', 'Cuautla', 'Cuernavaca', 'Tepic', 'Monterrey', 'Oaxaca', 'Tehuantepec', 'Puebla', 'Tehuacan', 'Teziutlan', 'Queretaro', 'Cancun', 'Chetumal', 'Rio Verde', 'SLP', 'Culiacan', 'Mazatlan', 'Guaymas', 'Hermosillo', 'Nogales', 'Villahermosa', 'Victoria', 'Matamoros', 'Nuevo Laredo', 'Reynosa', 'Tampico', 'Tlaxcala', 'Acayucan', 'Coatzacoalcos', 'Cordoba', 'Minatitlan', 'Orizaba', 'Poza Rica', 'Veracruz', 'Xalapa', 'Merida', 'Zacatecas']\n"
     ]
    }
   ],
   "source": [
    "#Load mun data\n",
    "mun_schema = 'metropolis'\n",
    "mun_table = 'metro_gdf'\n",
    "query = f\"SELECT * FROM {mun_schema}.{mun_table}\" \n",
    "gdf_mun = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "\n",
    "#Find already loaded cities\n",
    "prox_schema = 'prox_analysis'\n",
    "prox_table = 'proximityanalysis_hexres9'\n",
    "query = f\"SELECT * FROM {prox_schema}.{prox_table}\"\n",
    "prox_all = aup.gdf_from_query(query, geometry_col='geometry')\n",
    "processed_city_list = list(prox_all.city.unique())\n",
    "print(len(processed_city_list))\n",
    "print(processed_city_list)\n",
    "\n",
    "#Run main function\n",
    "for city in gdf_mun.city.unique():\n",
    "        if city not in processed_city_list:\n",
    "            main(city, save=True, save_disk_space = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69062d34-c982-4300-aa5b-7c15b35c5594",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3c37d8-bb16-4463-9313-ddf96fde3d83",
   "metadata": {
    "tags": []
   },
   "source": [
    "#for city in gdf_mun.city.unique():\n",
    "    if city not in processed_city_list:\n",
    "        print(city)\n",
    "    else:\n",
    "        print(\"{} ya está procesada\".format(city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051514aa-536a-412b-becb-024edfe99e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
